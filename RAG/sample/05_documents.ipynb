{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34",
   "metadata": {
    "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 5:** 處理大型文件</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "在上一個 notebook 中，我們學習了 Running State Chain 和知識庫！到最後，我們擁有了進行簡單對話管理和自訂知識追蹤所需的所有工具。在這個 notebook 中，我們將採用相同的想法並轉向大型文件的領域，考慮當我們嘗試將大型檔案納入我們的 LLM 脈絡資訊(Context)時會遇到什麼樣的問題。\n",
    "\n",
    "**學習目標：**\n",
    "\n",
    "\n",
    "-   熟悉文件載入器以及它們可能為您提供的實用工具類型。\n",
    "-   學習如何透過分塊(Chunking)文件並逐步建構知識庫來解析脈絡資訊(Context)空間有限的大型文件。\n",
    "-   了解文件分塊(document chunks)的漸進式重新脈絡化(progressive recontextualization)、強制(coersion)和整合(consolidation)如何極其有用，以及它會遇到自然限制的地方。\n",
    "\n",
    "**值得思考的問題：**\n",
    "\n",
    "\n",
    "-   查看從您的 ArxivParser 出來的分塊，您會注意到有些分塊本身沒有什麼意義，或者在轉換為文字(Text)時已經完全損壞。是否值得對分塊進行清理？\n",
    "-   考慮文件摘要工作流程（或任何處理大量文件分塊列表的類似工作流程），這應該多久發生一次，什麼時候是合理的？\n",
    "\n",
    "**環境設置：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
   "metadata": {
    "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "# %pip install -qq arxiv pymupdf\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c33c07-19b8-4c81-8d99-30fa2b3b2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: 01-ai/yi-large. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-0528', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-llama-8b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-14b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-32b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-7b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-12b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-1b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-27b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-4b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/shieldgemma-9b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='gotocompany/gemma-2-9b-cpt-sahabatai-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.3-8b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-guardian-3.0-8b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='igenius/colosseum_355b_instruct_16k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='igenius/italia_10b_instruct_16k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='marin/marin-8b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-4-scout-17b-16e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-multimodal-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mistral-medium-3-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-nemotron', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-24b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-3.1-24b-instruct-2503', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/embed-qa-4', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-content-safety', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-topic-control', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-4b-v1.1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-8b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-vl-8b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-ultra-253b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nv-embedqa-1b-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nv-embedqa-1b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-base', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemoretriever-parse', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embed-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-nv-embed-v1'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedcode-7b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-e5-v5', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-mistral-7b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nvclip', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvquery/meta/llama-3.3-70b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-7b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen3-235b-a22b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwq-32b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='snowflake/arctic-embed-l', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-arctic-embed-l'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='speakleash/bielik-11b-v2.3-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='tiiuae/falcon3-7b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='utter-project/eurollm-9b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-creative-122b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8583a1-c10a-41da-8256-49520f868670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第一部分：** 與文件聊天\n",
    "\n",
    "\n",
    "這個 notebook 將開始圍繞使用 LLM 與文件聊天的較長討論串。在一個聊天模型在巨大的公共資料儲存庫上訓練，而在自訂資料上重新訓練它們成本高得令人望而卻步的世界中，讓 LLM 對一組 PDF 甚至 YouTube 影片進行分析推理(Reasoning)的想法開啟了許多機會！\n",
    "\n",
    "-   **您的 LLM 可以擁有基於人類可讀文件的可修改知識庫，** 這意味著您可以直接控制它有權存取什麼樣的資料，並可以指示它與之互動。\n",
    "-   **您的 LLM 可以整理並直接從您的文件集中提取參考資料。** 透過充分的提示(Prompt)工程和指令跟隨先驗假設，您可以強制您的模型僅基於您提供的材料行動。\n",
    "-   **您的 LLM 甚至可能與您的文件互動，根據需要進行自動修改。** 這開啟了自動內容精煉(Refinement)和綜合結果(Synthesis)操作的途徑，這將在稍後探討。\n",
    "\n",
    "列出一些可能性相當容易，從那裡您可以讓您的想像力自由馳騁...但我們還沒有獲得做這件事的工具，對吧？\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **最單純的(Naive)方法：填充(Stuffing)您的文件**\n",
    "\n",
    "假設您有一些文字(Text)文件（PDF、部落格等）並想詢問與這些文件內容相關的問題。您可以嘗試的一種方法涉及取得文件的表示並將其全部餵給聊天模型！從文件角度來看，這被稱為[**文件填充(document stuffing)**](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/)。\n",
    "\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
    ">\n",
    "> 來自 [**Stuff | LangChain**🦜️🔗](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/)\n",
    "\n",
    "如果您的模型足夠強大且您的文件足夠短，這可能會運作得很好，但不應該期望它對整個文件運作得很好。許多現代 LLM 由於訓練限制而在處理長脈絡資訊(Context)方面有顯著困難。現在大型模型的惡化並不那麼災難性，但無論您使用哪個模型，良好的指令跟隨可能會很快崩潰（假設您存取的是原始模型）。\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**您需要解決的文件分析推理(Reasoning)的關鍵問題是：**\n",
    "\n",
    "-   我們如何將文件分割成可以進行分析推理(Reasoning)的片段？\n",
    "-   隨著文件大小和數量的增加，我們如何有效地找到並考慮這些片段？\n",
    "\n",
    "這門課程將探索幾種方法來解決這些問題，同時繼續建構我們的 LLM 流程協調管理(Orchestration)技能。***這個 notebook 將擴展我們之前的 Running Chain 技能，用於更漸進的分析推理(Reasoning)制定，而下一個 notebook 將介紹一些新技術來適當地解決大規模檢索(Retrieval)。*** 透過這種體驗，我們將繼續利用尖端的開源解決方案來使我們的解決方案標準化和可整合。\n",
    "\n",
    "說到這裡，文件載入框架領域有許多強大的選項，兩個主要參與者將在整個課程中出現：\n",
    "\n",
    "-   [**LangChain**](https://python.langchain.com/docs/get_started/introduction) 透過一般分塊(Chunking)策略和與內嵌框架/服務的強大整合，為將 LLM 連接到您自己的資料來源提供了一個簡單的框架。這個框架最初圍繞其對 LLM 功能的強大一般支援而發展，這表明其積極的優勢更接近鏈(Chain)想法(abstractions)和 Agent 協調。\n",
    "\n",
    "-   [**LlamaIndex**](https://gpt-index.readthedocs.io/en/stable/) 是一個用於 LLM 應用程式的資料框架，用於輸入(Intake)、結構化和存取私有或特定領域的資料。它後來分支出來包括類似於 LangChain 的一般 LLM 功能，但截至目前，它在解決 LLM 組件的文件方面仍然最強，因為其初始想法(abstractions)圍繞該問題為中心。\n",
    "\n",
    "建議閱讀更多關於 LlamaIndex 和 LangChain 的獨特優勢，並選擇最適合您的。由於 LlamaIndex 可以*與* LangChain 一起使用，框架的獨特功能可以在沒有太多問題的情況下一起利用。為了簡單起見，我們將在這門課程中堅持使用 LangChain，並將允許 [**NVIDIA/GenerativeAIExamples 儲存庫**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/notebooks) 為感興趣的人探索更深入的 LlamaIndex 選項。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310462b-f215-4d00-9d59-e613921bed0a",
   "metadata": {
    "id": "3310462b-f215-4d00-9d59-e613921bed0a"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第二部分：** 載入文件\n",
    "\n",
    "\n",
    "LangChain 提供各種[文件載入器](https://python.langchain.com/docs/integrations/document_loaders)來促進從許多不同來源和位置（本地儲存、私有 s3 儲存桶、公共網站、訊息 API 等）輸入(Intake)各種文件格式（HTML、PDF、程式碼）。這些載入器查詢您的資料來源並返回一個 `Document` 物件，該物件包含內容和中介資料(Metadata)，通常採用純文字(Text)或其他人類可讀格式。已經有很多文件載入器構建並準備使用，第一方 LangChain 選項列在[這裡](https://python.langchain.com/docs/integrations/document_loaders)。\n",
    "\n",
    "**在這個範例中，我們可以使用以下 LangChain 載入器之一載入我們選擇的研究論文：**\n",
    "\n",
    "-   [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file)：適用於任意檔案的一般有用檔案載入器；對您的文件結構不做太多假設，通常是足夠的。\n",
    "\n",
    "-   [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv)：一個更專業的檔案載入器，可以直接與 Arxiv 介面通訊。[只是許多範例中的一個](https://python.langchain.com/docs/integrations/document_loaders)，這將對您的資料做出一些更多假設，以產生更好的解析並自動填充中介資料(Metadata)（當您有多個文件/格式時很有用）。\n",
    "\n",
    "對於我們的程式碼範例，我們將預設使用 `ArxivLoader` 載入 [MRKL](https://arxiv.org/abs/2205.00445) 或 [ReAct](https://arxiv.org/abs/2210.03629) 發表論文中的一個，因為您在繼續聊天模型研究努力中可能會在某個時候遇到它們。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4382b61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1703112979370,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "b4382b61",
    "outputId": "d6e95b9b-97be-4984-a9fd-58a528091146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 436 ms, sys: 85 ms, total: 521 ms\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "## Loading in the file\n",
    "\n",
    "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
    "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()\n",
    "\n",
    "## More specialized loader, won't work for everything, but simple API and usually better results\n",
    "documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
    "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
    "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
    "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
    "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hw0SL--6cirp",
   "metadata": {
    "id": "hw0SL--6cirp"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "我們可以從我們的匯入中看到，這個連接器(Connector)為我們提供了兩個不同組件的存取：\n",
    "\n",
    "-   `page_content` 是文件的實際主體，採用某種人類可解釋的格式。\n",
    "\n",
    "-   `metadata` 是連接器(Connector)透過其資料來源提供的關於文件的相關資訊。\n",
    "\n",
    "下面，我們可以查看文件主體的長度以了解裡面有什麼，並且可能會注意到一個難以處理的文件長度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1703113455184,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
    "outputId": "98b9ef68-c36b-478f-9bbb-1e45b2c49d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents Retrieved: 1\n",
      "Sample of Document 1 Content (Total Length: 89593):\n",
      "From Local to Global: A GraphRAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1†\n",
      "Ha Trinh1†\n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Dasha Metropolitansky1\n",
      "Robert Osazuwa Ness1\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
      "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
      "†These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scal\n"
     ]
    }
   ],
   "source": [
    "## Printing out a sample of the content\n",
    "print(\"Number of Documents Retrieved:\", len(documents))\n",
    "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
    "print(documents[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JjUK2ZSd0HL",
   "metadata": {
    "id": "1JjUK2ZSd0HL"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "相比之下，中介資料(Metadata)將更保守地調整大小，以至於成為您喜愛的聊天模型的可行脈絡資訊(Context)組件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Py2lbRXlcX81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1703112982386,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Py2lbRXlcX81",
    "outputId": "07197dd4-1609-4ecf-ae54-cf6ef3d25458"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-19'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Metropolitansky, Robert Osazuwa Ness, Jonathan Larson'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of retrieval-augmented generation (RAG) to retrieve relevant\\ninformation from an external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source enables large language models\\n(LLMs) to answer questions over private and/or previously unseen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the main themes in the dataset?\", since this is\\ninherently a query-focused summarization (QFS) task, rather than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">closely related entities. Given a question, each community summary is\\nused to generate a partial response, before </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all partial responses are again\\nsummarized in a final response to the user. For a class of global </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-02-19'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha \u001b[0m\n",
       "\u001b[32mMetropolitansky, Robert Osazuwa Ness, Jonathan Larson'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to retrieve relevant\\ninformation from an external \u001b[0m\n",
       "\u001b[32mknowledge source enables large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to answer questions over private and/or previously unseen \u001b[0m\n",
       "\u001b[32mdocument\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are\u001b[0m\n",
       "\u001b[32mthe main themes in the dataset?\", since this is\\ninherently a query-focused summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task, rather than \u001b[0m\n",
       "\u001b[32man explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by \u001b[0m\n",
       "\u001b[32mtypical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based \u001b[0m\n",
       "\u001b[32mapproach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions \u001b[0m\n",
       "\u001b[32mand the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive \u001b[0m\n",
       "\u001b[32man entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of \u001b[0m\n",
       "\u001b[32mclosely related entities. Given a question, each community summary is\\nused to generate a partial response, before \u001b[0m\n",
       "\u001b[32mall partial responses are again\\nsummarized in a final response to the user. For a class of global \u001b[0m\n",
       "\u001b[32msensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial \u001b[0m\n",
       "\u001b[32mimprovements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046ea74-0b81-400e-8364-449f421d2add",
   "metadata": {
    "id": "7046ea74-0b81-400e-8364-449f421d2add"
   },
   "source": [
    "<br>\n",
    "\n",
    "雖然接受中介資料(Metadata)格式並完全忽略主體可能很誘人，但有一些關鍵的選擇功能無法在不深入完整文字(Text)的情況下實現：\n",
    "\n",
    "-   **中介資料(Metadata)不被保證。** 在 `arxiv` 的情況下，論文摘要、標題、作者和日期是提交的必要組件，所以能夠查詢它們並不令人驚訝。但對於任意 PDF 或網頁，情況不一定如此。\n",
    "\n",
    "-   **Agent 將無法更深入地了解文件內容。** 摘要很好知道並可以按原樣使用，但不提供與主體在任何容量上互動的直接路徑（至少從我們學到的內容來看不是這樣）。\n",
    "\n",
    "-   **Agent 仍然無法同時對太多文件進行分析推理(Reasoning)。** 也許在 MRKL/ReAct 範例中，您可以將這兩個摘要合併為一個脈絡資訊(Context)並詢問一些問題。但是當您需要同時與 5 個文件互動時會發生什麼？整個目錄呢？很快，您會注意到您的脈絡資訊(Context)視窗將被資訊過載，僅僅是為了總結或甚至列出您感興趣的文件！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0449e4",
   "metadata": {
    "id": "4e0449e4"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## **第三部分：** 轉換文件\n",
    "\n",
    "\n",
    "一旦文件被載入，如果我們打算將它們作為脈絡資訊(Context)傳遞到我們的 LLM 中，它們通常需要被轉換。一種轉換方法被稱為**分塊(chunking)**，它將大塊內容分解為較小的片段。這種技術很有價值，因為它有助於[最佳化從向量資料庫返回內容的相關性](https://www.pinecone.io/learn/chunking-strategies/)。\n",
    "\n",
    "LangChain 提供[各種文件轉換器](https://python.langchain.com/docs/integrations/document_transformers/)，其中我們將使用 [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)。這個選項將允許我們基於自然停止點的偏好來分割我們的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1703112527056,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
    "outputId": "a4e666e5-5a5c-413b-f5a4-acca742d80d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "## Some nice custom preprocessing\n",
    "# documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# def include_doc(doc):\n",
    "#     ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
    "#     string = doc.page_content\n",
    "#     if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
    "print(len(docs_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1703112530925,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
    "outputId": "1cf24605-65bb-40a2-e7aa-e2d9a8fb6382"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Local to Global: A GraphRAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1†\n",
      "Ha Trinh1†\n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Dasha Metropolitansky1\n",
      "Robert Osazuwa Ness1\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
      "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
      "†These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scale to the quantities of text indexed by typ-\n",
      "ical RAG systems. To combine the strengths of these contrasting methods, we\n",
      "propose GraphRAG, a graph-based approach to question answering over private\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propose GraphRAG, a graph-based approach to question answering over private\n",
      "text corpora that scales with both the generality of user questions and the quantity\n",
      "of source text. Our approach uses an LLM to build a graph index in two stages:\n",
      "first, to derive an entity knowledge graph from the source documents, then to pre-\n",
      "generate community summaries for all groups of closely related entities. Given a\n",
      "question, each community summary is used to generate a partial response, before\n",
      "all partial responses are again summarized in a final response to the user. For a\n",
      "class of global sensemaking questions over datasets in the 1 million token range,\n",
      "we show that GraphRAG leads to substantial improvements over a conventional\n",
      "RAG baseline for both the comprehensiveness and diversity of generated answers.\n",
      "1\n",
      "Introduction\n",
      "Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\n",
      "LLMs to answer queries based on data that is too large to contain in a language model’s context\n",
      "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n",
      "at once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to\n",
      "a large external corpus of text records and retrieves a subset of records that are individually relevant\n",
      "to the query and collectively small enough to fit into the context window of the LLM. The LLM then\n",
      "Preprint. Under review.\n",
      "arXiv:2404.16130v2  [cs.CL]  19 Feb 2025\n",
      "generates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang,\n",
      "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call\n",
      "vector RAG, works well for queries that can be answered with information localized within a small\n",
      "set of records. However, vector RAG approaches do not support sensemaking queries, meaning\n",
      "queries that require global understanding of the entire dataset, such as ”What are the key trends in\n",
      "how scientific discoveries are influenced by interdisciplinary research over the past decade?”\n",
      "Sensemaking tasks require reasoning over “connections (which can be among people, places, and\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from few-shot exemplars specialized to those domains.\n",
      "The LLM can also be prompted to extract claims about detected entities. Claims are important\n",
      "factual statements about entities, such as dates, events, and interactions with other entities. As\n",
      "with entities and relationships, in-context learning exemplars can provide domain-specific guidance.\n",
      "Claim descriptions extracted from the example tetx chunk are as follows:\n",
      "• NeoChip’s shares surged during their first week of trading on the NewTech Exchange.\n",
      "• NeoChip debuted as a publicly listed company on the NewTech Exchange.\n",
      "• Quantum Systems acquired NeoChip in 2016 and held ownership until NeoChip went pub-\n",
      "lic.\n",
      "See Appendix A for prompts and details on our implementation of entity and claim extraction.\n",
      "3.1.3\n",
      "Entities & Relationships →Knowledge Graph\n",
      "The use of an LLM to extract entities, relationships, and claims is a form of abstractive summariza-\n",
      "tion – these are meaningful summaries of concepts that, in the case of relationships and claims, may\n",
      "not be explicitly stated in the text. The entity/relationship/claim extraction processes creates mul-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.8\n",
      "-3.68\n",
      "0.003\n",
      "C1\n",
      "TS\n",
      "47.92\n",
      "52.08\n",
      "-2.41\n",
      "0.126\n",
      "46.64\n",
      "53.36\n",
      "-2.91\n",
      "0.04\n",
      "C2\n",
      "TS\n",
      "48.8\n",
      "51.2\n",
      "-2.23\n",
      "0.179\n",
      "48.32\n",
      "51.68\n",
      "-2.12\n",
      "0.179\n",
      "C3\n",
      "TS\n",
      "48.08\n",
      "51.92\n",
      "-2.23\n",
      "0.179\n",
      "48.32\n",
      "51.68\n",
      "-2.56\n",
      "0.074\n",
      "C0\n",
      "SS\n",
      "35.12\n",
      "64.88\n",
      "-6.17\n",
      "<0.001\n",
      "41.44\n",
      "58.56\n",
      "-4.82\n",
      "<0.001\n",
      "C1\n",
      "SS\n",
      "40.32\n",
      "59.68\n",
      "-4.83\n",
      "<0.001\n",
      "45.2\n",
      "54.8\n",
      "-3.19\n",
      "0.017\n",
      "C2\n",
      "SS\n",
      "40.4\n",
      "59.6\n",
      "-4.67\n",
      "<0.001\n",
      "44.88\n",
      "55.12\n",
      "-3.65\n",
      "0.003\n",
      "C3\n",
      "SS\n",
      "40.48\n",
      "59.52\n",
      "-4.69\n",
      "<0.001\n",
      "45.6\n",
      "54.4\n",
      "-2.86\n",
      "0.043\n",
      "TS\n",
      "SS\n",
      "43.6\n",
      "56.4\n",
      "-3.96\n",
      "<0.001\n",
      "46\n",
      "54\n",
      "-2.68\n",
      "0.066\n",
      "C0\n",
      "C1\n",
      "46.96\n",
      "53.04\n",
      "-2.87\n",
      "0.037\n",
      "47.6\n",
      "52.4\n",
      "-2.17\n",
      "0.179\n",
      "C0\n",
      "C2\n",
      "48.4\n",
      "51.6\n",
      "-2.06\n",
      "0.197\n",
      "48.48\n",
      "51.52\n",
      "-1.61\n",
      "0.321\n",
      "C1\n",
      "C2\n",
      "49.84\n",
      "50.16\n",
      "-1\n",
      "0.952\n",
      "49.28\n",
      "50.72\n",
      "-1.6\n",
      "0.321\n",
      "C0\n",
      "C3\n",
      "48.4\n",
      "51.6\n",
      "-1.8\n",
      "0.29\n",
      "47.2\n",
      "52.8\n",
      "-2.62\n",
      "0.071\n",
      "C1\n",
      "C3\n",
      "49.76\n",
      "50.24\n",
      "0\n",
      "1\n",
      "48.8\n",
      "51.2\n",
      "-1.29\n",
      "0.321\n",
      "C2\n",
      "C3\n",
      "50\n",
      "50\n",
      "0\n",
      "1\n",
      "48.8\n",
      "51.2\n",
      "-1.84\n",
      "0.262\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in (0, 1, 2, 15, -1):\n",
    "    pprint(f\"[Document {i}]\")\n",
    "    print(docs_split[i].page_content)\n",
    "    pprint(\"=\"*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc",
   "metadata": {
    "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "我們的分塊(Chunking)方法相當最單純的(naive)，但突出了為我們的應用程式至少讓某些東西運作的容易性。我們努力保持分塊大小較小，以便我們的模型能夠有效地將其作為脈絡資訊(Context)使用，但我們如何對所有這些片段進行分析推理(Reasoning)？\n",
    "\n",
    "**當為任意文件集擴展和最佳化這種方法時，一些潛在選項包括：**\n",
    "\n",
    "-   識別邏輯中斷或綜合結果(Synthesis)技術（手動、自動、LLM 輔助等）。\n",
    "-   旨在構建富含獨特和相關資訊的分塊，避免冗餘以最大化資料庫實用性。\n",
    "-   自訂分塊以適應文件的性質，確保分塊在脈絡資訊(Context)上相關且連貫。\n",
    "-   在每個分塊中包含關鍵概念、關鍵字或中介資料(Metadata)片段，以改善資料庫中的可搜尋性和相關性。\n",
    "-   持續評估分塊效果，並準備調整策略以在大小和內容豐富性之間達到最佳平衡。\n",
    "-   考慮階層結構(Hierarchy)系統（隱式生成或明確指定）以改善檢索(Retrieval)嘗試。\n",
    "    -   如果感興趣，請查看 [**LlamaIndex 索引指南中的樹狀結構**](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index) 作為起點。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-0QApYgNbyJD",
   "metadata": {
    "id": "-0QApYgNbyJD"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第四部分：[練習]** 精煉(Refinement)摘要\n",
    "\n",
    "\n",
    "為了自動對大型文件進行分析推理(Reasoning)，一個潛在的想法可能是使用 LLM 創建密集摘要或知識庫。類似於我們在上一個 notebook 中透過槽填充維護對話的運行歷史紀錄(History)，保持整個文件的運行歷史紀錄(History)有什麼問題嗎？\n",
    "\n",
    "在這一節中，我們專注於 LLM 的一個令人興奮的應用：**自動精煉(Refinement)、強制和大量整合(Aggregate)資料**。具體來說，我們將實作一個簡單但有用的可運行物件(Runnable)，它使用 while 迴圈和 Running State Chain 制定來總結一組文件分塊。這個過程通常被稱為 [**「文件精煉(document refinement)」**](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/)，很大程度上類似於我們之前以對話為中心的槽填充練習；唯一的區別是現在我們處理的是大型文件而不是不斷增長的聊天歷史紀錄(History)。\n",
    "\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1J2XR8Cc8YSkVJMiJCknMkgA02mBT8riZ\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>\n",
    ">\n",
    "> From [**Refine | LangChain**🦜️🔗](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **DocumentSummaryBase 模型**\n",
    "\n",
    "\n",
    "就像上一個 notebook 中的 `KnowledgeBase` 類別一樣，我們可以創建一個 `DocumentSummaryBase` 結構，設計用於封裝文件的精髓。下面的一個將使用 `running_summary` 欄位來查詢模型以獲得最終摘要，同時嘗試使用 `main_ideas` 和 `loose_ends` 欄位作為瓶頸，以防止運行摘要移動得太快。這是我們必須透過提示(Prompt)工程強制執行的東西，所以也提供了 `summary_prompt`，它顯示了這些資訊將如何使用。請根據需要隨意修改它，以使其適用於您選擇的模型。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gE8y2JvLvZ5T",
   "metadata": {
    "id": "gE8y2JvLvZ5T"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class DocumentSummaryBase(BaseModel):\n",
    "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
    "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
    "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
    "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
    "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
    "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
    "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
    "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LkjfpOAvlEd",
   "metadata": {
    "id": "7LkjfpOAvlEd"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "我們也將利用這個機會從上一個 notebook 中帶回 `RExtract` 函式(function)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "khRhVghHxBaz",
   "metadata": {
    "id": "khRhVghHxBaz"
   },
   "outputs": [],
   "source": [
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFtME_s4PRoW",
   "metadata": {
    "id": "oFtME_s4PRoW"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "考慮到這一點，以下程式碼在 for 迴圈中invoke Running State Chain 來疊代(Iterate)您的文件！唯一必要的修改應該是 `parse_chain` 實作，它應該透過來自上一個 notebook 的適當配置的 `RExtract` 鏈(Chain)傳遞狀態。在此之後，系統應該運作得相當好，以維護文件的運行摘要（儘管根據使用的模型，可能需要對提示(Prompt)進行一些調整）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6sODIfHUgz6m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79192,
     "status": "ok",
     "timestamp": 1703112894722,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6sODIfHUgz6m",
    "outputId": "7b5aee70-078b-458e-d2a7-e8601b789fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 15 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">traded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in-context learning.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using few-shot exemplars for in-context learning.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What types of global sensemaking questions does GraphRAG aim to answer?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What is the impact of the context window size on the performance of GraphRAG?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text \u001b[0m\n",
       "\u001b[32mcorpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is \u001b[0m\n",
       "\u001b[32mprompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power \u001b[0m\n",
       "\u001b[32mprocessor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly \u001b[0m\n",
       "\u001b[32mtraded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for \u001b[0m\n",
       "\u001b[32min-context learning.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored \u001b[0m\n",
       "\u001b[32musing few-shot exemplars for in-context learning.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What types of global sensemaking questions does GraphRAG aim to answer?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What is the impact of the context window size on the performance of GraphRAG?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latest_summary = \"\"\n",
    "\n",
    "## TODO: Use the techniques from the previous notebook to complete the exercise\n",
    "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
    "    '''\n",
    "    Exercise: Create a chain that summarizes\n",
    "    '''\n",
    "    ###########################################################################################\n",
    "    ## START TODO:\n",
    "\n",
    "    def summarize_docs(docs):        \n",
    "        ## TODO: Initialize the parse_chain appropriately; should include an RExtract instance.\n",
    "        ## HINT: You can get a class using the <object>.__class__ attribute...\n",
    "        # parse_chain = RunnableAssign({'info_base' : (lambda x: None)})\n",
    "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
    "        ## TODO: Initialize a valid starting state. Should be similar to notebook 4\n",
    "        # state = {}\n",
    "        state = {'info_base' : knowledge}\n",
    "\n",
    "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            ## TODO: Update the state as appropriate using your parse_chain component\n",
    "            state['input'] = doc.page_content\n",
    "            state = parse_chain.invoke(state)\n",
    "\n",
    "            assert 'info_base' in state \n",
    "            if verbose:\n",
    "                print(f\"Considered {i+1} documents\")\n",
    "                pprint(state['info_base'])\n",
    "                latest_summary = state['info_base']\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        return state['info_base']\n",
    "        \n",
    "    ## END TODO\n",
    "    ###########################################################################################\n",
    "    \n",
    "    return RunnableLambda(summarize_docs)\n",
    "\n",
    "# instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_llm = instruct_model | StrOutputParser()\n",
    "\n",
    "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
    "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
    "summary = summarizer.invoke(docs_split[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">traded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in-context learning.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using few-shot exemplars for in-context learning.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What types of global sensemaking questions does GraphRAG aim to answer?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What is the impact of the context window size on the performance of GraphRAG?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text \u001b[0m\n",
       "\u001b[32mcorpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is \u001b[0m\n",
       "\u001b[32mprompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power \u001b[0m\n",
       "\u001b[32mprocessor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly \u001b[0m\n",
       "\u001b[32mtraded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for \u001b[0m\n",
       "\u001b[32min-context learning.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored \u001b[0m\n",
       "\u001b[32musing few-shot exemplars for in-context learning.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What types of global sensemaking questions does GraphRAG aim to answer?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What is the impact of the context window size on the performance of GraphRAG?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(latest_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKtoLf6DPv4Z",
   "metadata": {
    "id": "tKtoLf6DPv4Z"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第五部分：** 綜合結果(Synthesis)資料處理\n",
    "\n",
    "當我們結束使用 LLM 進行文件摘要的探索時，重要的是要承認更廣泛的脈絡資訊(Context)和潛在挑戰。雖然我們已經展示了提取簡潔、有意義摘要的可行方法，但讓我們考慮為什麼這樣的方法至關重要以及它所涉及的複雜性。\n",
    "\n",
    "**精煉(Refinement)的一般化(Generalized)**\n",
    "\n",
    "\n",
    "重要的是要注意，這種「漸進式摘要」技術僅僅是一個起始鏈(Chain)，它對初始資料和所需輸出格式做出很少假設。相同的技術可以廣泛擴展，以生成具有已知中介資料(Metadata)、積極假設和下游目標的綜合結果(Synthesis)精煉(Refinement)。\n",
    "\n",
    "**考慮這些潛在應用：**\n",
    "\n",
    "1.  **整合(Aggregate)資料**：構建將原始資料從文件分塊轉換為連貫、有用摘要的結構。\n",
    "\n",
    "2.  **分類和子主題分析**：創建將分塊中的見解分類到定義類別中的系統，追蹤每個類別中出現的子主題。\n",
    "\n",
    "3.  **整合(Aggregate)為密集資訊分塊**：將這些結構精煉(Refinement)為將見解提煉成緊湊片段，並富含直接引用以進行更深入分析。\n",
    "\n",
    "這些應用暗示了創建**特定領域知識圖譜(Knowledge Graph)**，可以被對話聊天模型存取和遍訪(Traverse)。一些實用工具已經存在，可以透過 [**LangChain 知識圖譜(Knowledge Graphs)**](https://python.langchain.com/docs/tutorials/graph/) 等工具自動生成這些。雖然您可能需要開發階層結構(Hierarchy)和工具來構建和遍訪(Traverse)這樣的結構，但當您可以為您的使用案例適當精煉(Refinement)足夠的知識圖譜(Knowledge Graph)時，這是一個可行的選項！對於那些對更高級的知識圖譜(Knowledge Graph)構建技術感興趣的人，這些技術依賴於更大的系統和向量相似性，我們發現 [**LangChain x Neo4j 文章**](https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/) 很有趣。\n",
    "\n",
    "**解決大規模資料處理的挑戰**\n",
    "\n",
    "\n",
    "雖然我們的方法開啟了令人興奮的可能性，但它並非沒有挑戰，特別是在處理大量資料時：\n",
    "\n",
    "-   **通用預處理限制**：雖然摘要相對直接，但開發在各種脈絡資訊(Context)中普遍有效的階層結構(Hierarchy)是具有挑戰性的。\n",
    "\n",
    "-   **粒度和訪問(Navigate)成本**：在階層結構(Hierarchy)中實現詳細粒度可能是資源密集的，需要複雜的整合(Aggregate)或廣泛的分支(Branch)來維持每次互動的可管理脈絡資訊(Context)大小。\n",
    "\n",
    "-   **對精確指令執行的依賴**：使用我們目前的工具訪問(Navigate)這樣的階層結構(Hierarchy)將嚴重依賴於具有強大提示(Prompt)工程的強大指令調整模型。推論(Inference)延遲和參數預測錯誤的風險可能很大，所以使用 LLM 進行此操作可能是一個挑戰。\n",
    "\n",
    "當您在課程中進步時，請追蹤這些挑戰如何透過後續技術得到解決。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdFSMXOVRzEa",
   "metadata": {
    "id": "cdFSMXOVRzEa"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第六部分：** 總結\n",
    "\n",
    "這個 notebook 的目標是介紹圍繞聊天模型大型文件處理的問題和技術。在下一個 notebook 中，我們將調查一個具有非常不同優缺點集合的補充工具；**使用內嵌模型的語義檢索(Retrieval)**。\n",
    "\n",
    "\n",
    "\n",
    "### <font color=\"#76b900\">**做得很好！**</font>\n",
    "\n",
    "### **下一步：**\n",
    "\n",
    "\n",
    "1.  **[可選]** 重新訪問(Navigate) notebook 頂部的**「值得思考的問題」部分**，並思考一些可能的答案。\n",
    "\n",
    "2.  **[可選]** 這個 notebook 包括一些基本的文件處理鏈(Chain)，但沒有涉及 [Map Reduce](https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/) 鏈(Chain)，這些也非常有用並建立在大致相同的直覺上。這些是一個很好的下一步，所以請查看它們！\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
