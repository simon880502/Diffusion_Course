{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73275a49",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60ab08",
   "metadata": {},
   "source": [
    "# 6. 評量（（Assessment）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fdc5d",
   "metadata": {},
   "source": [
    "恭喜你完成今天的課程！希望這是一次有趣的旅程，並且你能學到一些新技能作為紀念品。現在是時候來測試這些技能了。\n",
    "\n",
    "在這個評量中，挑戰是訓練一個能夠基於  [MNIST 數據集](https://en.wikipedia.org/wiki/MNIST_database)  生成手寫圖像的新模型。傳統上，神經網路會有測試數據集，但在生成式人工智慧（Generative AI）中，這並不一定如此。美在觀者眼中，是否接受過擬合（overfitting）取決於你作為開發者的判斷。\n",
    "\n",
    "因此，我們創建了一個在 MNIST 數據集上訓練的分類器模型，它在 MNIST 測試數據集上的準確率超過 99%。如果這個模型能夠正確識別 95% 的你生成的圖像，你將獲得證書！\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1083571",
   "metadata": {},
   "source": [
    "## 6.1 數據集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb74e7",
   "metadata": {},
   "source": [
    "\n",
    "讓我們開始吧，以下是這個評量中使用的函式庫。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c241ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# User defined libraries\n",
    "from utils import other_utils\n",
    "from utils import ddpm_utils\n",
    "from utils import UNet_utils\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69e9d3",
   "metadata": {},
   "source": [
    "我們之前使用的 FashionMnist 數據集在結構上與 MNIST 相似，因此我們將使用許多相同的程式碼來加載它。我們不會隨機水平翻轉，因為數字通常不會倒著讀。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(data_transform, train=True):\n",
    "    return torchvision.datasets.MNIST(\n",
    "        \"./data/\",\n",
    "        download=True,\n",
    "        train=train,\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "def load_transformed_MNIST(img_size, batch_size):\n",
    "    data_transforms = [\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),  # Scales data into [0,1]\n",
    "    ]\n",
    "\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    train_set = load_MNIST(data_transform, train=True)\n",
    "    test_set = load_MNIST(data_transform, train=False)\n",
    "    data = torch.utils.data.ConcatDataset([train_set, test_set])\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return data, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313fe37",
   "metadata": {},
   "source": [
    "我們將挑戰的分類器模型預期圖像大小為 `28 x 28` 像素。圖像是黑白的，有`10` 個類別，每個類別代表一個數字。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "IMG_CH = 1\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "data, dataloader = load_transformed_MNIST(IMG_SIZE, BATCH_SIZE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d3353",
   "metadata": {},
   "source": [
    "## 6.2 設定擴散過程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541da32",
   "metadata": {},
   "source": [
    "讓我們由設定擴散過程開始。為了節省時間，我們已經列出了推薦的 `Beta` 排程的超參數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 15\n",
    "\n",
    "T = nrows * ncols\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4ad7f",
   "metadata": {},
   "source": [
    "**TODO**: 我們仍需要計算一些在我們的 `q` 和 `reverse_q` 函式中將要使用的變數。可以替換下面的 `FIXME`s 嗎？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.0 - B\n",
    "a_bar = FIXME(a, dim=0)\n",
    "sqrt_a_bar = FIXME(a_bar)  # Mean Coefficient\n",
    "sqrt_one_minus_a_bar = FIXME(1 - a_bar)  # St. Dev. Coefficient\n",
    "\n",
    "# Reverse diffusion variables\n",
    "sqrt_a_inv = FIXME(1 / a)\n",
    "pred_noise_coeff = (1 - a) / FIXME(1 - a_bar)  # Predicted Noise Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247201",
   "metadata": {},
   "source": [
    "**TODO**: 下面的 `q` 函式幾乎完成了，但我們需要找到圖像與噪訊的正確比例。這是如何做到的？\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(x_0, t):\n",
    "        t = t.int()\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_a_bar_t = sqrt_a_bar[t, None, None, None]\n",
    "        sqrt_one_minus_a_bar_t = sqrt_one_minus_a_bar[t, None, None, None]\n",
    "\n",
    "        x_t = FIXME * x_0 + FIXME * noise\n",
    "        return x_t, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83430633",
   "metadata": {},
   "source": [
    "請花點時間確認結果是否符合你的預期。圖像是否從清晰可識別開始，然後在噪訊中消失？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "x_0 = data[0][0].to(device)\n",
    "xs = []\n",
    "\n",
    "for t in range(T):\n",
    "    t_tenser = torch.Tensor([t]).type(torch.int64)\n",
    "    x_t, _ = q(x_0, t_tenser)\n",
    "    img = torch.squeeze(x_t).cpu()\n",
    "    xs.append(img)\n",
    "    ax = plt.subplot(nrows, ncols, t + 1)\n",
    "    ax.axis('off')\n",
    "    other_utils.show_tensor_image(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac62782",
   "metadata": {},
   "source": [
    "**TODO**: `reverse_q` 函式幾乎完成了，但還有一些 `FIXME`s。每個 `FIXME` 可以是：\n",
    "\n",
    "* `x_t` - 隱向量（Latent Vector）\n",
    "* `t` - 目前的時間步長（Timestep）\n",
    "* `e_t` - 在當前時間步長預測的噪訊\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_q(x_t, t, e_t):\n",
    "    t = t.int()\n",
    "    pred_noise_coeff_t = pred_noise_coeff[t]\n",
    "    sqrt_a_inv_t = sqrt_a_inv[t]\n",
    "    u_t = sqrt_a_inv_t * (FIXME - pred_noise_coeff_t * FIXME)\n",
    "    if FIXME[0] == 0:  # All t values should be the same\n",
    "        return u_t  # Reverse diffusion complete!\n",
    "    else:\n",
    "        B_t = B[t - 1]  # Apply noise from the previos timestep\n",
    "        new_noise = torch.randn_like(x_t)\n",
    "        return u_t + torch.sqrt(B_t) * new_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf714",
   "metadata": {},
   "source": [
    "## 6.3 設定 U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674845f5",
   "metadata": {},
   "source": [
    "我們將使用之前使用過的相同 U-Net 架構："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, T, img_ch, img_size, down_chs=(64, 64, 128), t_embed_dim=8, c_embed_dim=10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = img_size // 4  # 2 ** (len(down_chs) - 1)\n",
    "        small_group_size = 8\n",
    "        big_group_size = 32\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = ResidualConvBlock(img_ch, down_chs[0], small_group_size)\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size)\n",
    "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size)\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
    "\n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(down_chs[2] * latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2] * latent_image_size**2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
    "        self.t_emb1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
    "        self.t_emb2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
    "        self.c_embed1 = EmbedBlock(c_embed_dim, up_chs[0])\n",
    "        self.c_embed2 = EmbedBlock(c_embed_dim, up_chs[1])\n",
    "\n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
    "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size)\n",
    "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size)\n",
    "\n",
    "        # Match output channels and one last concatenation\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
    "            nn.GroupNorm(small_group_size, up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_ch, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c, c_mask):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "\n",
    "        latent_vec = self.dense_emb(latent_vec)\n",
    "        t = t.float() / self.T  # Convert from [0, T] to [0, 1]\n",
    "        t = self.sinusoidaltime(t)\n",
    "        t_emb1 = self.t_emb1(t)\n",
    "        t_emb2 = self.t_emb2(t)\n",
    "\n",
    "        c = c * c_mask\n",
    "        c_emb1 = self.c_embed1(c)\n",
    "        c_emb2 = self.c_embed2(c)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(c_emb1 * up0 + t_emb1, down2)\n",
    "        up2 = self.up2(c_emb2 * up1 + t_emb2, down1)\n",
    "        return self.out(torch.cat((up2, down0), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81457189",
   "metadata": {},
   "source": [
    "**TODO**: 不幸的是，模組區塊的名稱被打亂了。可以根據函式的功能添加正確的模組名稱嗎？每種類型都有一個。\n",
    "\n",
    "* `GELUConvBlock`\n",
    "* `RearrangePoolBlock`\n",
    "* `DownBlock`\n",
    "* `UpBlock`\n",
    "* `SinusoidalPositionEmbedBlock`\n",
    "* `EmbedBlock`\n",
    "* `ResidualConvBlock`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, group_size):\n",
    "        super(DownBlock, self).__init__()\n",
    "        layers = [\n",
    "            GELUConvBlock(in_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            RearrangePoolBlock(out_chs, group_size),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Unflatten(1, (emb_dim, 1, 1)),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, group_size):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, out_ch, 3, 1, 1),\n",
    "            nn.GroupNorm(group_size, out_ch),\n",
    "            nn.GELU(),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, group_size):\n",
    "        super().__init__()\n",
    "        self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)\n",
    "        self.conv = GELUConvBlock(4 * in_chs, in_chs, group_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rearrange(x)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e16954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, group_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = GELUConvBlock(in_chs, out_chs, group_size)\n",
    "        self.conv2 = GELUConvBlock(out_chs, out_chs, group_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        out = x1 + x2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, group_size):\n",
    "        super(UpBlock, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c053d8d",
   "metadata": {},
   "source": [
    "\n",
    "現在所有的部分都已經定義好了，讓我們定義模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    T, IMG_CH, IMG_SIZE, down_chs=(64, 64, 128), t_embed_dim=8, c_embed_dim=N_CLASSES\n",
    ")\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model = torch.compile(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d2d0f",
   "metadata": {},
   "source": [
    "## 6.4 模型訓練\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ee902",
   "metadata": {},
   "source": [
    "**TODO**: 我們應該創建一個函式來隨機丟棄脈絡資訊（Context）。那個函式是什麼？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask(c, drop_prob):\n",
    "    c_hot = F.one_hot(c.to(torch.int64), num_classes=N_CLASSES).to(device)\n",
    "    c_mask = torch.FIXME(torch.ones_like(c_hot).float() - drop_prob).to(device)\n",
    "    return c_hot, c_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f820f1d",
   "metadata": {},
   "source": [
    "**TODO**: 接下來，讓我們定義損失函式。應該使用什麼類型的損失函式？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t, *model_args):\n",
    "    x_noisy, noise = q(x_0, t)\n",
    "    noise_pred = model(x_noisy, t, *model_args)\n",
    "    return F.FIXME(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb1a5b",
   "metadata": {},
   "source": [
    "這主要是為了驗證模型是否正確地進行訓練。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c345ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(model, img_ch, img_size, ncols, *model_args, axis_on=False):\n",
    "    # Noise to generate images from\n",
    "    x_t = torch.randn((1, img_ch, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    hidden_rows = T / ncols\n",
    "    plot_number = 1\n",
    "\n",
    "    # Go from T to 0 removing and adding noise until t = 0\n",
    "    for i in range(0, T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device).float()\n",
    "        e_t = model(x_t, t, *model_args)  # Predicted noise\n",
    "        x_t = reverse_q(x_t, t, e_t)\n",
    "        if i % hidden_rows == 0:\n",
    "            ax = plt.subplot(1, ncols+1, plot_number)\n",
    "            if not axis_on:\n",
    "                ax.axis('off')\n",
    "            other_utils.show_tensor_image(x_t.detach().cpu())\n",
    "            plot_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e5610",
   "metadata": {},
   "source": [
    "**TODO**: 時候到了，訓練模型吧！可以修復 `FIXME`s，並讓它運行嗎？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "preview_c = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        c_drop_prob = FIXME\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
    "        x = batch[0].to(device)\n",
    "        c_hot, c_mask = get_context_mask(FIXME, c_drop_prob)  # New\n",
    "        loss = get_loss(model, x, t, c_hot, c_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()} | C: {preview_c}\")\n",
    "            c_drop_prob = 0 # Do not drop context for preview\n",
    "            c_hot, c_mask = get_context_mask(torch.Tensor([preview_c]), c_drop_prob)\n",
    "            sample_images(model, IMG_CH, IMG_SIZE, ncols, c_hot, c_mask)\n",
    "            preview_c = (preview_c + 1) % N_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784eb180",
   "metadata": {},
   "source": [
    "## 6.5 採樣(Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8bedd0",
   "metadata": {},
   "source": [
    "這是拼圖的最後一塊。我們可以將生成器與分類器進行比較，但就目前而言，要達到 95% 的準確率將非常幸運。讓我們使用無分類器擴散引導（Classifier-Free Diffusion Guidance）來提高我們的機會。\n",
    "\n",
    "**TODO**: 在 `sample_w` 公式下面有一個 `FIXME`。可以記得在擴散過程中添加權重的公式嗎？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b10158",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_w(model, c, w):\n",
    "    input_size = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "    n_samples = len(c)\n",
    "    w = torch.tensor([w]).float()\n",
    "    w = w[:, None, None, None].to(device)  # Make w broadcastable\n",
    "    x_t = torch.randn(n_samples, *input_size).to(device)\n",
    "\n",
    "    # One c for each w\n",
    "    c = c.repeat(len(w), 1)\n",
    "\n",
    "    # Double the batch\n",
    "    c = c.repeat(2, 1)\n",
    "\n",
    "    # Don't drop context at test time\n",
    "    c_mask = torch.ones_like(c).to(device)\n",
    "    c_mask[n_samples:] = 0.0\n",
    "\n",
    "    x_t_store = []\n",
    "    for i in range(0, T)[::-1]:\n",
    "        # Duplicate t for each sample\n",
    "        t = torch.tensor([i]).to(device)\n",
    "        t = t.repeat(n_samples, 1, 1, 1)\n",
    "\n",
    "        # Double the batch\n",
    "        x_t = x_t.repeat(2, 1, 1, 1)\n",
    "        t = t.repeat(2, 1, 1, 1)\n",
    "\n",
    "        # Find weighted noise\n",
    "        e_t = model(x_t, t, c, c_mask)\n",
    "        e_t_keep_c = e_t[:n_samples]\n",
    "        e_t_drop_c = e_t[n_samples:]\n",
    "        e_t = FIXME\n",
    "\n",
    "        # Deduplicate batch for reverse diffusion\n",
    "        x_t = x_t[:n_samples]\n",
    "        t = t[:n_samples]\n",
    "        x_t = reverse_q(x_t, t, e_t)\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003d964",
   "metadata": {},
   "source": [
    "**TODO**: 讓我們試試看。嘗試多次運行下面的程式碼區塊。可以通過修改 `w` 使得數字始終被識別嗎？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3203fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "w = 0.0  # Change me\n",
    "c = torch.arange(N_CLASSES).to(device)\n",
    "c_drop_prob = 0 \n",
    "c_hot, c_mask = get_context_mask(c, c_drop_prob)\n",
    "\n",
    "x_0 = sample_w(model, c_hot, w)\n",
    "other_utils.to_image(make_grid(x_0.cpu(), nrow=N_CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2342b8",
   "metadata": {},
   "source": [
    "\n",
    "這對於自動評分器很重要。輸出形狀是否為 `[10, 1, 28, 28]`？如果是，你就準備好測試模型了！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f7807",
   "metadata": {},
   "source": [
    "## 6.6 運行評量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2c329",
   "metadata": {},
   "source": [
    "\n",
    "要評估你的模型，請運行以下兩個程式碼區塊。\n",
    "\n",
    "注意： `run_assessment` 假設你的模型名稱為 `model`，而你的擴散權重名稱為 `w`。如果你出於任何原因修改了這些變數名稱，請更新要傳遞給 `run_assessment` 的引數名稱。如果你的結果接近但不完全符合，試試修改上面的 `w`。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25834296",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_assessment(model, sample_w, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90006a68",
   "metadata": {},
   "source": [
    "## 6.7 生成證書"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4557a",
   "metadata": {},
   "source": [
    "\n",
    "如果你通過了評量，請返回課程頁面（如下所示），並點擊「評估任務」按鈕，這將為你生成課程證書。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fa579",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c0679",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
