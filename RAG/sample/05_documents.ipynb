{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34",
   "metadata": {
    "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 5:**Â è™•ç†å¤§å‹æ–‡ä»¶</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "åœ¨ä¸Šä¸€å€‹ notebook ä¸­ï¼Œæˆ‘å€‘å­¸ç¿’äº† Running State Chain å’ŒçŸ¥è­˜åº«ï¼åˆ°æœ€å¾Œï¼Œæˆ‘å€‘æ“æœ‰äº†é€²è¡Œç°¡å–®å°è©±ç®¡ç†å’Œè‡ªè¨‚çŸ¥è­˜è¿½è¹¤æ‰€éœ€çš„æ‰€æœ‰å·¥å…·ã€‚åœ¨é€™å€‹ notebook ä¸­ï¼Œæˆ‘å€‘å°‡æ¡ç”¨ç›¸åŒçš„æƒ³æ³•ä¸¦è½‰å‘å¤§å‹æ–‡ä»¶çš„é ˜åŸŸï¼Œè€ƒæ…®ç•¶æˆ‘å€‘å˜—è©¦å°‡å¤§å‹æª”æ¡ˆç´å…¥æˆ‘å€‘çš„ LLM è„ˆçµ¡è³‡è¨Š(Context)æ™‚æœƒé‡åˆ°ä»€éº¼æ¨£çš„å•é¡Œã€‚\n",
    "\n",
    "**å­¸ç¿’ç›®æ¨™ï¼š**\n",
    "\n",
    "\n",
    "-   ç†Ÿæ‚‰æ–‡ä»¶è¼‰å…¥å™¨ä»¥åŠå®ƒå€‘å¯èƒ½ç‚ºæ‚¨æä¾›çš„å¯¦ç”¨å·¥å…·é¡å‹ã€‚\n",
    "-   å­¸ç¿’å¦‚ä½•é€éåˆ†å¡Š(Chunking)æ–‡ä»¶ä¸¦é€æ­¥å»ºæ§‹çŸ¥è­˜åº«ä¾†è§£æè„ˆçµ¡è³‡è¨Š(Context)ç©ºé–“æœ‰é™çš„å¤§å‹æ–‡ä»¶ã€‚\n",
    "-   äº†è§£æ–‡ä»¶åˆ†å¡Š(document chunks)çš„æ¼¸é€²å¼é‡æ–°è„ˆçµ¡åŒ–(progressive recontextualization)ã€å¼·åˆ¶(coersion)å’Œæ•´åˆ(consolidation)å¦‚ä½•æ¥µå…¶æœ‰ç”¨ï¼Œä»¥åŠå®ƒæœƒé‡åˆ°è‡ªç„¶é™åˆ¶çš„åœ°æ–¹ã€‚\n",
    "\n",
    "**å€¼å¾—æ€è€ƒçš„å•é¡Œï¼š**\n",
    "\n",
    "\n",
    "-   æŸ¥çœ‹å¾æ‚¨çš„ ArxivParser å‡ºä¾†çš„åˆ†å¡Šï¼Œæ‚¨æœƒæ³¨æ„åˆ°æœ‰äº›åˆ†å¡Šæœ¬èº«æ²’æœ‰ä»€éº¼æ„ç¾©ï¼Œæˆ–è€…åœ¨è½‰æ›ç‚ºæ–‡å­—(Text)æ™‚å·²ç¶“å®Œå…¨æå£ã€‚æ˜¯å¦å€¼å¾—å°åˆ†å¡Šé€²è¡Œæ¸…ç†ï¼Ÿ\n",
    "-   è€ƒæ…®æ–‡ä»¶æ‘˜è¦å·¥ä½œæµç¨‹ï¼ˆæˆ–ä»»ä½•è™•ç†å¤§é‡æ–‡ä»¶åˆ†å¡Šåˆ—è¡¨çš„é¡ä¼¼å·¥ä½œæµç¨‹ï¼‰ï¼Œé€™æ‡‰è©²å¤šä¹…ç™¼ç”Ÿä¸€æ¬¡ï¼Œä»€éº¼æ™‚å€™æ˜¯åˆç†çš„ï¼Ÿ\n",
    "\n",
    "**ç’°å¢ƒè¨­ç½®ï¼š**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
   "metadata": {
    "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "# %pip install -qq arxiv pymupdf\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c33c07-19b8-4c81-8d99-30fa2b3b2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: 01-ai/yi-large. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-0528', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-llama-8b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-14b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-32b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-7b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-12b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-1b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-27b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-3-4b-it', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/shieldgemma-9b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='gotocompany/gemma-2-9b-cpt-sahabatai-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.3-8b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-guardian-3.0-8b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='igenius/colosseum_355b_instruct_16k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='igenius/italia_10b_instruct_16k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='marin/marin-8b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-4-scout-17b-16e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-multimodal-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mistral-medium-3-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-nemotron', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-24b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-3.1-24b-instruct-2503', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/embed-qa-4', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-content-safety', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-topic-control', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-4b-v1.1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-8b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-vl-8b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-ultra-253b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nv-embedqa-1b-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.2-nv-embedqa-1b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-base', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemoretriever-parse', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embed-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-nv-embed-v1'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedcode-7b-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-e5-v5', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nv-embedqa-mistral-7b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nvclip', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvquery/meta/llama-3.3-70b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-7b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen3-235b-a22b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwq-32b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='snowflake/arctic-embed-l', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-arctic-embed-l'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='speakleash/bielik-11b-v2.3-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='tiiuae/falcon3-7b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='utter-project/eurollm-9b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-creative-122b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8583a1-c10a-41da-8256-49520f868670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ä¸€éƒ¨åˆ†ï¼š**Â èˆ‡æ–‡ä»¶èŠå¤©\n",
    "\n",
    "\n",
    "é€™å€‹ notebook å°‡é–‹å§‹åœç¹ä½¿ç”¨ LLM èˆ‡æ–‡ä»¶èŠå¤©çš„è¼ƒé•·è¨è«–ä¸²ã€‚åœ¨ä¸€å€‹èŠå¤©æ¨¡å‹åœ¨å·¨å¤§çš„å…¬å…±è³‡æ–™å„²å­˜åº«ä¸Šè¨“ç·´ï¼Œè€Œåœ¨è‡ªè¨‚è³‡æ–™ä¸Šé‡æ–°è¨“ç·´å®ƒå€‘æˆæœ¬é«˜å¾—ä»¤äººæœ›è€Œå»æ­¥çš„ä¸–ç•Œä¸­ï¼Œè®“ LLM å°ä¸€çµ„ PDF ç”šè‡³ YouTube å½±ç‰‡é€²è¡Œåˆ†ææ¨ç†(Reasoning)çš„æƒ³æ³•é–‹å•Ÿäº†è¨±å¤šæ©Ÿæœƒï¼\n",
    "\n",
    "-   **æ‚¨çš„ LLM å¯ä»¥æ“æœ‰åŸºæ–¼äººé¡å¯è®€æ–‡ä»¶çš„å¯ä¿®æ”¹çŸ¥è­˜åº«ï¼Œ**Â é€™æ„å‘³è‘—æ‚¨å¯ä»¥ç›´æ¥æ§åˆ¶å®ƒæœ‰æ¬Šå­˜å–ä»€éº¼æ¨£çš„è³‡æ–™ï¼Œä¸¦å¯ä»¥æŒ‡ç¤ºå®ƒèˆ‡ä¹‹äº’å‹•ã€‚\n",
    "-   **æ‚¨çš„ LLM å¯ä»¥æ•´ç†ä¸¦ç›´æ¥å¾æ‚¨çš„æ–‡ä»¶é›†ä¸­æå–åƒè€ƒè³‡æ–™ã€‚**Â é€éå……åˆ†çš„æç¤º(Prompt)å·¥ç¨‹å’ŒæŒ‡ä»¤è·Ÿéš¨å…ˆé©—å‡è¨­ï¼Œæ‚¨å¯ä»¥å¼·åˆ¶æ‚¨çš„æ¨¡å‹åƒ…åŸºæ–¼æ‚¨æä¾›çš„ææ–™è¡Œå‹•ã€‚\n",
    "-   **æ‚¨çš„ LLM ç”šè‡³å¯èƒ½èˆ‡æ‚¨çš„æ–‡ä»¶äº’å‹•ï¼Œæ ¹æ“šéœ€è¦é€²è¡Œè‡ªå‹•ä¿®æ”¹ã€‚**Â é€™é–‹å•Ÿäº†è‡ªå‹•å…§å®¹ç²¾ç…‰(Refinement)å’Œç¶œåˆçµæœ(Synthesis)æ“ä½œçš„é€”å¾‘ï¼Œé€™å°‡åœ¨ç¨å¾Œæ¢è¨ã€‚\n",
    "\n",
    "åˆ—å‡ºä¸€äº›å¯èƒ½æ€§ç›¸ç•¶å®¹æ˜“ï¼Œå¾é‚£è£¡æ‚¨å¯ä»¥è®“æ‚¨çš„æƒ³åƒåŠ›è‡ªç”±é¦³é¨...ä½†æˆ‘å€‘é‚„æ²’æœ‰ç²å¾—åšé€™ä»¶äº‹çš„å·¥å…·ï¼Œå°å§ï¼Ÿ\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **æœ€å–®ç´”çš„(Naive)æ–¹æ³•ï¼šå¡«å……(Stuffing)æ‚¨çš„æ–‡ä»¶**\n",
    "\n",
    "å‡è¨­æ‚¨æœ‰ä¸€äº›æ–‡å­—(Text)æ–‡ä»¶ï¼ˆPDFã€éƒ¨è½æ ¼ç­‰ï¼‰ä¸¦æƒ³è©¢å•èˆ‡é€™äº›æ–‡ä»¶å…§å®¹ç›¸é—œçš„å•é¡Œã€‚æ‚¨å¯ä»¥å˜—è©¦çš„ä¸€ç¨®æ–¹æ³•æ¶‰åŠå–å¾—æ–‡ä»¶çš„è¡¨ç¤ºä¸¦å°‡å…¶å…¨éƒ¨é¤µçµ¦èŠå¤©æ¨¡å‹ï¼å¾æ–‡ä»¶è§’åº¦ä¾†çœ‹ï¼Œé€™è¢«ç¨±ç‚º[**æ–‡ä»¶å¡«å……(document stuffing)**](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/)ã€‚\n",
    "\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
    ">\n",
    "> ä¾†è‡ªÂ [**Stuff | LangChain**ğŸ¦œï¸ğŸ”—](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/)\n",
    "\n",
    "å¦‚æœæ‚¨çš„æ¨¡å‹è¶³å¤ å¼·å¤§ä¸”æ‚¨çš„æ–‡ä»¶è¶³å¤ çŸ­ï¼Œé€™å¯èƒ½æœƒé‹ä½œå¾—å¾ˆå¥½ï¼Œä½†ä¸æ‡‰è©²æœŸæœ›å®ƒå°æ•´å€‹æ–‡ä»¶é‹ä½œå¾—å¾ˆå¥½ã€‚è¨±å¤šç¾ä»£ LLM ç”±æ–¼è¨“ç·´é™åˆ¶è€Œåœ¨è™•ç†é•·è„ˆçµ¡è³‡è¨Š(Context)æ–¹é¢æœ‰é¡¯è‘—å›°é›£ã€‚ç¾åœ¨å¤§å‹æ¨¡å‹çš„æƒ¡åŒ–ä¸¦ä¸é‚£éº¼ç½é›£æ€§ï¼Œä½†ç„¡è«–æ‚¨ä½¿ç”¨å“ªå€‹æ¨¡å‹ï¼Œè‰¯å¥½çš„æŒ‡ä»¤è·Ÿéš¨å¯èƒ½æœƒå¾ˆå¿«å´©æ½°ï¼ˆå‡è¨­æ‚¨å­˜å–çš„æ˜¯åŸå§‹æ¨¡å‹ï¼‰ã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**æ‚¨éœ€è¦è§£æ±ºçš„æ–‡ä»¶åˆ†ææ¨ç†(Reasoning)çš„é—œéµå•é¡Œæ˜¯ï¼š**\n",
    "\n",
    "-   æˆ‘å€‘å¦‚ä½•å°‡æ–‡ä»¶åˆ†å‰²æˆå¯ä»¥é€²è¡Œåˆ†ææ¨ç†(Reasoning)çš„ç‰‡æ®µï¼Ÿ\n",
    "-   éš¨è‘—æ–‡ä»¶å¤§å°å’Œæ•¸é‡çš„å¢åŠ ï¼Œæˆ‘å€‘å¦‚ä½•æœ‰æ•ˆåœ°æ‰¾åˆ°ä¸¦è€ƒæ…®é€™äº›ç‰‡æ®µï¼Ÿ\n",
    "\n",
    "é€™é–€èª²ç¨‹å°‡æ¢ç´¢å¹¾ç¨®æ–¹æ³•ä¾†è§£æ±ºé€™äº›å•é¡Œï¼ŒåŒæ™‚ç¹¼çºŒå»ºæ§‹æˆ‘å€‘çš„ LLM æµç¨‹å”èª¿ç®¡ç†(Orchestration)æŠ€èƒ½ã€‚***é€™å€‹ notebook å°‡æ“´å±•æˆ‘å€‘ä¹‹å‰çš„ Running Chain æŠ€èƒ½ï¼Œç”¨æ–¼æ›´æ¼¸é€²çš„åˆ†ææ¨ç†(Reasoning)åˆ¶å®šï¼Œè€Œä¸‹ä¸€å€‹ notebook å°‡ä»‹ç´¹ä¸€äº›æ–°æŠ€è¡“ä¾†é©ç•¶åœ°è§£æ±ºå¤§è¦æ¨¡æª¢ç´¢(Retrieval)ã€‚***Â é€éé€™ç¨®é«”é©—ï¼Œæˆ‘å€‘å°‡ç¹¼çºŒåˆ©ç”¨å°–ç«¯çš„é–‹æºè§£æ±ºæ–¹æ¡ˆä¾†ä½¿æˆ‘å€‘çš„è§£æ±ºæ–¹æ¡ˆæ¨™æº–åŒ–å’Œå¯æ•´åˆã€‚\n",
    "\n",
    "èªªåˆ°é€™è£¡ï¼Œæ–‡ä»¶è¼‰å…¥æ¡†æ¶é ˜åŸŸæœ‰è¨±å¤šå¼·å¤§çš„é¸é …ï¼Œå…©å€‹ä¸»è¦åƒèˆ‡è€…å°‡åœ¨æ•´å€‹èª²ç¨‹ä¸­å‡ºç¾ï¼š\n",
    "\n",
    "-   [**LangChain**](https://python.langchain.com/docs/get_started/introduction)Â é€éä¸€èˆ¬åˆ†å¡Š(Chunking)ç­–ç•¥å’Œèˆ‡å…§åµŒæ¡†æ¶/æœå‹™çš„å¼·å¤§æ•´åˆï¼Œç‚ºå°‡ LLM é€£æ¥åˆ°æ‚¨è‡ªå·±çš„è³‡æ–™ä¾†æºæä¾›äº†ä¸€å€‹ç°¡å–®çš„æ¡†æ¶ã€‚é€™å€‹æ¡†æ¶æœ€åˆåœç¹å…¶å° LLM åŠŸèƒ½çš„å¼·å¤§ä¸€èˆ¬æ”¯æ´è€Œç™¼å±•ï¼Œé€™è¡¨æ˜å…¶ç©æ¥µçš„å„ªå‹¢æ›´æ¥è¿‘éˆ(Chain)æƒ³æ³•(abstractions)å’Œ Agent å”èª¿ã€‚\n",
    "\n",
    "-   [**LlamaIndex**](https://gpt-index.readthedocs.io/en/stable/)Â æ˜¯ä¸€å€‹ç”¨æ–¼ LLM æ‡‰ç”¨ç¨‹å¼çš„è³‡æ–™æ¡†æ¶ï¼Œç”¨æ–¼è¼¸å…¥(Intake)ã€çµæ§‹åŒ–å’Œå­˜å–ç§æœ‰æˆ–ç‰¹å®šé ˜åŸŸçš„è³‡æ–™ã€‚å®ƒå¾Œä¾†åˆ†æ”¯å‡ºä¾†åŒ…æ‹¬é¡ä¼¼æ–¼ LangChain çš„ä¸€èˆ¬ LLM åŠŸèƒ½ï¼Œä½†æˆªè‡³ç›®å‰ï¼Œå®ƒåœ¨è§£æ±º LLM çµ„ä»¶çš„æ–‡ä»¶æ–¹é¢ä»ç„¶æœ€å¼·ï¼Œå› ç‚ºå…¶åˆå§‹æƒ³æ³•(abstractions)åœç¹è©²å•é¡Œç‚ºä¸­å¿ƒã€‚\n",
    "\n",
    "å»ºè­°é–±è®€æ›´å¤šé—œæ–¼ LlamaIndex å’Œ LangChain çš„ç¨ç‰¹å„ªå‹¢ï¼Œä¸¦é¸æ“‡æœ€é©åˆæ‚¨çš„ã€‚ç”±æ–¼ LlamaIndex å¯ä»¥*èˆ‡*Â LangChain ä¸€èµ·ä½¿ç”¨ï¼Œæ¡†æ¶çš„ç¨ç‰¹åŠŸèƒ½å¯ä»¥åœ¨æ²’æœ‰å¤ªå¤šå•é¡Œçš„æƒ…æ³ä¸‹ä¸€èµ·åˆ©ç”¨ã€‚ç‚ºäº†ç°¡å–®èµ·è¦‹ï¼Œæˆ‘å€‘å°‡åœ¨é€™é–€èª²ç¨‹ä¸­å …æŒä½¿ç”¨ LangChainï¼Œä¸¦å°‡å…è¨±Â [**NVIDIA/GenerativeAIExamples å„²å­˜åº«**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/notebooks)Â ç‚ºæ„Ÿèˆˆè¶£çš„äººæ¢ç´¢æ›´æ·±å…¥çš„ LlamaIndex é¸é …ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310462b-f215-4d00-9d59-e613921bed0a",
   "metadata": {
    "id": "3310462b-f215-4d00-9d59-e613921bed0a"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬äºŒéƒ¨åˆ†ï¼š**Â è¼‰å…¥æ–‡ä»¶\n",
    "\n",
    "\n",
    "LangChain æä¾›å„ç¨®[æ–‡ä»¶è¼‰å…¥å™¨](https://python.langchain.com/docs/integrations/document_loaders)ä¾†ä¿ƒé€²å¾è¨±å¤šä¸åŒä¾†æºå’Œä½ç½®ï¼ˆæœ¬åœ°å„²å­˜ã€ç§æœ‰ s3 å„²å­˜æ¡¶ã€å…¬å…±ç¶²ç«™ã€è¨Šæ¯ API ç­‰ï¼‰è¼¸å…¥(Intake)å„ç¨®æ–‡ä»¶æ ¼å¼ï¼ˆHTMLã€PDFã€ç¨‹å¼ç¢¼ï¼‰ã€‚é€™äº›è¼‰å…¥å™¨æŸ¥è©¢æ‚¨çš„è³‡æ–™ä¾†æºä¸¦è¿”å›ä¸€å€‹Â `Document`Â ç‰©ä»¶ï¼Œè©²ç‰©ä»¶åŒ…å«å…§å®¹å’Œä¸­ä»‹è³‡æ–™(Metadata)ï¼Œé€šå¸¸æ¡ç”¨ç´”æ–‡å­—(Text)æˆ–å…¶ä»–äººé¡å¯è®€æ ¼å¼ã€‚å·²ç¶“æœ‰å¾ˆå¤šæ–‡ä»¶è¼‰å…¥å™¨æ§‹å»ºä¸¦æº–å‚™ä½¿ç”¨ï¼Œç¬¬ä¸€æ–¹ LangChain é¸é …åˆ—åœ¨[é€™è£¡](https://python.langchain.com/docs/integrations/document_loaders)ã€‚\n",
    "\n",
    "**åœ¨é€™å€‹ç¯„ä¾‹ä¸­ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ LangChain è¼‰å…¥å™¨ä¹‹ä¸€è¼‰å…¥æˆ‘å€‘é¸æ“‡çš„ç ”ç©¶è«–æ–‡ï¼š**\n",
    "\n",
    "-   [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file)ï¼šé©ç”¨æ–¼ä»»æ„æª”æ¡ˆçš„ä¸€èˆ¬æœ‰ç”¨æª”æ¡ˆè¼‰å…¥å™¨ï¼›å°æ‚¨çš„æ–‡ä»¶çµæ§‹ä¸åšå¤ªå¤šå‡è¨­ï¼Œé€šå¸¸æ˜¯è¶³å¤ çš„ã€‚\n",
    "\n",
    "-   [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv)ï¼šä¸€å€‹æ›´å°ˆæ¥­çš„æª”æ¡ˆè¼‰å…¥å™¨ï¼Œå¯ä»¥ç›´æ¥èˆ‡ Arxiv ä»‹é¢é€šè¨Šã€‚[åªæ˜¯è¨±å¤šç¯„ä¾‹ä¸­çš„ä¸€å€‹](https://python.langchain.com/docs/integrations/document_loaders)ï¼Œé€™å°‡å°æ‚¨çš„è³‡æ–™åšå‡ºä¸€äº›æ›´å¤šå‡è¨­ï¼Œä»¥ç”¢ç”Ÿæ›´å¥½çš„è§£æä¸¦è‡ªå‹•å¡«å……ä¸­ä»‹è³‡æ–™(Metadata)ï¼ˆç•¶æ‚¨æœ‰å¤šå€‹æ–‡ä»¶/æ ¼å¼æ™‚å¾ˆæœ‰ç”¨ï¼‰ã€‚\n",
    "\n",
    "å°æ–¼æˆ‘å€‘çš„ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼Œæˆ‘å€‘å°‡é è¨­ä½¿ç”¨Â `ArxivLoader`Â è¼‰å…¥Â [MRKL](https://arxiv.org/abs/2205.00445)Â æˆ–Â [ReAct](https://arxiv.org/abs/2210.03629)Â ç™¼è¡¨è«–æ–‡ä¸­çš„ä¸€å€‹ï¼Œå› ç‚ºæ‚¨åœ¨ç¹¼çºŒèŠå¤©æ¨¡å‹ç ”ç©¶åŠªåŠ›ä¸­å¯èƒ½æœƒåœ¨æŸå€‹æ™‚å€™é‡åˆ°å®ƒå€‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4382b61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1703112979370,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "b4382b61",
    "outputId": "d6e95b9b-97be-4984-a9fd-58a528091146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 436 ms, sys: 85 ms, total: 521 ms\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "## Loading in the file\n",
    "\n",
    "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
    "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()\n",
    "\n",
    "## More specialized loader, won't work for everything, but simple API and usually better results\n",
    "documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
    "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
    "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
    "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
    "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hw0SL--6cirp",
   "metadata": {
    "id": "hw0SL--6cirp"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "æˆ‘å€‘å¯ä»¥å¾æˆ‘å€‘çš„åŒ¯å…¥ä¸­çœ‹åˆ°ï¼Œé€™å€‹é€£æ¥å™¨(Connector)ç‚ºæˆ‘å€‘æä¾›äº†å…©å€‹ä¸åŒçµ„ä»¶çš„å­˜å–ï¼š\n",
    "\n",
    "-   `page_content`Â æ˜¯æ–‡ä»¶çš„å¯¦éš›ä¸»é«”ï¼Œæ¡ç”¨æŸç¨®äººé¡å¯è§£é‡‹çš„æ ¼å¼ã€‚\n",
    "\n",
    "-   `metadata`Â æ˜¯é€£æ¥å™¨(Connector)é€éå…¶è³‡æ–™ä¾†æºæä¾›çš„é—œæ–¼æ–‡ä»¶çš„ç›¸é—œè³‡è¨Šã€‚\n",
    "\n",
    "ä¸‹é¢ï¼Œæˆ‘å€‘å¯ä»¥æŸ¥çœ‹æ–‡ä»¶ä¸»é«”çš„é•·åº¦ä»¥äº†è§£è£¡é¢æœ‰ä»€éº¼ï¼Œä¸¦ä¸”å¯èƒ½æœƒæ³¨æ„åˆ°ä¸€å€‹é›£ä»¥è™•ç†çš„æ–‡ä»¶é•·åº¦ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1703113455184,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
    "outputId": "98b9ef68-c36b-478f-9bbb-1e45b2c49d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents Retrieved: 1\n",
      "Sample of Document 1 Content (Total Length: 89593):\n",
      "From Local to Global: A GraphRAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1â€ \n",
      "Ha Trinh1â€ \n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Dasha Metropolitansky1\n",
      "Robert Osazuwa Ness1\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
      "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
      "â€ These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as â€œWhat are the main themes in the dataset?â€, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scal\n"
     ]
    }
   ],
   "source": [
    "## Printing out a sample of the content\n",
    "print(\"Number of Documents Retrieved:\", len(documents))\n",
    "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
    "print(documents[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JjUK2ZSd0HL",
   "metadata": {
    "id": "1JjUK2ZSd0HL"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸­ä»‹è³‡æ–™(Metadata)å°‡æ›´ä¿å®ˆåœ°èª¿æ•´å¤§å°ï¼Œä»¥è‡³æ–¼æˆç‚ºæ‚¨å–œæ„›çš„èŠå¤©æ¨¡å‹çš„å¯è¡Œè„ˆçµ¡è³‡è¨Š(Context)çµ„ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Py2lbRXlcX81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1703112982386,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Py2lbRXlcX81",
    "outputId": "07197dd4-1609-4ecf-ae54-cf6ef3d25458"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-19'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Metropolitansky, Robert Osazuwa Ness, Jonathan Larson'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of retrieval-augmented generation (RAG) to retrieve relevant\\ninformation from an external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source enables large language models\\n(LLMs) to answer questions over private and/or previously unseen </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the main themes in the dataset?\", since this is\\ninherently a query-focused summarization (QFS) task, rather than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">closely related entities. Given a question, each community summary is\\nused to generate a partial response, before </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all partial responses are again\\nsummarized in a final response to the user. For a class of global </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-02-19'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha \u001b[0m\n",
       "\u001b[32mMetropolitansky, Robert Osazuwa Ness, Jonathan Larson'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to retrieve relevant\\ninformation from an external \u001b[0m\n",
       "\u001b[32mknowledge source enables large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to answer questions over private and/or previously unseen \u001b[0m\n",
       "\u001b[32mdocument\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are\u001b[0m\n",
       "\u001b[32mthe main themes in the dataset?\", since this is\\ninherently a query-focused summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task, rather than \u001b[0m\n",
       "\u001b[32man explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by \u001b[0m\n",
       "\u001b[32mtypical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based \u001b[0m\n",
       "\u001b[32mapproach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions \u001b[0m\n",
       "\u001b[32mand the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive \u001b[0m\n",
       "\u001b[32man entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of \u001b[0m\n",
       "\u001b[32mclosely related entities. Given a question, each community summary is\\nused to generate a partial response, before \u001b[0m\n",
       "\u001b[32mall partial responses are again\\nsummarized in a final response to the user. For a class of global \u001b[0m\n",
       "\u001b[32msensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial \u001b[0m\n",
       "\u001b[32mimprovements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046ea74-0b81-400e-8364-449f421d2add",
   "metadata": {
    "id": "7046ea74-0b81-400e-8364-449f421d2add"
   },
   "source": [
    "<br>\n",
    "\n",
    "é›–ç„¶æ¥å—ä¸­ä»‹è³‡æ–™(Metadata)æ ¼å¼ä¸¦å®Œå…¨å¿½ç•¥ä¸»é«”å¯èƒ½å¾ˆèª˜äººï¼Œä½†æœ‰ä¸€äº›é—œéµçš„é¸æ“‡åŠŸèƒ½ç„¡æ³•åœ¨ä¸æ·±å…¥å®Œæ•´æ–‡å­—(Text)çš„æƒ…æ³ä¸‹å¯¦ç¾ï¼š\n",
    "\n",
    "-   **ä¸­ä»‹è³‡æ–™(Metadata)ä¸è¢«ä¿è­‰ã€‚**Â åœ¨Â `arxiv`Â çš„æƒ…æ³ä¸‹ï¼Œè«–æ–‡æ‘˜è¦ã€æ¨™é¡Œã€ä½œè€…å’Œæ—¥æœŸæ˜¯æäº¤çš„å¿…è¦çµ„ä»¶ï¼Œæ‰€ä»¥èƒ½å¤ æŸ¥è©¢å®ƒå€‘ä¸¦ä¸ä»¤äººé©šè¨ã€‚ä½†å°æ–¼ä»»æ„ PDF æˆ–ç¶²é ï¼Œæƒ…æ³ä¸ä¸€å®šå¦‚æ­¤ã€‚\n",
    "\n",
    "-   **Agent å°‡ç„¡æ³•æ›´æ·±å…¥åœ°äº†è§£æ–‡ä»¶å…§å®¹ã€‚**Â æ‘˜è¦å¾ˆå¥½çŸ¥é“ä¸¦å¯ä»¥æŒ‰åŸæ¨£ä½¿ç”¨ï¼Œä½†ä¸æä¾›èˆ‡ä¸»é«”åœ¨ä»»ä½•å®¹é‡ä¸Šäº’å‹•çš„ç›´æ¥è·¯å¾‘ï¼ˆè‡³å°‘å¾æˆ‘å€‘å­¸åˆ°çš„å…§å®¹ä¾†çœ‹ä¸æ˜¯é€™æ¨£ï¼‰ã€‚\n",
    "\n",
    "-   **Agent ä»ç„¶ç„¡æ³•åŒæ™‚å°å¤ªå¤šæ–‡ä»¶é€²è¡Œåˆ†ææ¨ç†(Reasoning)ã€‚**Â ä¹Ÿè¨±åœ¨ MRKL/ReAct ç¯„ä¾‹ä¸­ï¼Œæ‚¨å¯ä»¥å°‡é€™å…©å€‹æ‘˜è¦åˆä½µç‚ºä¸€å€‹è„ˆçµ¡è³‡è¨Š(Context)ä¸¦è©¢å•ä¸€äº›å•é¡Œã€‚ä½†æ˜¯ç•¶æ‚¨éœ€è¦åŒæ™‚èˆ‡ 5 å€‹æ–‡ä»¶äº’å‹•æ™‚æœƒç™¼ç”Ÿä»€éº¼ï¼Ÿæ•´å€‹ç›®éŒ„å‘¢ï¼Ÿå¾ˆå¿«ï¼Œæ‚¨æœƒæ³¨æ„åˆ°æ‚¨çš„è„ˆçµ¡è³‡è¨Š(Context)è¦–çª—å°‡è¢«è³‡è¨Šéè¼‰ï¼Œåƒ…åƒ…æ˜¯ç‚ºäº†ç¸½çµæˆ–ç”šè‡³åˆ—å‡ºæ‚¨æ„Ÿèˆˆè¶£çš„æ–‡ä»¶ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0449e4",
   "metadata": {
    "id": "4e0449e4"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ä¸‰éƒ¨åˆ†ï¼š**Â è½‰æ›æ–‡ä»¶\n",
    "\n",
    "\n",
    "ä¸€æ—¦æ–‡ä»¶è¢«è¼‰å…¥ï¼Œå¦‚æœæˆ‘å€‘æ‰“ç®—å°‡å®ƒå€‘ä½œç‚ºè„ˆçµ¡è³‡è¨Š(Context)å‚³éåˆ°æˆ‘å€‘çš„ LLM ä¸­ï¼Œå®ƒå€‘é€šå¸¸éœ€è¦è¢«è½‰æ›ã€‚ä¸€ç¨®è½‰æ›æ–¹æ³•è¢«ç¨±ç‚º**åˆ†å¡Š(chunking)**ï¼Œå®ƒå°‡å¤§å¡Šå…§å®¹åˆ†è§£ç‚ºè¼ƒå°çš„ç‰‡æ®µã€‚é€™ç¨®æŠ€è¡“å¾ˆæœ‰åƒ¹å€¼ï¼Œå› ç‚ºå®ƒæœ‰åŠ©æ–¼[æœ€ä½³åŒ–å¾å‘é‡è³‡æ–™åº«è¿”å›å…§å®¹çš„ç›¸é—œæ€§](https://www.pinecone.io/learn/chunking-strategies/)ã€‚\n",
    "\n",
    "LangChain æä¾›[å„ç¨®æ–‡ä»¶è½‰æ›å™¨](https://python.langchain.com/docs/integrations/document_transformers/)ï¼Œå…¶ä¸­æˆ‘å€‘å°‡ä½¿ç”¨Â [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)ã€‚é€™å€‹é¸é …å°‡å…è¨±æˆ‘å€‘åŸºæ–¼è‡ªç„¶åœæ­¢é»çš„åå¥½ä¾†åˆ†å‰²æˆ‘å€‘çš„æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1703112527056,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
    "outputId": "a4e666e5-5a5c-413b-f5a4-acca742d80d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "## Some nice custom preprocessing\n",
    "# documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "# def include_doc(doc):\n",
    "#     ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
    "#     string = doc.page_content\n",
    "#     if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
    "print(len(docs_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1703112530925,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
    "outputId": "1cf24605-65bb-40a2-e7aa-e2d9a8fb6382"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Local to Global: A GraphRAG Approach to\n",
      "Query-Focused Summarization\n",
      "Darren Edge1â€ \n",
      "Ha Trinh1â€ \n",
      "Newman Cheng2\n",
      "Joshua Bradley2\n",
      "Alex Chao3\n",
      "Apurva Mody3\n",
      "Steven Truitt2\n",
      "Dasha Metropolitansky1\n",
      "Robert Osazuwa Ness1\n",
      "Jonathan Larson1\n",
      "1Microsoft Research\n",
      "2Microsoft Strategic Missions and Technologies\n",
      "3Microsoft Office of the CTO\n",
      "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
      "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
      "â€ These authors contributed equally to this work\n",
      "Abstract\n",
      "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
      "tion from an external knowledge source enables large language models (LLMs)\n",
      "to answer questions over private and/or previously unseen document collections.\n",
      "However, RAG fails on global questions directed at an entire text corpus, such\n",
      "as â€œWhat are the main themes in the dataset?â€, since this is inherently a query-\n",
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scale to the quantities of text indexed by typ-\n",
      "ical RAG systems. To combine the strengths of these contrasting methods, we\n",
      "propose GraphRAG, a graph-based approach to question answering over private\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propose GraphRAG, a graph-based approach to question answering over private\n",
      "text corpora that scales with both the generality of user questions and the quantity\n",
      "of source text. Our approach uses an LLM to build a graph index in two stages:\n",
      "first, to derive an entity knowledge graph from the source documents, then to pre-\n",
      "generate community summaries for all groups of closely related entities. Given a\n",
      "question, each community summary is used to generate a partial response, before\n",
      "all partial responses are again summarized in a final response to the user. For a\n",
      "class of global sensemaking questions over datasets in the 1 million token range,\n",
      "we show that GraphRAG leads to substantial improvements over a conventional\n",
      "RAG baseline for both the comprehensiveness and diversity of generated answers.\n",
      "1\n",
      "Introduction\n",
      "Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\n",
      "LLMs to answer queries based on data that is too large to contain in a language modelâ€™s context\n",
      "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n",
      "at once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to\n",
      "a large external corpus of text records and retrieves a subset of records that are individually relevant\n",
      "to the query and collectively small enough to fit into the context window of the LLM. The LLM then\n",
      "Preprint. Under review.\n",
      "arXiv:2404.16130v2  [cs.CL]  19 Feb 2025\n",
      "generates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang,\n",
      "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call\n",
      "vector RAG, works well for queries that can be answered with information localized within a small\n",
      "set of records. However, vector RAG approaches do not support sensemaking queries, meaning\n",
      "queries that require global understanding of the entire dataset, such as â€What are the key trends in\n",
      "how scientific discoveries are influenced by interdisciplinary research over the past decade?â€\n",
      "Sensemaking tasks require reasoning over â€œconnections (which can be among people, places, and\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from few-shot exemplars specialized to those domains.\n",
      "The LLM can also be prompted to extract claims about detected entities. Claims are important\n",
      "factual statements about entities, such as dates, events, and interactions with other entities. As\n",
      "with entities and relationships, in-context learning exemplars can provide domain-specific guidance.\n",
      "Claim descriptions extracted from the example tetx chunk are as follows:\n",
      "â€¢ NeoChipâ€™s shares surged during their first week of trading on the NewTech Exchange.\n",
      "â€¢ NeoChip debuted as a publicly listed company on the NewTech Exchange.\n",
      "â€¢ Quantum Systems acquired NeoChip in 2016 and held ownership until NeoChip went pub-\n",
      "lic.\n",
      "See Appendix A for prompts and details on our implementation of entity and claim extraction.\n",
      "3.1.3\n",
      "Entities & Relationships â†’Knowledge Graph\n",
      "The use of an LLM to extract entities, relationships, and claims is a form of abstractive summariza-\n",
      "tion â€“ these are meaningful summaries of concepts that, in the case of relationships and claims, may\n",
      "not be explicitly stated in the text. The entity/relationship/claim extraction processes creates mul-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.8\n",
      "-3.68\n",
      "0.003\n",
      "C1\n",
      "TS\n",
      "47.92\n",
      "52.08\n",
      "-2.41\n",
      "0.126\n",
      "46.64\n",
      "53.36\n",
      "-2.91\n",
      "0.04\n",
      "C2\n",
      "TS\n",
      "48.8\n",
      "51.2\n",
      "-2.23\n",
      "0.179\n",
      "48.32\n",
      "51.68\n",
      "-2.12\n",
      "0.179\n",
      "C3\n",
      "TS\n",
      "48.08\n",
      "51.92\n",
      "-2.23\n",
      "0.179\n",
      "48.32\n",
      "51.68\n",
      "-2.56\n",
      "0.074\n",
      "C0\n",
      "SS\n",
      "35.12\n",
      "64.88\n",
      "-6.17\n",
      "<0.001\n",
      "41.44\n",
      "58.56\n",
      "-4.82\n",
      "<0.001\n",
      "C1\n",
      "SS\n",
      "40.32\n",
      "59.68\n",
      "-4.83\n",
      "<0.001\n",
      "45.2\n",
      "54.8\n",
      "-3.19\n",
      "0.017\n",
      "C2\n",
      "SS\n",
      "40.4\n",
      "59.6\n",
      "-4.67\n",
      "<0.001\n",
      "44.88\n",
      "55.12\n",
      "-3.65\n",
      "0.003\n",
      "C3\n",
      "SS\n",
      "40.48\n",
      "59.52\n",
      "-4.69\n",
      "<0.001\n",
      "45.6\n",
      "54.4\n",
      "-2.86\n",
      "0.043\n",
      "TS\n",
      "SS\n",
      "43.6\n",
      "56.4\n",
      "-3.96\n",
      "<0.001\n",
      "46\n",
      "54\n",
      "-2.68\n",
      "0.066\n",
      "C0\n",
      "C1\n",
      "46.96\n",
      "53.04\n",
      "-2.87\n",
      "0.037\n",
      "47.6\n",
      "52.4\n",
      "-2.17\n",
      "0.179\n",
      "C0\n",
      "C2\n",
      "48.4\n",
      "51.6\n",
      "-2.06\n",
      "0.197\n",
      "48.48\n",
      "51.52\n",
      "-1.61\n",
      "0.321\n",
      "C1\n",
      "C2\n",
      "49.84\n",
      "50.16\n",
      "-1\n",
      "0.952\n",
      "49.28\n",
      "50.72\n",
      "-1.6\n",
      "0.321\n",
      "C0\n",
      "C3\n",
      "48.4\n",
      "51.6\n",
      "-1.8\n",
      "0.29\n",
      "47.2\n",
      "52.8\n",
      "-2.62\n",
      "0.071\n",
      "C1\n",
      "C3\n",
      "49.76\n",
      "50.24\n",
      "0\n",
      "1\n",
      "48.8\n",
      "51.2\n",
      "-1.29\n",
      "0.321\n",
      "C2\n",
      "C3\n",
      "50\n",
      "50\n",
      "0\n",
      "1\n",
      "48.8\n",
      "51.2\n",
      "-1.84\n",
      "0.262\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in (0, 1, 2, 15, -1):\n",
    "    pprint(f\"[Document {i}]\")\n",
    "    print(docs_split[i].page_content)\n",
    "    pprint(\"=\"*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc",
   "metadata": {
    "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "æˆ‘å€‘çš„åˆ†å¡Š(Chunking)æ–¹æ³•ç›¸ç•¶æœ€å–®ç´”çš„(naive)ï¼Œä½†çªå‡ºäº†ç‚ºæˆ‘å€‘çš„æ‡‰ç”¨ç¨‹å¼è‡³å°‘è®“æŸäº›æ±è¥¿é‹ä½œçš„å®¹æ˜“æ€§ã€‚æˆ‘å€‘åŠªåŠ›ä¿æŒåˆ†å¡Šå¤§å°è¼ƒå°ï¼Œä»¥ä¾¿æˆ‘å€‘çš„æ¨¡å‹èƒ½å¤ æœ‰æ•ˆåœ°å°‡å…¶ä½œç‚ºè„ˆçµ¡è³‡è¨Š(Context)ä½¿ç”¨ï¼Œä½†æˆ‘å€‘å¦‚ä½•å°æ‰€æœ‰é€™äº›ç‰‡æ®µé€²è¡Œåˆ†ææ¨ç†(Reasoning)ï¼Ÿ\n",
    "\n",
    "**ç•¶ç‚ºä»»æ„æ–‡ä»¶é›†æ“´å±•å’Œæœ€ä½³åŒ–é€™ç¨®æ–¹æ³•æ™‚ï¼Œä¸€äº›æ½›åœ¨é¸é …åŒ…æ‹¬ï¼š**\n",
    "\n",
    "-   è­˜åˆ¥é‚è¼¯ä¸­æ–·æˆ–ç¶œåˆçµæœ(Synthesis)æŠ€è¡“ï¼ˆæ‰‹å‹•ã€è‡ªå‹•ã€LLM è¼”åŠ©ç­‰ï¼‰ã€‚\n",
    "-   æ—¨åœ¨æ§‹å»ºå¯Œå«ç¨ç‰¹å’Œç›¸é—œè³‡è¨Šçš„åˆ†å¡Šï¼Œé¿å…å†—é¤˜ä»¥æœ€å¤§åŒ–è³‡æ–™åº«å¯¦ç”¨æ€§ã€‚\n",
    "-   è‡ªè¨‚åˆ†å¡Šä»¥é©æ‡‰æ–‡ä»¶çš„æ€§è³ªï¼Œç¢ºä¿åˆ†å¡Šåœ¨è„ˆçµ¡è³‡è¨Š(Context)ä¸Šç›¸é—œä¸”é€£è²«ã€‚\n",
    "-   åœ¨æ¯å€‹åˆ†å¡Šä¸­åŒ…å«é—œéµæ¦‚å¿µã€é—œéµå­—æˆ–ä¸­ä»‹è³‡æ–™(Metadata)ç‰‡æ®µï¼Œä»¥æ”¹å–„è³‡æ–™åº«ä¸­çš„å¯æœå°‹æ€§å’Œç›¸é—œæ€§ã€‚\n",
    "-   æŒçºŒè©•ä¼°åˆ†å¡Šæ•ˆæœï¼Œä¸¦æº–å‚™èª¿æ•´ç­–ç•¥ä»¥åœ¨å¤§å°å’Œå…§å®¹è±å¯Œæ€§ä¹‹é–“é”åˆ°æœ€ä½³å¹³è¡¡ã€‚\n",
    "-   è€ƒæ…®éšå±¤çµæ§‹(Hierarchy)ç³»çµ±ï¼ˆéš±å¼ç”Ÿæˆæˆ–æ˜ç¢ºæŒ‡å®šï¼‰ä»¥æ”¹å–„æª¢ç´¢(Retrieval)å˜—è©¦ã€‚\n",
    "    -   å¦‚æœæ„Ÿèˆˆè¶£ï¼Œè«‹æŸ¥çœ‹Â [**LlamaIndex ç´¢å¼•æŒ‡å—ä¸­çš„æ¨¹ç‹€çµæ§‹**](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index)Â ä½œç‚ºèµ·é»ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-0QApYgNbyJD",
   "metadata": {
    "id": "-0QApYgNbyJD"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬å››éƒ¨åˆ†ï¼š[ç·´ç¿’]**Â ç²¾ç…‰(Refinement)æ‘˜è¦\n",
    "\n",
    "\n",
    "ç‚ºäº†è‡ªå‹•å°å¤§å‹æ–‡ä»¶é€²è¡Œåˆ†ææ¨ç†(Reasoning)ï¼Œä¸€å€‹æ½›åœ¨çš„æƒ³æ³•å¯èƒ½æ˜¯ä½¿ç”¨ LLM å‰µå»ºå¯†é›†æ‘˜è¦æˆ–çŸ¥è­˜åº«ã€‚é¡ä¼¼æ–¼æˆ‘å€‘åœ¨ä¸Šä¸€å€‹ notebook ä¸­é€éæ§½å¡«å……ç¶­è­·å°è©±çš„é‹è¡Œæ­·å²ç´€éŒ„(History)ï¼Œä¿æŒæ•´å€‹æ–‡ä»¶çš„é‹è¡Œæ­·å²ç´€éŒ„(History)æœ‰ä»€éº¼å•é¡Œå—ï¼Ÿ\n",
    "\n",
    "åœ¨é€™ä¸€ç¯€ä¸­ï¼Œæˆ‘å€‘å°ˆæ³¨æ–¼ LLM çš„ä¸€å€‹ä»¤äººèˆˆå¥®çš„æ‡‰ç”¨ï¼š**è‡ªå‹•ç²¾ç…‰(Refinement)ã€å¼·åˆ¶å’Œå¤§é‡æ•´åˆ(Aggregate)è³‡æ–™**ã€‚å…·é«”ä¾†èªªï¼Œæˆ‘å€‘å°‡å¯¦ä½œä¸€å€‹ç°¡å–®ä½†æœ‰ç”¨çš„å¯é‹è¡Œç‰©ä»¶(Runnable)ï¼Œå®ƒä½¿ç”¨ while è¿´åœˆå’Œ Running State Chain åˆ¶å®šä¾†ç¸½çµä¸€çµ„æ–‡ä»¶åˆ†å¡Šã€‚é€™å€‹éç¨‹é€šå¸¸è¢«ç¨±ç‚ºÂ [**ã€Œæ–‡ä»¶ç²¾ç…‰(document refinement)ã€**](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/)ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šé¡ä¼¼æ–¼æˆ‘å€‘ä¹‹å‰ä»¥å°è©±ç‚ºä¸­å¿ƒçš„æ§½å¡«å……ç·´ç¿’ï¼›å”¯ä¸€çš„å€åˆ¥æ˜¯ç¾åœ¨æˆ‘å€‘è™•ç†çš„æ˜¯å¤§å‹æ–‡ä»¶è€Œä¸æ˜¯ä¸æ–·å¢é•·çš„èŠå¤©æ­·å²ç´€éŒ„(History)ã€‚\n",
    "\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1J2XR8Cc8YSkVJMiJCknMkgA02mBT8riZ\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>\n",
    ">\n",
    "> From [**Refine | LangChain**ğŸ¦œï¸ğŸ”—](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **DocumentSummaryBase æ¨¡å‹**\n",
    "\n",
    "\n",
    "å°±åƒä¸Šä¸€å€‹ notebook ä¸­çš„Â `KnowledgeBase`Â é¡åˆ¥ä¸€æ¨£ï¼Œæˆ‘å€‘å¯ä»¥å‰µå»ºä¸€å€‹Â `DocumentSummaryBase`Â çµæ§‹ï¼Œè¨­è¨ˆç”¨æ–¼å°è£æ–‡ä»¶çš„ç²¾é«“ã€‚ä¸‹é¢çš„ä¸€å€‹å°‡ä½¿ç”¨Â `running_summary`Â æ¬„ä½ä¾†æŸ¥è©¢æ¨¡å‹ä»¥ç²å¾—æœ€çµ‚æ‘˜è¦ï¼ŒåŒæ™‚å˜—è©¦ä½¿ç”¨Â `main_ideas`Â å’ŒÂ `loose_ends`Â æ¬„ä½ä½œç‚ºç“¶é ¸ï¼Œä»¥é˜²æ­¢é‹è¡Œæ‘˜è¦ç§»å‹•å¾—å¤ªå¿«ã€‚é€™æ˜¯æˆ‘å€‘å¿…é ˆé€éæç¤º(Prompt)å·¥ç¨‹å¼·åˆ¶åŸ·è¡Œçš„æ±è¥¿ï¼Œæ‰€ä»¥ä¹Ÿæä¾›äº†Â `summary_prompt`ï¼Œå®ƒé¡¯ç¤ºäº†é€™äº›è³‡è¨Šå°‡å¦‚ä½•ä½¿ç”¨ã€‚è«‹æ ¹æ“šéœ€è¦éš¨æ„ä¿®æ”¹å®ƒï¼Œä»¥ä½¿å…¶é©ç”¨æ–¼æ‚¨é¸æ“‡çš„æ¨¡å‹ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gE8y2JvLvZ5T",
   "metadata": {
    "id": "gE8y2JvLvZ5T"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class DocumentSummaryBase(BaseModel):\n",
    "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
    "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
    "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
    "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
    "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
    "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
    "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
    "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LkjfpOAvlEd",
   "metadata": {
    "id": "7LkjfpOAvlEd"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "æˆ‘å€‘ä¹Ÿå°‡åˆ©ç”¨é€™å€‹æ©Ÿæœƒå¾ä¸Šä¸€å€‹ notebook ä¸­å¸¶å›Â `RExtract`Â å‡½å¼(function)ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "khRhVghHxBaz",
   "metadata": {
    "id": "khRhVghHxBaz"
   },
   "outputs": [],
   "source": [
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFtME_s4PRoW",
   "metadata": {
    "id": "oFtME_s4PRoW"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "è€ƒæ…®åˆ°é€™ä¸€é»ï¼Œä»¥ä¸‹ç¨‹å¼ç¢¼åœ¨ for è¿´åœˆä¸­invoke Running State Chain ä¾†ç–Šä»£(Iterate)æ‚¨çš„æ–‡ä»¶ï¼å”¯ä¸€å¿…è¦çš„ä¿®æ”¹æ‡‰è©²æ˜¯Â `parse_chain`Â å¯¦ä½œï¼Œå®ƒæ‡‰è©²é€éä¾†è‡ªä¸Šä¸€å€‹ notebook çš„é©ç•¶é…ç½®çš„Â `RExtract`Â éˆ(Chain)å‚³éç‹€æ…‹ã€‚åœ¨æ­¤ä¹‹å¾Œï¼Œç³»çµ±æ‡‰è©²é‹ä½œå¾—ç›¸ç•¶å¥½ï¼Œä»¥ç¶­è­·æ–‡ä»¶çš„é‹è¡Œæ‘˜è¦ï¼ˆå„˜ç®¡æ ¹æ“šä½¿ç”¨çš„æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦å°æç¤º(Prompt)é€²è¡Œä¸€äº›èª¿æ•´ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6sODIfHUgz6m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79192,
     "status": "ok",
     "timestamp": 1703112894722,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6sODIfHUgz6m",
    "outputId": "7b5aee70-078b-458e-d2a7-e8601b789fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered 15 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">traded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in-context learning.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using few-shot exemplars for in-context learning.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What types of global sensemaking questions does GraphRAG aim to answer?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What is the impact of the context window size on the performance of GraphRAG?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text \u001b[0m\n",
       "\u001b[32mcorpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is \u001b[0m\n",
       "\u001b[32mprompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power \u001b[0m\n",
       "\u001b[32mprocessor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly \u001b[0m\n",
       "\u001b[32mtraded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for \u001b[0m\n",
       "\u001b[32min-context learning.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored \u001b[0m\n",
       "\u001b[32musing few-shot exemplars for in-context learning.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What types of global sensemaking questions does GraphRAG aim to answer?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What is the impact of the context window size on the performance of GraphRAG?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latest_summary = \"\"\n",
    "\n",
    "## TODO: Use the techniques from the previous notebook to complete the exercise\n",
    "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
    "    '''\n",
    "    Exercise: Create a chain that summarizes\n",
    "    '''\n",
    "    ###########################################################################################\n",
    "    ## START TODO:\n",
    "\n",
    "    def summarize_docs(docs):        \n",
    "        ## TODO: Initialize the parse_chain appropriately; should include an RExtract instance.\n",
    "        ## HINT: You can get a class using the <object>.__class__ attribute...\n",
    "        # parse_chain = RunnableAssign({'info_base' : (lambda x: None)})\n",
    "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
    "        ## TODO: Initialize a valid starting state. Should be similar to notebook 4\n",
    "        # state = {}\n",
    "        state = {'info_base' : knowledge}\n",
    "\n",
    "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            ## TODO: Update the state as appropriate using your parse_chain component\n",
    "            state['input'] = doc.page_content\n",
    "            state = parse_chain.invoke(state)\n",
    "\n",
    "            assert 'info_base' in state \n",
    "            if verbose:\n",
    "                print(f\"Considered {i+1} documents\")\n",
    "                pprint(state['info_base'])\n",
    "                latest_summary = state['info_base']\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        return state['info_base']\n",
    "        \n",
    "    ## END TODO\n",
    "    ###########################################################################################\n",
    "    \n",
    "    return RunnableLambda(summarize_docs)\n",
    "\n",
    "# instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_llm = instruct_model | StrOutputParser()\n",
    "\n",
    "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
    "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
    "summary = summarizer.invoke(docs_split[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">traded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in-context learning.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using few-shot exemplars for in-context learning.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What types of global sensemaking questions does GraphRAG aim to answer?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What is the impact of the context window size on the performance of GraphRAG?'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'GraphRAG, a graph-based approach for question answering, utilizes an LLM over private text \u001b[0m\n",
       "\u001b[32mcorpora. It outperforms conventional RAG in global sensemaking queries when using GPT-4 as the LLM. The LLM is \u001b[0m\n",
       "\u001b[32mprompted to extract entities, relationships, and short descriptions from text chunks. NeoChip, a low-power \u001b[0m\n",
       "\u001b[32mprocessor firm for wearables and IoT devices, was previously owned by Quantum Systems until it became publicly \u001b[0m\n",
       "\u001b[32mtraded in 2016. The LLM can extract domain-specific entities and relationships based on few-shot exemplars for \u001b[0m\n",
       "\u001b[32min-context learning.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG, a graph-based approach, uses LLM to build a graph index for question answering.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG outperforms conventional RAG for global sensemaking queries using GPT-4 as the LLM.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The LLM extracts entities, relationships, and short descriptions from text chunks, which can be tailored \u001b[0m\n",
       "\u001b[32musing few-shot exemplars for in-context learning.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG significantly improve over conventional RAG for global sensemaking questions?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What types of global sensemaking questions does GraphRAG aim to answer?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What is the impact of the context window size on the performance of GraphRAG?'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(latest_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKtoLf6DPv4Z",
   "metadata": {
    "id": "tKtoLf6DPv4Z"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬äº”éƒ¨åˆ†ï¼š**Â ç¶œåˆçµæœ(Synthesis)è³‡æ–™è™•ç†\n",
    "\n",
    "ç•¶æˆ‘å€‘çµæŸä½¿ç”¨ LLM é€²è¡Œæ–‡ä»¶æ‘˜è¦çš„æ¢ç´¢æ™‚ï¼Œé‡è¦çš„æ˜¯è¦æ‰¿èªæ›´å»£æ³›çš„è„ˆçµ¡è³‡è¨Š(Context)å’Œæ½›åœ¨æŒ‘æˆ°ã€‚é›–ç„¶æˆ‘å€‘å·²ç¶“å±•ç¤ºäº†æå–ç°¡æ½”ã€æœ‰æ„ç¾©æ‘˜è¦çš„å¯è¡Œæ–¹æ³•ï¼Œä½†è®“æˆ‘å€‘è€ƒæ…®ç‚ºä»€éº¼é€™æ¨£çš„æ–¹æ³•è‡³é—œé‡è¦ä»¥åŠå®ƒæ‰€æ¶‰åŠçš„è¤‡é›œæ€§ã€‚\n",
    "\n",
    "**ç²¾ç…‰(Refinement)çš„ä¸€èˆ¬åŒ–(Generalized)**\n",
    "\n",
    "\n",
    "é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œé€™ç¨®ã€Œæ¼¸é€²å¼æ‘˜è¦ã€æŠ€è¡“åƒ…åƒ…æ˜¯ä¸€å€‹èµ·å§‹éˆ(Chain)ï¼Œå®ƒå°åˆå§‹è³‡æ–™å’Œæ‰€éœ€è¼¸å‡ºæ ¼å¼åšå‡ºå¾ˆå°‘å‡è¨­ã€‚ç›¸åŒçš„æŠ€è¡“å¯ä»¥å»£æ³›æ“´å±•ï¼Œä»¥ç”Ÿæˆå…·æœ‰å·²çŸ¥ä¸­ä»‹è³‡æ–™(Metadata)ã€ç©æ¥µå‡è¨­å’Œä¸‹æ¸¸ç›®æ¨™çš„ç¶œåˆçµæœ(Synthesis)ç²¾ç…‰(Refinement)ã€‚\n",
    "\n",
    "**è€ƒæ…®é€™äº›æ½›åœ¨æ‡‰ç”¨ï¼š**\n",
    "\n",
    "1.  **æ•´åˆ(Aggregate)è³‡æ–™**ï¼šæ§‹å»ºå°‡åŸå§‹è³‡æ–™å¾æ–‡ä»¶åˆ†å¡Šè½‰æ›ç‚ºé€£è²«ã€æœ‰ç”¨æ‘˜è¦çš„çµæ§‹ã€‚\n",
    "\n",
    "2.  **åˆ†é¡å’Œå­ä¸»é¡Œåˆ†æ**ï¼šå‰µå»ºå°‡åˆ†å¡Šä¸­çš„è¦‹è§£åˆ†é¡åˆ°å®šç¾©é¡åˆ¥ä¸­çš„ç³»çµ±ï¼Œè¿½è¹¤æ¯å€‹é¡åˆ¥ä¸­å‡ºç¾çš„å­ä¸»é¡Œã€‚\n",
    "\n",
    "3.  **æ•´åˆ(Aggregate)ç‚ºå¯†é›†è³‡è¨Šåˆ†å¡Š**ï¼šå°‡é€™äº›çµæ§‹ç²¾ç…‰(Refinement)ç‚ºå°‡è¦‹è§£æç…‰æˆç·Šæ¹Šç‰‡æ®µï¼Œä¸¦å¯Œå«ç›´æ¥å¼•ç”¨ä»¥é€²è¡Œæ›´æ·±å…¥åˆ†æã€‚\n",
    "\n",
    "é€™äº›æ‡‰ç”¨æš—ç¤ºäº†å‰µå»º**ç‰¹å®šé ˜åŸŸçŸ¥è­˜åœ–è­œ(Knowledge Graph)**ï¼Œå¯ä»¥è¢«å°è©±èŠå¤©æ¨¡å‹å­˜å–å’Œéè¨ª(Traverse)ã€‚ä¸€äº›å¯¦ç”¨å·¥å…·å·²ç¶“å­˜åœ¨ï¼Œå¯ä»¥é€éÂ [**LangChain çŸ¥è­˜åœ–è­œ(Knowledge Graphs)**](https://python.langchain.com/docs/tutorials/graph/)Â ç­‰å·¥å…·è‡ªå‹•ç”Ÿæˆé€™äº›ã€‚é›–ç„¶æ‚¨å¯èƒ½éœ€è¦é–‹ç™¼éšå±¤çµæ§‹(Hierarchy)å’Œå·¥å…·ä¾†æ§‹å»ºå’Œéè¨ª(Traverse)é€™æ¨£çš„çµæ§‹ï¼Œä½†ç•¶æ‚¨å¯ä»¥ç‚ºæ‚¨çš„ä½¿ç”¨æ¡ˆä¾‹é©ç•¶ç²¾ç…‰(Refinement)è¶³å¤ çš„çŸ¥è­˜åœ–è­œ(Knowledge Graph)æ™‚ï¼Œé€™æ˜¯ä¸€å€‹å¯è¡Œçš„é¸é …ï¼å°æ–¼é‚£äº›å°æ›´é«˜ç´šçš„çŸ¥è­˜åœ–è­œ(Knowledge Graph)æ§‹å»ºæŠ€è¡“æ„Ÿèˆˆè¶£çš„äººï¼Œé€™äº›æŠ€è¡“ä¾è³´æ–¼æ›´å¤§çš„ç³»çµ±å’Œå‘é‡ç›¸ä¼¼æ€§ï¼Œæˆ‘å€‘ç™¼ç¾Â [**LangChain x Neo4j æ–‡ç« **](https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/)Â å¾ˆæœ‰è¶£ã€‚\n",
    "\n",
    "**è§£æ±ºå¤§è¦æ¨¡è³‡æ–™è™•ç†çš„æŒ‘æˆ°**\n",
    "\n",
    "\n",
    "é›–ç„¶æˆ‘å€‘çš„æ–¹æ³•é–‹å•Ÿäº†ä»¤äººèˆˆå¥®çš„å¯èƒ½æ€§ï¼Œä½†å®ƒä¸¦éæ²’æœ‰æŒ‘æˆ°ï¼Œç‰¹åˆ¥æ˜¯åœ¨è™•ç†å¤§é‡è³‡æ–™æ™‚ï¼š\n",
    "\n",
    "-   **é€šç”¨é è™•ç†é™åˆ¶**ï¼šé›–ç„¶æ‘˜è¦ç›¸å°ç›´æ¥ï¼Œä½†é–‹ç™¼åœ¨å„ç¨®è„ˆçµ¡è³‡è¨Š(Context)ä¸­æ™®éæœ‰æ•ˆçš„éšå±¤çµæ§‹(Hierarchy)æ˜¯å…·æœ‰æŒ‘æˆ°æ€§çš„ã€‚\n",
    "\n",
    "-   **ç²’åº¦å’Œè¨ªå•(Navigate)æˆæœ¬**ï¼šåœ¨éšå±¤çµæ§‹(Hierarchy)ä¸­å¯¦ç¾è©³ç´°ç²’åº¦å¯èƒ½æ˜¯è³‡æºå¯†é›†çš„ï¼Œéœ€è¦è¤‡é›œçš„æ•´åˆ(Aggregate)æˆ–å»£æ³›çš„åˆ†æ”¯(Branch)ä¾†ç¶­æŒæ¯æ¬¡äº’å‹•çš„å¯ç®¡ç†è„ˆçµ¡è³‡è¨Š(Context)å¤§å°ã€‚\n",
    "\n",
    "-   **å°ç²¾ç¢ºæŒ‡ä»¤åŸ·è¡Œçš„ä¾è³´**ï¼šä½¿ç”¨æˆ‘å€‘ç›®å‰çš„å·¥å…·è¨ªå•(Navigate)é€™æ¨£çš„éšå±¤çµæ§‹(Hierarchy)å°‡åš´é‡ä¾è³´æ–¼å…·æœ‰å¼·å¤§æç¤º(Prompt)å·¥ç¨‹çš„å¼·å¤§æŒ‡ä»¤èª¿æ•´æ¨¡å‹ã€‚æ¨è«–(Inference)å»¶é²å’Œåƒæ•¸é æ¸¬éŒ¯èª¤çš„é¢¨éšªå¯èƒ½å¾ˆå¤§ï¼Œæ‰€ä»¥ä½¿ç”¨ LLM é€²è¡Œæ­¤æ“ä½œå¯èƒ½æ˜¯ä¸€å€‹æŒ‘æˆ°ã€‚\n",
    "\n",
    "ç•¶æ‚¨åœ¨èª²ç¨‹ä¸­é€²æ­¥æ™‚ï¼Œè«‹è¿½è¹¤é€™äº›æŒ‘æˆ°å¦‚ä½•é€éå¾ŒçºŒæŠ€è¡“å¾—åˆ°è§£æ±ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdFSMXOVRzEa",
   "metadata": {
    "id": "cdFSMXOVRzEa"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬å…­éƒ¨åˆ†ï¼š**Â ç¸½çµ\n",
    "\n",
    "é€™å€‹ notebook çš„ç›®æ¨™æ˜¯ä»‹ç´¹åœç¹èŠå¤©æ¨¡å‹å¤§å‹æ–‡ä»¶è™•ç†çš„å•é¡Œå’ŒæŠ€è¡“ã€‚åœ¨ä¸‹ä¸€å€‹ notebook ä¸­ï¼Œæˆ‘å€‘å°‡èª¿æŸ¥ä¸€å€‹å…·æœ‰éå¸¸ä¸åŒå„ªç¼ºé»é›†åˆçš„è£œå……å·¥å…·ï¼›**ä½¿ç”¨å…§åµŒæ¨¡å‹çš„èªç¾©æª¢ç´¢(Retrieval)**ã€‚\n",
    "\n",
    "\n",
    "\n",
    "### <font color=\"#76b900\">**åšå¾—å¾ˆå¥½ï¼**</font>\n",
    "\n",
    "### **ä¸‹ä¸€æ­¥ï¼š**\n",
    "\n",
    "\n",
    "1.  **[å¯é¸]**Â é‡æ–°è¨ªå•(Navigate) notebook é ‚éƒ¨çš„**ã€Œå€¼å¾—æ€è€ƒçš„å•é¡Œã€éƒ¨åˆ†**ï¼Œä¸¦æ€è€ƒä¸€äº›å¯èƒ½çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "2.  **[å¯é¸]**Â é€™å€‹ notebook åŒ…æ‹¬ä¸€äº›åŸºæœ¬çš„æ–‡ä»¶è™•ç†éˆ(Chain)ï¼Œä½†æ²’æœ‰æ¶‰åŠÂ [Map Reduce](https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/)Â éˆ(Chain)ï¼Œé€™äº›ä¹Ÿéå¸¸æœ‰ç”¨ä¸¦å»ºç«‹åœ¨å¤§è‡´ç›¸åŒçš„ç›´è¦ºä¸Šã€‚é€™äº›æ˜¯ä¸€å€‹å¾ˆå¥½çš„ä¸‹ä¸€æ­¥ï¼Œæ‰€ä»¥è«‹æŸ¥çœ‹å®ƒå€‘ï¼\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
