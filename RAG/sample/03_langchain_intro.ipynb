{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea",
   "metadata": {
    "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 3:** LangChain 表達式語言(Expression Language)</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "在上一個 notebook 中，我們介紹了一些我們將在 LLM 應用程式中利用的服務，包括一些外部 LLM 平台和本地託管(Hosting)的前端服務。這兩個組件都整合了 LangChain，但它還沒有被突出作為主要焦點。預期您對 LangChain 和 LLM 有一些經驗，但這個 notebook 旨在讓您跟上進度，為後續章節做準備！\n",
    "\n",
    "這個 notebook 旨在引導您整合和應用 LangChain，這是一個領先的大型語言模型 (LLM) 流程協調管理(Orchestration)函式庫(library)，與上次的 AI 基礎模型端點(Endpoints)結合使用。無論您是經驗豐富的開發者還是 LLM 新手，這門課程都將增強您在建構複雜 LLM 應用程式方面的理解和技能。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **學習目標：**\n",
    "\n",
    "\n",
    "-   學習如何利用鏈(Chain)和可運行物件來流程協調管理(Orchestration)有趣的 LLM 系統。\n",
    "\n",
    "-   熟悉使用 LLM 進行外部對話和內部分析推理(Reasoning)。\n",
    "\n",
    "-   能夠在您的 notebook 中啟動並運行簡單的 [**Gradio**](https://www.gradio.app/) 介面。\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### **值得思考的問題：**\n",
    "\n",
    "-   什麼樣的實用工具可能是保持資訊在管線(Pipeline)中流動所必需的 **（下一個 notebook 的入門）**。\n",
    "-   當您遇到 [**Gradio**](https://www.gradio.app/) 時，考慮您之前在哪些地方見過這種風格的介面。一些可能的地方可能包括 [**HuggingFace Spaces**](https://huggingface.co/spaces)...\n",
    "-   在章節末尾，您將了解到您可以將鏈(Chain)作為路由(Routes)傳遞，並透過連接埠跨環境存取它們。如果您試圖從其他微服務(MICROSERVICES)接收鏈(Chain)，您應該廣告什麼樣的要求？\n",
    "\n",
    "<br>\n",
    "\n",
    "### **環境設置：**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "i-ZOOmvbybyE",
   "metadata": {
    "id": "i-ZOOmvbybyE"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d427c-68e7-45ca-90f7-8799aa9d7eff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **考慮您的模型**\n",
    "\n",
    "\n",
    "回到 [**NVIDIA NGC 目錄**](https://catalog.ngc.nvidia.com/)，我們將能夠找到一些有趣的模型選擇，您可以從您的環境中invoke。這些模型都在那裡是因為它們在正式環境(Production)管線(Pipeline)中有有效用途，所以四處看看並找出哪些模型最適合您的使用案例是個好主意。\n",
    "\n",
    "**提供的程式碼已經包含了一些已列出的模型，但如果您注意到有嚴格更好的選項或某個模型不再可用，您可能想要（或需要）升級到其他模型。*這個註解將適用於課程的其餘部分，所以請記住這一點！***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jhAKeBiDurIz",
   "metadata": {
    "id": "jhAKeBiDurIz"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第一部分：** 什麼是 LangChain？\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evFkwVD6ux-B",
   "metadata": {
    "id": "evFkwVD6ux-B"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "LangChain 是一個常見(popular)的 LLM 流程協調管理(Orchestration)函式庫(library)，用於幫助設置具有一個或多個 LLM 組件的系統。這個函式庫(library)，無論好壞，都極其常見(popular)，並且基於該領域的新發展而快速變化，這意味著某人可能在 LangChain 的某些部分有很多經驗，而對其他部分幾乎沒有或完全沒有熟悉度（要麼是因為有太多不同的功能，要麼是該領域是新的，功能只是最近才實作）。\n",
    "\n",
    "這個 notebook 將使用 **LangChain 表達式語言 (LCEL)** 從基本鏈(Chain)規範提升到更高級的對話管理實踐，所以希望這個旅程會很愉快，甚至經驗豐富的 LangChain 開發者也可能學到一些新東西！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "<!-- > <img style=\"max-width: 400px;\" src=\"imgs/langchain-diagram.png\" /> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/langchain-diagram.png\" width=400px/>\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1NS7dmLf5ql04o5CyPZnd1gnXXgO8-jbR\" width=400px/> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff906c9-d776-4117-b4f6-19ff535b8f57",
   "metadata": {
    "id": "3ff906c9-d776-4117-b4f6-19ff535b8f57"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第二部分：** 鏈(Chain)和 Runnables\n",
    "\n",
    "\n",
    "在探索新函式庫(library)時，重要的是要注意函式庫(library)的核心系統是什麼以及它們是如何使用的。\n",
    "\n",
    "在 LangChain 中，主要的構建塊*過去是*經典的 **鏈(Chain)**：一個執行特定功能的小模組，可以與其他鏈(Chain)連結起來組成一個系統。因此，就所有意圖和目的而言，它是一個「構建塊系統(building-block system)」抽象化(abstractions)，其中構建塊(building blocks)易於創建，具有一致的方法（`invoke`、`generate`、`stream` 等），並且可以連結起來作為一個系統一起工作。一些傳統鏈(Chain)的範例包括 `LLMChain`、`ConversationChain`、`TransformationChain`、`SequentialChain` 等。\n",
    "\n",
    "最近，出現了一個新的推薦規範，它顯著更容易使用且極其簡潔(compact)，即 **LangChain 表達式語言 (LCEL)**。這種新格式依賴於不同類型的原語 - **Runnable** - 它只是一個包裝函式(function)的物件。允許字典(dictionaries)隱式轉換(implicitly converted)為Runnables，並讓 **pipe |** 運算子創建一個將資料從左傳遞到右的Runnable（即 `fn1 | fn2` 是一個Runnable），您就有了一種指定複雜邏輯的簡單方法！\n",
    "\n",
    "以下是一些非常具代表性的範例Runnable，透過 `RunnableLambda` 類別創建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e676de75-cc9b-4fa2-8ce7-d3bd6f9949b7",
   "metadata": {
    "id": "e676de75-cc9b-4fa2-8ce7-d3bd6f9949b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "\n",
      "Welcome Home!\n",
      "1: Welcome Home!\n",
      "2: Welcome Home!\n",
      "\n",
      "Output: Welcome Home!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from functools import partial\n",
    "\n",
    "################################################################################\n",
    "## Very simple \"take input and return it\"\n",
    "identity = RunnableLambda(lambda x: x)  ## Or RunnablePassthrough works\n",
    "\n",
    "################################################################################\n",
    "## Given an arbitrary function, you can make a runnable with it\n",
    "def print_and_return(x, preface=\"\"):\n",
    "    print(f\"{preface}{x}\")\n",
    "    return x\n",
    "\n",
    "rprint0 = RunnableLambda(print_and_return)\n",
    "\n",
    "################################################################################\n",
    "## You can also pre-fill some of values using functools.partial\n",
    "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1: \"))\n",
    "\n",
    "################################################################################\n",
    "## And you can use the same idea to make your own custom Runnable generator\n",
    "def RPrint(preface=\"\"):\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "################################################################################\n",
    "## Chaining two runnables\n",
    "chain1 = identity | rprint0\n",
    "chain1.invoke(\"Hello World!\")\n",
    "print()\n",
    "\n",
    "################################################################################\n",
    "## Chaining that one in as well\n",
    "output = (\n",
    "    chain1           ## Prints \"Welcome Home!\" & passes \"Welcome Home!\" onward\n",
    "    | rprint1        ## Prints \"1: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
    "    | RPrint(\"2: \")  ## Prints \"2: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
    ").invoke(\"Welcome Home!\")\n",
    "\n",
    "## Final Output Is Preserved As \"Welcome Home!\"\n",
    "print(\"\\nOutput:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb670bea-4593-49a2-9b58-6ff2becf1dbe",
   "metadata": {
    "id": "cb670bea-4593-49a2-9b58-6ff2becf1dbe"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## **第三部分：** 運用聊天模型(Chat Models)及字典管線(Dictionary Pipeline)\n",
    "\n",
    "\n",
    "您可以用Runnable做很多事情，但重要的是正式化(Formalize)一些最佳實踐。目前，使用*dictionaries*作為我們的預設變數容器是最容易的，原因如下：\n",
    "\n",
    "**傳遞dictionaries幫助我們按名稱追蹤變數。**\n",
    "\n",
    "由於dictionaries允許我們傳播命名變數（由鍵引用的值），使用它們對於鎖定我們鏈(Chain)組件的輸出和期望非常有用。\n",
    "\n",
    "**LangChain 提示(Prompt)預期收到的dictionary類別的值。**\n",
    "\n",
    "在 LCEL 中指定一個接受dictionary並產生字串的 LLM 鏈(Chain)是非常直觀的，將該字串提升回dictionary也同樣容易。這是非常有意的，部分是由於上述原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xMHAThLp_AgQ",
   "metadata": {
    "id": "xMHAThLp_AgQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **範例 1：** 簡單的 LLM 鏈(Chain)\n",
    "\n",
    "\n",
    "經典 LangChain 最基本的組件之一是接受**提示(Prompt)*\\*和 **LLM** 的 `LLMChain`：\n",
    "\n",
    "-   提示(Prompt)，通常從像 `PromptTemplate.from_template(\"string with {key1} and {key2}\")` 這樣的呼叫中檢索(Retrieval)，指定創建字串作為輸出的模板。可以傳入字典 `{\"key1\" : 1, \"key2\" : 2}` 來獲得輸出 `\"string with 1 and 2\"`。\n",
    "\n",
    "    -   對於像 `ChatNVIDIA` 這樣的聊天模型，您會使用 `ChatPromptTemplate.from_messages` 代替。\n",
    "\n",
    "-   LLM 接受字串並返回生成的字串。\n",
    "\n",
    "    -   像 `ChatNVIDIA` 這樣的聊天模型使用訊息代替，但想法是一樣的！在最後使用 **StrOutputParser** 將從訊息中提取內容。\n",
    "\n",
    "以下是如上所述的簡單聊天鏈(Chain)的輕量級範例。它所做的就是接受輸入(Intake)字典並使用它填入系統訊息以指定整體元目標和使用者輸入(Intake)以指定查詢模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uYBqZ_Za985q",
   "metadata": {
    "id": "uYBqZ_Za985q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Birds are quite a delightful find,\n",
      "With feathers and wings, they soar and entwine.\n",
      "In trees, they alight, with tails so bright,\n",
      "And sing their songs, with morning light.\n",
      "\n",
      "Some have beaks that curve, some have beaks that straight,\n",
      "Their chirps and chatter, fill the air and create\n",
      "A symphony sweet, of melodic sound,\n",
      "As birds take flight, their magic's all around.\n",
      "\n",
      "From robins to sparrows, to hawks on high,\n",
      "Each species unique, yet all touch the sky.\n",
      "With colors bright, and forms so grand,\n",
      "Birds are a wonder, in this world so bland.\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## Simple Chat Pipeline\n",
    "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Only respond in rhymes\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "print(rhyme_chain.invoke({\"input\" : \"Tell me about birds!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4torS7DgBk2T",
   "metadata": {
    "id": "4torS7DgBk2T"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "除了按原樣使用程式碼命令外，我們還可以嘗試使用 [**Gradio 介面**](https://www.gradio.app/guides/creating-a-chatbot-fast)來練習我們的模型。Gradio 是一個常見(popular)的工具，提供簡單的構建塊來創建自訂生成式 AI 介面！下面的範例顯示了如何使用這個特定的範例鏈(Chain)製作一個簡單的 gradio 聊天介面："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "CAQ_DjX7-2oO",
   "metadata": {
    "id": "CAQ_DjX7-2oO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://8cde11603d619413dd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8cde11603d619413dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://8cde11603d619413dd.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "#######################################################\n",
    "## Non-streaming Interface like that shown above\n",
    "\n",
    "# def rhyme_chat(message, history):\n",
    "#     return rhyme_chain.invoke({\"input\" : message})\n",
    "\n",
    "# gr.ChatInterface(rhyme_chat).launch()\n",
    "\n",
    "#######################################################\n",
    "## Streaming Interface\n",
    "\n",
    "def rhyme_chat_stream(message, history):\n",
    "    ## This is a generator function, where each call will yield the next entry\n",
    "    buffer = \"\"\n",
    "    for token in rhyme_chain.stream({\"input\" : message}):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "## Uncomment when you're ready to try this.\n",
    "demo = gr.ChatInterface(rhyme_chat_stream).queue()\n",
    "window_kwargs = {} # or {\"server_name\": \"0.0.0.0\", \"root_path\": \"/7860/\"}\n",
    "demo.launch(share=True, debug=True, **window_kwargs) \n",
    "\n",
    "## IMPORTANT!! When you're done, please click the Square button (twice to be safe) to stop the session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tBGUORsV96_E",
   "metadata": {
    "id": "tBGUORsV96_E"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **範例 2：內部回應**\n",
    "\n",
    "\n",
    "有時，您還希望在回應實際向使用者輸出之前，在幕後進行一些快速分析推理(Reasoning)。執行此任務時，您需要一個內建強大指令跟隨(instruction-following)能力的的模型。\n",
    "\n",
    "以下是一個「零樣本(Zero-Shot)分類」管線(Pipeline)的範例，它將嘗試將句子分類為幾個類別中的一個。\n",
    "\n",
    "**依照順序，這個零樣本(Zero-Shot)分類鏈(Chain)會：**\n",
    "-   接受具有兩個必需鍵的字典，`input` 和 `options`。\n",
    "-   將其通過零樣本(Zero-Shot)提示(Prompt)傳遞以獲得我們 LLM 的輸入(Intake)。\n",
    "-   將該字串傳遞給模型以獲得結果。\n",
    "\n",
    "**任務：** 挑選幾個您認為適合這種任務的模型，看看它們表現如何！具體來說：\n",
    "\n",
    "-   **嘗試找到在多個範例中達到預期結果的模型。** 如果格式總是易於解析且極其可預測，那麼模型可能是可以的。\n",
    "-   **嘗試找到速度快的模型！** 這很重要，因為內部分析推理(Reasoning)通常在生成「面向使用者(user-facing)」的回應開始之前在幕後發生。因此，它可能會延遲「面向使用者」生成的開始，使您的系統感覺遲緩。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b1868d-4ece-4ee6-b6c3-92ec7164bda5",
   "metadata": {
    "id": "39b1868d-4ece-4ee6-b6c3-92ec7164bda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "car\n",
      "--------------------------------------------------------------------------------\n",
      "boat\n",
      "--------------------------------------------------------------------------------\n",
      "airplane\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## Feel free to try out some more models and see if there are better lightweight options\n",
    "## https://build.nvidia.com\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n",
    "\n",
    "sys_msg = (\n",
    "    \"Choose the most likely topic classification given the sentence as context.\"\n",
    "    \" Only one word, no explanation.\\n[Options : {options}]\"\n",
    ")\n",
    "\n",
    "## One-shot classification prompt with heavy format assumptions.\n",
    "zsc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg),\n",
    "    (\"user\", \"[[The sea is awesome]]\"),\n",
    "    (\"assistant\", \"boat\"),\n",
    "    (\"user\", \"[[{input}]]\"),\n",
    "])\n",
    "\n",
    "## Roughly equivalent as above for <s>[INST]instruction[/INST]response</s> format\n",
    "zsc_prompt = ChatPromptTemplate.from_template(\n",
    "    f\"{sys_msg}\\n\\n\"\n",
    "    \"[[The sea is awesome]][/INST]boat</s><s>[INST]\"\n",
    "    \"[[{input}]]\"\n",
    ")\n",
    "\n",
    "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()\n",
    "\n",
    "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
    "    return zsc_chain.invoke({\"input\" : input, \"options\" : options}).split()[0]\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jXdjidkkKG9W",
   "metadata": {
    "id": "jXdjidkkKG9W"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **範例 3：多組件鏈(Multi-Component Chain)**\n",
    "\n",
    "上一個範例顯示了我們如何透過將字典傳遞到 `提示(Prompt) -> LLM` 鏈(Chain)來強制將字典轉換為字串，所以這是一個容易激發容器選擇的結構(motivate the container choice)。但是將字串輸出轉換回字典同樣容易嗎？\n",
    "\n",
    "**是的，確實如此！** 最簡單的方法實際上是使用 LCEL *\"implicit runnable\"* 語法，它允許您使用函式(function)字典（包括鏈(Chain)）作為runnable，該runnable運行每個函式(function)並將值映射到輸出字典中的鍵。\n",
    "\n",
    "以下是一個範例可以練習這些實用工具，同時還提供一些您在實作中可能會發現有用的額外工具。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Yi8-lKSIKhXe",
   "metadata": {
    "id": "Yi8-lKSIKhXe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: {'input': 'Hello World'}\n",
      "B: {'input': 'Hello World'}\n",
      "C: Hello World\n",
      "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
      "E: Hello\n",
      "F: HELLO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': 'HELLO'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from functools import partial\n",
    "\n",
    "################################################################################\n",
    "## Example of dictionary enforcement methods\n",
    "def make_dictionary(v, key):\n",
    "    if isinstance(v, dict):\n",
    "        return v\n",
    "    return {key : v}\n",
    "\n",
    "def RInput(key='input'):\n",
    "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "def ROutput(key='output'):\n",
    "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "def RPrint(preface=\"\"):\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "################################################################################\n",
    "## Common LCEL utility for pulling values from dictionaries\n",
    "from operator import itemgetter\n",
    "\n",
    "up_and_down = (\n",
    "    RPrint(\"A: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | RInput()\n",
    "    | RPrint(\"B: \")\n",
    "    ## Pull-values-from-dictionary utility\n",
    "    | itemgetter(\"input\")\n",
    "    | RPrint(\"C: \")\n",
    "    ## Anything-in Dictionary-out implicit map\n",
    "    | {\n",
    "        'word1' : (lambda x : x.split()[0]),\n",
    "        'word2' : (lambda x : x.split()[1]),\n",
    "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
    "    }\n",
    "    | RPrint(\"D: \")\n",
    "    | itemgetter(\"word1\")\n",
    "    | RPrint(\"E: \")\n",
    "    ## Anything-in anything-out lambda application\n",
    "    | RunnableLambda(lambda x: x.upper())\n",
    "    | RPrint(\"F: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | ROutput()\n",
    ")\n",
    "\n",
    "up_and_down.invoke({\"input\" : \"Hello World\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "kk0t7kUcMhFT",
   "metadata": {
    "id": "kk0t7kUcMhFT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Hello World\n",
      "B: {'input': 'Hello World'}\n",
      "C: Hello World\n",
      "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
      "E: Hello\n",
      "F: HELLO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': 'HELLO'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE how the dictionary enforcement methods make it easy to make the following syntax equivalent\n",
    "up_and_down.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LVIagnD0byq1",
   "metadata": {
    "id": "LVIagnD0byq1"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第四部分：[練習]** 重新主題化聊天機器人\n",
    "\n",
    "\n",
    "下面是一個詩歌生成範例，展示了您如何在單一 Agent 的組織兩個不同的任務。系統回到簡單的 Gradio 範例，但用一些樣板回應和幕後邏輯擴展了它。\n",
    "\n",
    "它的主要功能如下：\n",
    "\n",
    "-   在第一次回應時，它將基於您的回應生成一首詩。\n",
    "-   在後續回應中，它將保持您原始韻律的格式和結構，同時修改詩歌的主題。\n",
    "\n",
    "**問題：** 目前，系統在第一部分應該運作得很好，但第二部分尚未實作。\n",
    "\n",
    "**目標：** 實作 `rhyme_chat2_stream` 方法的其餘部分，使 Agent 能夠正常運作。\n",
    "\n",
    "為了使 gradio 組件更容易分析推理(Reasoning)，提供了一個簡化的 `queue_fake_streaming_gradio` 方法，它將使用標準 Python `input` 方法模擬 gradio 聊天事件循環"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21077ce-8ad1-4d55-933a-1bfe54d07e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: 01-ai/yi-large. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-4-scout-17b-16e-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='mistralai/mistral-medium-3-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-nemotron', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-24b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-3.1-24b-instruct-2503', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvquery/meta/llama-3.3-70b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model for model in ChatNVIDIA.get_available_models() \n",
    "     if (\"mistral\" in model.id or \"meta/llama\" in model.id) \n",
    "         and model.model_type in ('chat', None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "B-bzsMyrKQ5m",
   "metadata": {
    "id": "B-bzsMyrKQ5m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Agent ]: Let me help you make a poem! What would you like for me to write?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    103\u001b[39m history = [[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mLet me help you make a poem! What would you like for me to write?\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m## Simulating the queueing of a streaming gradio interface, using python input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43mqueue_fake_streaming_gradio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_stream\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhyme_chat2_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mqueue_fake_streaming_gradio\u001b[39m\u001b[34m(chat_stream, history, max_questions)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m## Mimic of the gradio loop with an initial message from the agent.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_questions):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     message = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m[ Human ]: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[ Agent ]: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m     history_entry = [message, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from copy import deepcopy\n",
    "\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")  ## Feel free to change the models\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([(\"user\", (\n",
    "    \"INSTRUCTION: Only respond in rhymes (under 5 sentences and using plain English)\"\n",
    "    \"\\n\\nPROMPT: {input}\"\n",
    "))])\n",
    "\n",
    "prompt2 =  ChatPromptTemplate.from_messages([(\"user\", (\n",
    "    \"INSTRUCTION: Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
    "    \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
    "    \" Try not to rhyme a word with itself.\"\n",
    "    \"\\n\\nOriginal Poem: {input}\"\n",
    "    \"\\n\\nNew Topic: {topic}\"\n",
    "))])\n",
    "\n",
    "## These are the main chains, constructed here as modules of functionality.\n",
    "chain1 = prompt1 | instruct_llm | StrOutputParser()  ## only expects input\n",
    "chain2 = prompt2 | instruct_llm | StrOutputParser()  ## expects both input and topic\n",
    "################################################################################\n",
    "## SUMMARY OF TASK: chain1 currently gets invoked for the first input.\n",
    "##  Please invoke chain2 for subsequent invocations.\n",
    "\n",
    "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
    "    '''This is a generator function, where each call will yield the next entry'''\n",
    "\n",
    "    first_poem = None\n",
    "    for entry in history:\n",
    "        if entry[0] and entry[1]:\n",
    "            ## If a generation occurred as a direct result of a user input,\n",
    "            ##  keep that response (the first poem generated) and break out\n",
    "            first_poem = entry[1]\n",
    "            break\n",
    "\n",
    "    if first_poem is None:\n",
    "        ## First Case: There is no initial poem generated. Better make one up!\n",
    "\n",
    "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        ## iterate over stream generator for first generation\n",
    "        inst_out = \"\"\n",
    "        chat_gen = chain1.stream({\"input\" : message})\n",
    "        for token in chat_gen:\n",
    "            inst_out += token\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "    else:\n",
    "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
    "\n",
    "        # yield f\"Not Implemented!!!\"; return ## <- TODO: Comment this out\n",
    "                \n",
    "        ########################################################################\n",
    "        ## TODO: Invoke the second chain to generate the new rhymes.\n",
    "\n",
    "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
    "        yield buffer\n",
    "        \n",
    "        ## iterate over stream generator for second generation\n",
    "        chat_gen = chain2.stream({\"input\" : first_poem, \"topic\" : message})\n",
    "        for token in chat_gen:\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "        ## END TODO\n",
    "        ########################################################################\n",
    "\n",
    "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "################################################################################\n",
    "## Below: This is a small-scale simulation of the gradio routine.\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=3):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "queue_fake_streaming_gradio(\n",
    "    chat_stream = rhyme_chat2_stream,\n",
    "    history = history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "k9-1X9EVQ42t",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k9-1X9EVQ42t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://5fe9d6f21bda489a01.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5fe9d6f21bda489a01.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://5fe9d6f21bda489a01.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple way to initialize history for the ChatInterface\n",
    "chatbot = gr.Chatbot(value = [[None, \"Let me help you make a poem! What would you like for me to write?\"]])\n",
    "\n",
    "## IF USING COLAB: Share=False is faster\n",
    "gr.ChatInterface(rhyme_chat2_stream, chatbot=chatbot).queue().launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VJ6mEQvgkH7w",
   "metadata": {
    "id": "VJ6mEQvgkH7w"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## **第五部分：[練習]** 使用更深層的 LangChain 整合\n",
    "\n",
    "\n",
    "這個練習讓您有機會了解一些關於 [**LangServe**](https://python.langchain.com/docs/langserve/) 的範例程式碼。具體來說，我們參考 [**`frontend`**](frontend) 目錄以及 [**`09_langserve.ipynb`**](09_langserve.ipynb) notebook。\n",
    "\n",
    "-   訪問(Navigate) [**`09_langserve.ipynb`**](09_langserve.ipynb) 並運行提供的腳本(script)以啟動具有多個活躍路由(active routes)的伺服器。\n",
    "-   完成後，驗證以下 **LangServe `RemoteRunnable`** 是否有效。 **`RemoteRunnable`** 的目標是使將 LangChain 鏈(Chain)託管(Hosting)為 API 端點(Endpoints)變得容易，所以以下只是一個測試以確保它有效。\n",
    "\n",
    "    -   如果第一次不起作用，可能存在操作順序問題。請隨時嘗試重新啟動 langserve notebook。\n",
    "\n",
    "**完成這些步驟後，以下類型的連接將可從課程中的任意 notebook 存取：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddb7207e-e3bb-41a0-b85c-b9c129a8c828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! I'm doing well, thanks for asking! It's great to meet you. Since I'm just a language model, I don't have feelings or emotions, but I'm always happy to chat with you. How about you? How's your day going so far?"
     ]
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
    "for token in llm.stream(\"Hello World! How is it going?\"):\n",
    "    print(token, end='')\n",
    "\n",
    "## Equivalent to the following, assuming you're using the same model\n",
    "# llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\") | StrOutputParser()\n",
    "# for token in llm.stream(\"Hello World! How is it going?\"):\n",
    "#     print(token, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c02e81-08ac-4bfc-aa61-1d5836d13d29",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "這個端點(Endpoints)的活躍使用者之一是 `frontend`，它在其 [**`frontend_server.py`**](./frontend/frontend_server.py) 實作中引用了它：\n",
    "\n",
    "```python\n",
    "## Necessary Endpoints\n",
    "chains_dict = {\n",
    "    'basic' : RemoteRunnable(\"http://lab:9012/basic_chat/\"),\n",
    "    'retriever' : RemoteRunnable(\"http://lab:9012/retriever/\"),  ## For the final assessment\n",
    "    'generator' : RemoteRunnable(\"http://lab:9012/generator/\"),  ## For the final assessment\n",
    "}\n",
    "\n",
    "basic_chain = chains_dict['basic']\n",
    "\n",
    "## Retrieval-Augmented Generation Chain\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign(\n",
    "        {'context' : itemgetter('input') \n",
    "        | chains_dict['retriever'] \n",
    "        | LongContextReorder().transform_documents\n",
    "        | docs2str\n",
    "    })\n",
    ")\n",
    "\n",
    "output_chain = RunnableAssign({\"output\" : chains_dict['generator'] }) | output_puller\n",
    "rag_chain = retrieval_chain | output_chain\n",
    "```\n",
    "\n",
    "因此，部署(Deploying) '/basic_chat' 鏈(Chain)前應該實作前端介面中的 **「基本」** 聊天功能。提醒一下，您可以透過以下生成的連結存取前端："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bad6b7c8-832c-4d4f-a8f1-422100ca0e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+':8090';\n",
       "element.innerHTML = '<a style=\"color:#76b900;\" target=\"_blank\" href='+url+'><h2>< Link To Gradio Frontend ></h2></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:#76b900;\" target=\"_blank\" href='+url+'><h2>< Link To Gradio Frontend ></h2></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800b6b8-9a63-47fd-82ba-a7cd5d15b62a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**當您開始進行評量(Assessment)時，您將重新思考(revisiting)這個想法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52531b-a874-4745-bca4-e8f4ea326cd2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**注意：** 在這種類型的環境中部署架構(Deployment)和依賴於 LangServe API 的這種策略非常非標準，專門為學生提供一些有趣的程式碼來查看。更穩定的配置可以通過最佳化的單函式(function)容器實現，可以在 [**NVIDIA/GenerativeAIExamples GitHub 儲存庫**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/notebooks) 中找到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1rEA-cZWwNSx",
   "metadata": {
    "id": "1rEA-cZWwNSx"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第六部分：** 總結\n",
    "\n",
    "\n",
    "這個 notebook 的目標是讓您接觸 LangChain 表達式語言方案(LangChain Expression Language scheme Schemes)，並提供 `gradio` 和 `LangServe` 介面的接觸，用於提供 LLM 功能！在後續的 notebook 中會有更多這方面的內容，但這個 notebook 推動了 LLM Agent 開發中的中級和新興作法(Paradigm)。\n",
    "\n",
    "<font color=\"#76b900\">**做得很好！**</font>\n",
    "\n",
    "\n",
    "**下一步：**\n",
    "\n",
    "\n",
    "1.  **[可選]** 花幾分鐘查看 `frontend` 目錄，了解部署架構(Deployment)配方和底層功能。\n",
    "\n",
    "2.  **[可選]** 重新訪問(Navigate) notebook 頂部的**「值得思考的問題」部分**，並思考一些可能的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4",
   "metadata": {
    "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
