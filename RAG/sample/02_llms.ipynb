{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ee3921-2244-4545-b0df-0b0ebebff32d",
   "metadata": {
    "id": "38ee3921-2244-4545-b0df-0b0ebebff32d"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zTeQZUUuG1u1",
   "metadata": {
    "id": "zTeQZUUuG1u1"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 2:** LLM 服務與 AI 基礎模型</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "在這個 notebook 中，我們將探索 LLM 服務！我們將討論在邊緣裝置(Edge Device)上部署架構(Deployment) LLM 的優缺點，以及透過可擴展的伺服器部署架構(Deployment)（如透過 NVIDIA AI Foundation 端點(Endpoints)存取的服務）向終端使用者提供強大模型的方法。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **學習目標：**\n",
    "\n",
    "-   了解在本地與可擴展雲端環境中運行 LLM 服務的優缺點。\n",
    "-   熟悉 AI 基礎模型端點(Endpoints)方案(Schemes)，包括：\n",
    "    -   由 `curl` 和 `requests` 等套件促進的原始低階連接介面\n",
    "    -   為使此介面與 LangChain 等開源軟體無縫運作而創建的想法(abstractions)\n",
    "-   熟練地從端點(Endpoints)池中檢索(Retrieval) LLM 生成結果，並能夠選擇模型子集來建構您的軟體。\n",
    "<br>\n",
    "\n",
    "### **值得思考的問題：**\n",
    "\n",
    "1.  您應該為開發 LLM 層層結構(Stack)的人提供什麼樣的模型存取權限，這與您需要為 AI 驅動的網路應用程式終端使用者提供的存取權限相比如何？\n",
    "\n",
    "2.  在考慮支援哪些裝置時，您對其本地計算資源做出了什麼樣的嚴格假設，以及您應該實施什麼類型的備用方案？\n",
    "\n",
    "    -   如果您想為客戶提供具有私有 LLM 部署架構(Deployment)存取權限的 jupyter labs 介面會如何？\n",
    "\n",
    "    -   如果現在您想用您的私有 LLM 部署架構(Deployment)支援他們的本地 jupyter lab 環境會如何？\n",
    "\n",
    "    -   如果您決定支援嵌入式裝置（即 Jetson Nano），是否需要改變任何東西？\n",
    "\n",
    "3.  **[較困難]** 假設您在雲端環境中的自己計算實體(Instance)上部署架構(Deployment)了 Stable Diffusion、Mixtral 和 Llama-13B，共享相同的 GPU 資源。您目前沒有 Stable Diffusion 的商業使用案例，但您的團隊正在用其他兩個進行 LLM 應用程式實驗。您應該從部署架構(Deployment)中移除 Stable Diffusion 嗎？\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc2467-fce5-4cda-800d-3b24463a32f4",
   "metadata": {
    "id": "d7dc2467-fce5-4cda-800d-3b24463a32f4"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## **第一部分**：將大型模型導入您的環境\n",
    "\n",
    "回想上一個 notebook，我們目前的環境在分配的雲端實體(Instance)上運行著幾個微服務(MICROSERVICES)：`docker-router`、`jupyter-notebook-server`、`frontend` 和 `llm-service`（以及其他服務）。\n",
    "\n",
    "-   **jupyter-notebook-server**：運行此 jupyter labs 會話並託管(Hosting)我們 python 環境的服務。\n",
    "-   **docker_router**：幫助我們至少觀察和監控微服務(MICROSERVICES)的服務。\n",
    "-   **frontend**：為我們提供簡單聊天介面的運行中的(Live)網站微服務(MICROSERVICES)。\n",
    "\n",
    "這個 notebook 將更多關注 `llm-service` 微服務(MICROSERVICES)，您將使用它（至少在幕後）與一系列[**基礎模型**](https://www.nvidia.com/en-us/ai-data-science/foundation-models/)介面！具體來說，您將使用[**NVIDIA AI 基礎模型**](https://catalog.ngc.nvidia.com/)的子集來原型化 AI 啟用的管線(Pipeline)並流程協調管理(Orchestration)重要的自然語言支援應用程式。\n",
    "\n",
    "\n",
    "\n",
    "$$---$$\n",
    "\n",
    "\n",
    "\n",
    "在幾乎每個領域中，部署架構(Deployment)大規模深度學習模型都是一項常見但具有挑戰性的任務。今天的模型，如 Llama 2（700 億參數）或像 Mixtral 7x8B 這樣的集成模型，是先進訓練方法、龐大資料資源和強大計算系統的產物。幸運的是，這些模型已經被訓練過，許多使用案例已經可以透過現成的解決方案實現。然而，真正的障礙在於有效地託管(Hosting)這些模型。\n",
    "\n",
    "**大型模型的部署架構(Deployment)情境：**\n",
    "\n",
    "1.  **高端資料中心部署架構(Deployment)：**\n",
    "> 在配備 NVIDIA [A100](https://www.nvidia.com/en-us/data-center/a100/)/[H100](https://www.nvidia.com/en-us/data-center/h100/)/[H200](https://www.nvidia.com/en-us/data-center/h200/) 等 GPU 的資料中心層層結構(Stack)上的未壓縮、未量化模型，以促進快速推論(Inference)和實驗。\n",
    ">\n",
    "> -   **優點**：非常適合可擴展的部署架構(Deployment)和實驗，這個層層結構(Stack)非常適合大型訓練工作流程或同時支援多個使用者或模型。\n",
    "> -   **缺點：** 除非使用案例涉及模型訓練/微調或與較低階模型組件介面，否則為您服務的每個使用者分配此資源是低效的。\n",
    "\n",
    "2.  **適度資料中心/專業消費者硬體部署架構(Deployment)：**\n",
    "\n",
    "> 量化和進一步最佳化的模型可以在更保守的資料中心 GPU（如 [L40](https://www.nvidia.com/en-us/data-center/l40/)/[A30](https://www.nvidia.com/en-us/data-center/products/a30-gpu/)/[A10](https://www.nvidia.com/en-us/data-center/products/a10-gpu/)）上運行（每個實體(Instance)一到兩個），甚至在一些現代消費者 GPU（如高 VRAM [RTX 40 系列 GPU](https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/)）上運行。\n",
    ">\n",
    "> -   **優點：** 這種設置平衡了推論(Inference)速度與單使用者應用程式的可管理限制。這些會話也可以按使用者部署架構(Deployment)，一次運行一到兩個大型模型，並可原始存取模型內部（即使它們需要量化）。\n",
    "> -   **缺點：** 為每個使用者部署架構(Deployment)實體(Instance)在規模上仍然昂貴，儘管對於某些利基工作負載可能是合理的。或者，假設使用者可以在其本地環境中存取這些資源可能是不合理的。\n",
    "\n",
    "\n",
    "3.  **消費者硬體部署架構(Deployment)：**\n",
    "\n",
    "> 儘管在透過神經網路傳播資料的能力方面嚴重受限，但大多數消費者硬體確實具有圖形使用者介面 (GUI)、具有網際網路存取的網路瀏覽器、一定量的記憶體（可以安全地假設至少 1 GB）和相當強大的 CPU。\n",
    ">\n",
    "> -   **缺點：** 目前大多數硬體在任何配置下一次無法運行超過一個本地大型模型，即使運行一個模型也需要大量的資源管理和最佳化限制。\n",
    "> -   **優點：** 在考慮您的服務應該支援什麼樣的使用者時，這是一個合理且包容的起始假設。\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dDqGtWuYwo9",
   "metadata": {
    "id": "8dDqGtWuYwo9"
   },
   "source": [
    "\n",
    "在這門課程中，您的環境將非常代表典型的消費者硬體；儘管我們可以用微服務(MICROSERVICES)啟動和原型化，但我們受到僅 CPU 計算環境的約束，這將難以運行 LLM 模型。雖然這是一個重大限制，但我們仍然能夠透過以下方式利用快速 LLM 功能：\n",
    "\n",
    "-   存取具有計算能力的服務來託管(Hosting)大型模型。\n",
    "\n",
    "-   用於命令輸入(Intake)和結果檢索(Retrieval)的簡化介面。\n",
    "\n",
    "有了我們在微服務(MICROSERVICES)和基於連接埠的連接方面的基礎，我們已經準備好探索為我們的開發環境獲得 LLM 存取的有效介面選項(interfacing options)！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154bced-9358-4e43-a85d-25eeef995f6a",
   "metadata": {
    "id": "f154bced-9358-4e43-a85d-25eeef995f6a"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第二部分：** 託管(Hosting)大型模型服務\n",
    "\n",
    "\n",
    "在我們追求在像我們這樣的資源受限環境（通常僅使用 CPU ）中提供大型語言模型 (LLM) 存取的過程中，我們將評估各種託管(Hosting)選項：\n",
    "\n",
    "**黑盒託管(Hosting)模型：**\n",
    "\n",
    "> 像 [**OpenAI**](https://openai.com/) 這樣的服務提供 API 來與 GPT-4 等黑盒模型互動。這些強大、整合良好的服務可以為複雜的管線(Pipeline)提供簡單的介面，自動追蹤記憶體、呼叫額外的模型，並根據需要整合多模態介面以簡化典型的使用情境。同時，它們保持操作不透明性，通常缺乏自託管(Hosting)的直接路徑。\n",
    ">\n",
    "> -   **優點：** 開箱即用易於使用，對普通使用者的進入門檻較低。\n",
    "> -   **缺點：** 黑盒部署架構(Deployment)存在潛在的隱私問題、有限的客製化以及規模成本影響。\n",
    "\n",
    "\n",
    "\n",
    "**自託管(Hosting)模型：**\n",
    "\n",
    "> 在幾乎所有規模化模型部署架構(Deployment)的幕後，都有一個或多個在資料中心運行的巨型模型，擁有可擴展的資源和閃電般快速的頻寬。儘管對於大規模部署架構(Deployment)大型模型並保持對提供介面的強力控制是必要的，但這些系統通常需要專業知識來設置，並且通常不適合僅為一個人支援非開發者工作流程。這樣的系統更適合同時支援許多使用者、多個模型和自訂介面。\n",
    ">\n",
    "> -   **優點：** 它們提供整合自訂資料集和 API 的能力，主要設計用於同時支援眾多使用者。\n",
    "> -   **缺點：** 這些設置需要技術專業知識來設置和正確配置。\n",
    "\n",
    "\n",
    "為了獲得兩全其美的效果，我們將利用 [**NVIDIA NGC 服務**](https://www.nvidia.com/en-us/gpu-cloud/)。NGC 提供一套用於設計和部署架構(Deployment) AI 解決方案的開發者工具。對我們需求的核心是 [NVIDIA AI 基礎模型](https://www.nvidia.com/en-us/ai-data-science/foundation-models/)，這些是預調整和預最佳化的模型，設計用於輕鬆開箱即用的可擴展部署架構(Deployment)（按原樣或進一步客製化）。此外，NGC 託管(Hosting)可存取的模型端點(Endpoints)，用於在[可擴展的 DGX 加速計算環境](https://www.nvidia.com/en-us/data-center/dgx-platform/)中查詢運行中的(Live)基礎模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae66826-e829-42b4-a125-c533d2e6ffae",
   "metadata": {
    "id": "bae66826-e829-42b4-a125-c533d2e6ffae"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第三部分：** 開始使用託管(Hosting)推論(Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2b1cf-9e8d-4687-822b-5bf8fbb4adb2",
   "metadata": {},
   "source": [
    "**當部署架構(Deployment)模型進行規模化推論(Inference)時，您通常需要採取的步驟如下：**\n",
    "-   識別您希望使用者存取的模型，並分配資源來託管(Hosting)它們。\n",
    "-   找出您希望使用者擁有什麼樣的控制，並公開他們存取它的方式。\n",
    "-   創建監控方案(Schemes)來追蹤/限制使用，並設置系統來根據需要擴展和節流。\n",
    "\n",
    "對於這門課程，您將使用由 NVIDIA 部署架構(Deployment)的模型，這些模型作為 [**NVIDIA NIM 微服務(MICROSERVICES)**](https://www.nvidia.com/en-us/ai/) 託管(Hosting)。NIM 生態系統由最佳化運行 AI 工作負載以進行規模化推論(Inference)部署架構(Deployment)的微服務(MICROSERVICES)組成。它們對本地推論(Inference)運作良好並提供標準化 API，但主要設計用於在規模化環境中特別良好地運作。這些特定模型部署架構(Deployment)在 NVIDIA DGX Cloud 上作為共享函式(function)，並透過 OpenAPI 風格的 API 閘道器推廣。讓我們解釋這意味著什麼：\n",
    "\n",
    "**在叢集式(Clustering)端：** 這些微服務(MICROSERVICES)託管(Hosting)在 Kubernetes 支援的平台上，該平台將負載擴展到最小和最大數量的 DGX 節點(Node)，並在單一函式(function)後面提供。換句話說：\n",
    "\n",
    "-   大型語言模型被下載到並部署架構(Deployment)在 **GPU 啟用的計算節點(Node)**（即強大的 CPU 和 4xH100-GPU 環境，在 DGX Pod 中物理整合）。\n",
    "-   啟動時，會啟動這些計算節點(Node)的選擇，這樣每當使用者向函式(function)發送請求時，其中一個節點(Node)將接收請求。\n",
    "    -   Kubernetes 將適當地路由這個流量。如果有空閒的計算節點(Node)，它將接收流量。如果它們都在工作，請求將被排隊，節點(Node)將盡快接收它。\n",
    "    -   在我們的情況下，這些節點(Node)仍然會很快接收請求，因為啟用了即時動態(in-flight)批次處理，意味著每個節點(Node)可以在完全「滿載」之前一次接收多達 256 個活躍請求。（256 是部署架構(Deployment)上的超參數）。\n",
    "-   隨著負載開始增加，自動擴展將啟動，更多節點(Node)將被啟動以避免請求處理延遲。\n",
    "\n",
    "以下圖像顯示了具有自訂（非 OpenAPI）API 的任意函式(function)調用。這是公共端點(Endpoints)最初推廣的方式，但現在變成是常見的實作方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zbt4oxFFhwo-",
   "metadata": {
    "id": "Zbt4oxFFhwo-"
   },
   "source": [
    "<!-- > <img style=\"max-width: 1000px;\" src=\"imgs/ai-playground-api.png\" /> -->\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ckAIZoy7tvtK1uNqzA9eV5RlKMbVqs1-\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/ai-playground-api.png\" width=800px/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae644b94-6566-4c3a-a87b-76c43230e0ab",
   "metadata": {},
   "source": [
    "\n",
    "**在閘道器端：** 為了使這個 API 更標準，使用 API 閘道器伺服器將這些函式(function)聚合在稱為 OpenAPI 的通用 API 後面。這個規範被包括 OpenAI 在內的許多人訂閱(subscribed)，所以使用 OpenAI 客戶端是一個有效的介面：\n",
    "\n",
    "<!-- > <img style=\"max-width: 800px;\" src=\"imgs/mixtral_api.png\" /> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/mixtral_api.png\" width=800px/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6f5c4-98d8-48cc-bc0f-5166519bdd39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "對於這門課程，您將希望使用連接到稱為 LangChain 的 LLM 流程協調管理(Orchestration)框架的更專業介面（稍後詳述）。在您這端，您將使用更客製化的介面，如來自 [`langchain_nvidia_ai_endpoints`](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) 函式庫(library)的 `ChatNVIDIA`。*稍後詳述。*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131d872-c4c9-4395-9ac3-b54abb8c473f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**在使用者端：** 將這些端點(Endpoints)整合到您的客戶端中，您可以設計整合、管線(Pipeline)和使用者體驗，利用這些生成式 AI 功能為您的應用程式賦予推理和生成能力。這樣應用程式的一個常見範例是 [**OpenAI 的 ChatGPT**](https://chat.openai.com/)，它是包括 GPT4、Dalle 和其他端點(Endpoints)的流程協調管理(Orchestration)。儘管它有時看起來像一個單一的智慧模型，但它僅僅是模型端點(Endpoints)的聚合，加上軟體工程來幫助管理狀態和脈絡資訊(Context)控制。這將在整個課程中得到加強，到最後您應該對如何為任意使用案例製作類似的聊天助手有概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0377e6a-0796-404d-a219-84e9b107c32c",
   "metadata": {},
   "source": [
    "<!-- > <img style=\"max-width: 700px;\" src=\"imgs/openai_chat.png\" /> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/openai_chat.png\" width=700px/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f329bd7-712a-4004-891f-e475eddef112",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第四部分：\\[練習\\]** 試用基礎模型端點(Endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c32c86-ffaf-4335-ad2f-6d48d61c8577",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在這個部分，您將最終能夠與您的 LLM 端點(Endpoints)互動！\n",
    "\n",
    "**從您自己的環境：** 您會想要前往 [`build.nvidia.com`](https://build.nvidia.com/) 並找到您想要使用的模型。例如，您可以前往 [**MistralAI 的 Mixtral-8x7b 模型**](https://build.nvidia.com/mistralai/mixtral-8x7b-instruct) 來查看如何使用模型的範例、進一步閱讀的連結，以及一些按鈕，如「申請自託管(Hosting)」和「獲取 API 金鑰」。\n",
    "\n",
    "-   點擊 **「申請自託管(Hosting)」** 將引導您了解 NVIDIA 微服務(MICROSERVICES)的資訊，並為您提供一些註冊途徑（即早期存取/NVIDIA AI Enterprise 路徑）或進入通知清單（一般存取路徑）。\n",
    "\n",
    "-   點擊 **「獲取 API 金鑰」** 將生成一個以「nvapi-」開頭的 API 金鑰，您可以透過網路請求將其提供給 API 端點(Endpoints)！\n",
    "\n",
    "如果您要這樣做，您需要像這樣將 API 金鑰添加到 notebook 中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e08c8c8-eb2d-4d56-a5c6-21e80fd1bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01c207-cbe1-4aa5-8b1c-e282c9a1730c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "**從您的課程環境：** 為了課程的目的，我們將透過在 `llm_client` 目錄中設置的伺服器（即 [**`llm_client/client_server.py`**](llm_client/client_server.py)）直接使用這些模型。圍繞其實作的細節超出了課程範圍，但將透過以下方式為您提供對模型選擇的無限存取：\n",
    "\n",
    "-   公開一些端點(Endpoints)，將您的請求傳遞給選好的模型。\n",
    "\n",
    "-   從 llm_client 微服務(MICROSERVICES)內填入 API 金鑰，這樣您就不會用完額度。\n",
    "\n",
    "***相同的程式碼也可以用作實作您自己的 GenAI 閘道器服務（如 [`integrate.api.nvidia.com`](https://docs.api.nvidia.com/nim/reference/nvidia-embedding-2b-infer) 或 [`api.openai.com`](https://platform.openai.com/docs/api-reference/introduction)）的有用起點。***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388f206-5745-41aa-b5b5-a954e0e92029",
   "metadata": {},
   "source": [
    "### **4.1.** 手動 Python 請求\n",
    "\n",
    "\n",
    "如我們之前所說，您可以使用 Python 的 `requests` 函式庫(library)與微服務(MICROSERVICES)或遠端 API 互動，通常會遵循以下過程：\n",
    "\n",
    "-   **匯入函式庫(library)：** 我們首先匯入用於 HTTP 請求的 requests 和用於處理 JSON 資料的 json。\n",
    "-   **API URL 和標頭：** 定義 API 端點(Endpoints)的 URL 和標頭，包括授權（API 金鑰）和資料格式偏好。\n",
    "-   **資料負載：** 指定您要發送的資料；在這裡，這是一個簡單的查詢。\n",
    "-   **發出請求：** 使用 `requests.post` 發送 POST 請求。您可以根據 API 的要求將 post 替換為 `get`、`put` 等。\n",
    "-   **回應處理：** 檢查狀態碼以確定請求是否成功（200 表示成功），然後處理資料。\n",
    "\n",
    "為了建立對服務的一些了解，我們可以嘗試看看它提供什麼樣的端點(Endpoints)和模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f519964-5796-4073-9f9f-81037839c255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/health': 'Health check endpoint to verify service is running.',\n",
       " '/hello': 'Returns a simple greeting, useful for initial testing.',\n",
       " '/': 'Lists all available endpoints with their descriptions.',\n",
       " '/set_keys': 'Update API keys for application',\n",
       " '/revert': 'Reverts API key to initial state',\n",
       " '/v1/models/{model:path}': 'Returns the listing of available models (or a single model)',\n",
       " '/v1/models': 'Returns the listing of available models (or a single model)',\n",
       " '/v1/{path:path}': 'Forwards requests based on the path to the appropriate OpenAPI endpoint.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "invoke_url = \"http://llm_client:9000\"\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "requests.get(invoke_url, headers=headers, stream=False).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835d15b7-eefe-4232-8536-5e72b2476a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      " - 01-ai/yi-large\n",
      " - abacusai/dracarys-llama-3.1-70b-instruct\n",
      " - ai21labs/jamba-1.5-large-instruct\n",
      " - ai21labs/jamba-1.5-mini-instruct\n",
      " - aisingapore/sea-lion-7b-instruct\n",
      " - baichuan-inc/baichuan2-13b-chat\n",
      " - databricks/dbrx-instruct\n",
      " - deepseek-ai/deepseek-coder-6.7b-instruct\n",
      " - deepseek-ai/deepseek-r1\n",
      " - deepseek-ai/deepseek-r1-0528\n",
      " - deepseek-ai/deepseek-r1-distill-llama-8b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-14b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-32b\n",
      " - deepseek-ai/deepseek-r1-distill-qwen-7b\n",
      " - google/codegemma-1.1-7b\n",
      " - google/codegemma-7b\n",
      " - google/gemma-2-27b-it\n",
      " - google/gemma-2-2b-it\n",
      " - google/gemma-2-9b-it\n",
      " - google/gemma-3-12b-it\n",
      " - google/gemma-3-1b-it\n",
      " - google/gemma-3-27b-it\n",
      " - google/gemma-3-4b-it\n",
      " - google/gemma-7b\n",
      " - google/recurrentgemma-2b\n",
      " - google/shieldgemma-9b\n",
      " - gotocompany/gemma-2-9b-cpt-sahabatai-instruct\n",
      " - ibm/granite-3.0-3b-a800m-instruct\n",
      " - ibm/granite-3.0-8b-instruct\n",
      " - ibm/granite-3.3-8b-instruct\n",
      " - ibm/granite-34b-code-instruct\n",
      " - ibm/granite-8b-code-instruct\n",
      " - ibm/granite-guardian-3.0-8b\n",
      " - igenius/colosseum_355b_instruct_16k\n",
      " - igenius/italia_10b_instruct_16k\n",
      " - institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1\n",
      " - institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1\n",
      " - marin/marin-8b-instruct\n",
      " - mediatek/breeze-7b-instruct\n",
      " - meta/codellama-70b\n",
      " - meta/llama-3.1-405b-instruct\n",
      " - meta/llama-3.1-70b-instruct\n",
      " - meta/llama-3.1-8b-instruct\n",
      " - meta/llama-3.2-11b-vision-instruct\n",
      " - meta/llama-3.2-1b-instruct\n",
      " - meta/llama-3.2-3b-instruct\n",
      " - meta/llama-3.2-90b-vision-instruct\n",
      " - meta/llama-3.3-70b-instruct\n",
      " - meta/llama-4-maverick-17b-128e-instruct\n",
      " - meta/llama-4-scout-17b-16e-instruct\n",
      " - meta/llama2-70b\n",
      " - meta/llama3-70b-instruct\n",
      " - meta/llama3-8b-instruct\n",
      " - microsoft/phi-3-medium-128k-instruct\n",
      " - microsoft/phi-3-medium-4k-instruct\n",
      " - microsoft/phi-3-mini-128k-instruct\n",
      " - microsoft/phi-3-mini-4k-instruct\n",
      " - microsoft/phi-3-small-128k-instruct\n",
      " - microsoft/phi-3-small-8k-instruct\n",
      " - microsoft/phi-3-vision-128k-instruct\n",
      " - microsoft/phi-3.5-mini-instruct\n",
      " - microsoft/phi-3.5-moe-instruct\n",
      " - microsoft/phi-3.5-vision-instruct\n",
      " - microsoft/phi-4-mini-instruct\n",
      " - microsoft/phi-4-multimodal-instruct\n",
      " - mistralai/codestral-22b-instruct-v0.1\n",
      " - mistralai/mamba-codestral-7b-v0.1\n",
      " - mistralai/mathstral-7b-v0.1\n",
      " - mistralai/mistral-7b-instruct-v0.2\n",
      " - mistralai/mistral-7b-instruct-v0.3\n",
      " - mistralai/mistral-large\n",
      " - mistralai/mistral-large-2-instruct\n",
      " - mistralai/mistral-medium-3-instruct\n",
      " - mistralai/mistral-nemotron\n",
      " - mistralai/mistral-small-24b-instruct\n",
      " - mistralai/mistral-small-3.1-24b-instruct-2503\n",
      " - mistralai/mixtral-8x22b-instruct-v0.1\n",
      " - mistralai/mixtral-8x7b-instruct-v0.1\n",
      " - nv-mistralai/mistral-nemo-12b-instruct\n",
      " - nvidia/embed-qa-4\n",
      " - nvidia/llama-3.1-nemoguard-8b-content-safety\n",
      " - nvidia/llama-3.1-nemoguard-8b-topic-control\n",
      " - nvidia/llama-3.1-nemotron-51b-instruct\n",
      " - nvidia/llama-3.1-nemotron-70b-instruct\n",
      " - nvidia/llama-3.1-nemotron-70b-reward\n",
      " - nvidia/llama-3.1-nemotron-nano-4b-v1.1\n",
      " - nvidia/llama-3.1-nemotron-nano-8b-v1\n",
      " - nvidia/llama-3.1-nemotron-nano-vl-8b-v1\n",
      " - nvidia/llama-3.1-nemotron-ultra-253b-v1\n",
      " - nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1\n",
      " - nvidia/llama-3.2-nv-embedqa-1b-v1\n",
      " - nvidia/llama-3.2-nv-embedqa-1b-v2\n",
      " - nvidia/llama-3.3-nemotron-super-49b-v1\n",
      " - nvidia/llama3-chatqa-1.5-70b\n",
      " - nvidia/llama3-chatqa-1.5-8b\n",
      " - nvidia/mistral-nemo-minitron-8b-8k-instruct\n",
      " - nvidia/mistral-nemo-minitron-8b-base\n",
      " - nvidia/nemoretriever-parse\n",
      " - nvidia/nemotron-4-340b-instruct\n",
      " - nvidia/nemotron-4-mini-hindi-4b-instruct\n",
      " - nvidia/nemotron-mini-4b-instruct\n",
      " - nvidia/neva-22b\n",
      " - nvidia/nv-embed-v1\n",
      " - nvidia/nv-embedcode-7b-v1\n",
      " - nvidia/nv-embedqa-e5-v5\n",
      " - nvidia/nv-embedqa-mistral-7b-v2\n",
      " - nvidia/nvclip\n",
      " - nvidia/usdcode-llama-3.1-70b-instruct\n",
      " - nvidia/vila\n",
      " - nvquery/meta/llama-3.3-70b-instruct\n",
      " - qwen/qwen2-7b-instruct\n",
      " - qwen/qwen2.5-7b-instruct\n",
      " - qwen/qwen2.5-coder-32b-instruct\n",
      " - qwen/qwen2.5-coder-7b-instruct\n",
      " - qwen/qwen3-235b-a22b\n",
      " - qwen/qwq-32b\n",
      " - rakuten/rakutenai-7b-chat\n",
      " - rakuten/rakutenai-7b-instruct\n",
      " - snowflake/arctic-embed-l\n",
      " - speakleash/bielik-11b-v2.3-instruct\n",
      " - thudm/chatglm3-6b\n",
      " - tiiuae/falcon3-7b-instruct\n",
      " - tokyotech-llm/llama-3-swallow-70b-instruct-v0.1\n",
      " - upstage/solar-10.7b-instruct\n",
      " - utter-project/eurollm-9b-instruct\n",
      " - writer/palmyra-creative-122b\n",
      " - writer/palmyra-fin-70b-32k\n",
      " - writer/palmyra-med-70b\n",
      " - writer/palmyra-med-70b-32k\n",
      " - zyphra/zamba2-7b-instruct\n",
      "\n",
      "Example Entry:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
       " 'object': 'model',\n",
       " 'created': 735790403,\n",
       " 'owned_by': 'mistralai'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "invoke_url = \"http://llm_client:9000/v1/models\"\n",
    "# invoke_url = \"https://api.openai.com/v1/models\"\n",
    "# invoke_url = \"https://integrate.api.nvidia.com/v1\"\n",
    "# invoke_url = \"http://llm_client:9000/v1/models/mistralai/mixtral-8x7b-instruct-v0.1\"\n",
    "# invoke_url = \"http://llm_client:9000/v1/models/mistralaimixtral-8x7b-instruct-v0.1\"\n",
    "headers = {\n",
    "    \"content-type\": \"application/json\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n",
    "}\n",
    "\n",
    "print(\"Available Models:\")\n",
    "response = requests.get(invoke_url, headers=headers, stream=False)\n",
    "# print(response.json())  ## <- Raw Response. Very Verbose\n",
    "for model_entry in response.json().get(\"data\", []):\n",
    "    print(\" -\", model_entry.get(\"id\"))\n",
    "\n",
    "print(\"\\nExample Entry:\")\n",
    "invoke_url = \"http://llm_client:9000/v1/models/mistralai/mixtral-8x7b-instruct-v0.1\"\n",
    "requests.get(invoke_url, headers=headers, stream=False).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e17d4-0b98-485c-b80c-7d2922f00631",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "我們在這門課程中不會在這個抽象(abstractions)層級上練習太多，但值得實作基本過程來確認，是的，這些請求正在透過我們的微服務(MICROSERVICES)以與伺服器遠端託管(Hosting)時幾乎相同的方式進入。您可以假設在課程的其餘部分，類似以下的互動正在您的客戶端幕後發生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed43fee2-0434-4709-b61f-2df26160bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "## Where are you sending your requests?\n",
    "invoke_url = \"http://llm_client:9000/v1/chat/completions\"\n",
    "\n",
    "## If you wanted to use your own API Key, it's very similar\n",
    "# if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "#     os.environ[\"NVIDIA_API_KEY\"] = getpass(\"NVIDIA_API_KEY: \")\n",
    "# invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "\n",
    "## If you wanted to use OpenAI, it's very similar\n",
    "# if not os.environ.get(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"):\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI_API_KEY: \")\n",
    "# invoke_url = \"https://api.openai.com/v1/models\"\n",
    "\n",
    "## Meta communication-level info about who you are, what you want, etc.\n",
    "headers = {\n",
    "    \"accept\": \"text/event-stream\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "    # \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n",
    "}\n",
    "\n",
    "## Arguments to your server function\n",
    "payload = {\n",
    "    \"model\": \"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "    \"messages\": [{\"role\":\"user\",\"content\":\"Tell me hello in French\"}],\n",
    "    \"temperature\": 0.5,   \n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True                \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56d6937-449d-465d-a257-a6e88ab7f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In French, \"hello\" can be translated as \"bonjour\" (pronounced: bohn-zhoor). This is a formal way to greet someone during the daytime. If you're saying hello in the evening, you can use \"bonsoir\" (pronounced: bohn-swahr). For a casual or informal setting, you can use \"salut\" (pronounced: sah-luu), which can be used for both greetings and farewells.\n",
      "\n",
      "Keep in mind that the pronunciation may vary slightly depending on the region and the speaker's accent. It's always a good idea to listen to native speakers and practice speaking the language to improve your pronunciation."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "## Use requests.post to send the header (streaming meta-info) the payload to the endpoint\n",
    "## Make sure streaming is enabled, and expect the response to have an iter_lines response.\n",
    "response = requests.post(invoke_url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "## If your response is an error message, this will raise an exception in Python\n",
    "try: \n",
    "    response.raise_for_status()  ## If not 200 or similar, this will raise an exception\n",
    "except Exception as e:\n",
    "    # print(response.json())\n",
    "    print(response.json())\n",
    "    raise e\n",
    "\n",
    "## Custom utility to make live a bit easier\n",
    "def get_stream_token(entry: bytes):\n",
    "    \"\"\"Utility: Coerces out ['choices'][0]['delta'][content] from the bytestream\"\"\"\n",
    "    if not entry: return \"\"\n",
    "    entry = entry.decode('utf-8')\n",
    "    if entry.startswith('data: '):\n",
    "        try: entry = json.loads(entry[5:])\n",
    "        except ValueError: return \"\"\n",
    "    return entry.get('choices', [{}])[0].get('delta', {}).get('content') or \"\"\n",
    "\n",
    "## If the post request is honored, you should be able to iterate over \n",
    "for line in response.iter_lines():\n",
    "    \n",
    "    ## Without Processing: data: {\"id\":\"...\", ... \"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"}...}...\n",
    "    # if line: print(line.decode(\"utf-8\"))\n",
    "\n",
    "    ## With Processing: An actual stream of tokens printed one-after-the-other as they come in\n",
    "    print(get_stream_token(line), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d6938-304d-465c-be29-6b95c79ef76c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **[注意事項]**\n",
    "\n",
    "\n",
    "**您可能會注意到聊天模型期望「messages」作為輸入(Intake)：**\n",
    "\n",
    "如果您更習慣於原始 LLM 介面（如本地 HuggingFace 模型的介面），這可能是意外的，但對於 OpenAI 模型的使用者來說會看起來相當標準。透過強制執行受限制的介面而不是原始文字(Text)完成介面，服務可以對使用者能做什麼有更多控制。這個介面有很多優缺點，以下是一些值得注意的：\n",
    "\n",
    "-   服務可能限制特定角色類型或參數的使用（即系統訊息限制、啟動訊息以獲得任意生成等）。\n",
    "-   服務可能強制執行自訂提示(Prompt)格式並在幕後實作依賴聊天介面的額外選項。\n",
    "-   服務可能使用更強的假設在推論(Inference)管線(Pipeline)中實作更深層的最佳化。\n",
    "-   服務可能模仿另一個常見(popular)介面以利用現有的生態系統相容性。\n",
    "\n",
    "所有這些都是有效的理由，在選擇或部署架構(Deployment)您自己的服務時，考慮哪些介面選項最適合您的特定使用案例是重要的。\n",
    "\n",
    "**您可能會注意到查詢模型有兩種基本方式：**\n",
    "\n",
    "您可以 **invoke 而不串流**，在這種情況下，服務回應將在完全計算後一次性到來。當您需要在做其他事情之前獲得模型的完整輸出時，這很好；例如，當您想要列印整個結果或將其用於下游任務時。回應主體將看起來像這樣：\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"d34d436a-c28b-4451-aa9c-02eed2141ed3\",\n",
    "    \"choices\": [{\n",
    "        \"index\": 0,\n",
    "        \"message\": { \"role\": \"assistant\", \"content\": \"Bonjour! ...\" },\n",
    "        \"finish_reason\": \"stop\"\n",
    "    }],\n",
    "    \"usage\": {\n",
    "        \"completion_tokens\": 450,\n",
    "        \"prompt_tokens\": 152,\n",
    "        \"total_tokens\": 602\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "您也可以 **invoke 並串流**，在這種情況下，服務將發出一系列請求，直到發送最終請求。當您可以在服務回應可用時使用它們時，這很好（這對於直接向使用者列印輸出的語言模型組件非常好，因為它在生成時列印）。在這種情況下，回應主體將看起來更像這樣：\n",
    "\n",
    "```json\n",
    "data:{\"id\":\"...\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"Bon\"},\"finish_reason\":null}]}\n",
    "data:{\"id\":\"...\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"j\"},\"finish_reason\":null}]}\n",
    "...\n",
    "data:{\"id\":\"...\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":\"stop\"}]}\n",
    "data:[DONE]\n",
    "``\n",
    "\n",
    "\n",
    "\n",
    "這兩個選項都可以使用 Python 的 `requests` 函式庫(library)相對容易地完成，但按原樣使用介面將導致大量重複程式碼。幸運的是，我們有一些系統使這顯著更容易使用並整合到更大的專案中！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f13e0-cc18-4b67-911a-27813b1dd0df",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### **4.2.** OpenAI 客戶端請求\n",
    "\n",
    "\n",
    "知道這個介面存在是好的，但按原樣使用它將導致大量重複程式碼和額外複雜性。幸運的是，我們有一些系統使這顯著更容易使用並整合到更大的專案中！使用請求之上的一個抽象(abstractions)層是使用更有主導性的客戶端，如 OpenAI 的客戶端。由於 NVIDIA 和 OpenAI 都訂閱 OpenAPI 規範，我們可以借用他們的客戶端。請注意，在幕後，相同的過程仍在進行，可能由像 [**httpx**](https://github.com/projectdiscovery/httpx) 或 [**aiohttp**](https://github.com/aio-libs/aiohttp) 這樣的較低階客戶端進行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f324d88-94e0-480a-9ad6-2ab1aed3eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It's a simple program that outputs the text \"Hello World\" to the console. I'm here to help answer any questions you have about programming or technology, so feel free to ask me anything!"
     ]
    }
   ],
   "source": [
    "## Using General OpenAI Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI()  ## Assumes OPENAI_API_KEY is set\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "#     api_key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
    "# )\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"http://llm_client:9000/v1\",\n",
    "    api_key = \"I don't have one\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Hello World\"}],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "## Streaming with Generator: Results come out as they're generated\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f17380f-eb8f-4cf8-9589-4d8300cb611c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='cmpl-58d997eb6b244875b6088b4832b54748', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), stop_reason=None)], created=1750230418, model='mistralai/mixtral-8x7b-instruct-v0.1', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=73, prompt_tokens=11, total_tokens=84, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Non-Streaming: Results come from server when they're all ready\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "    # model=\"gpt-4-turbo-2024-04-09\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Hello World\"}],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090b044-0880-479b-91bf-3deb2b0a0e46",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### **4.3.** ChatNVIDIA 客戶端請求\n",
    "\n",
    "\n",
    "到目前為止，我們已經看到通訊在兩個想法(abstractions)層級上發生：**原始請求**和 **API 客戶端**。在這門課程中，我們將希望使用稱為 LangChain 的框架進行 LLM 流程協調管理(Orchestration)，所以我們需要上升一個抽象(abstractions)層級到 **框架連接器(Connector)**。\n",
    "\n",
    "**連接器(Connector)**的目標是將任意 API 從其原生核心轉換為目標程式碼庫期望的 API。在這門課程中，我們將希望利用 LangChain 蓬勃發展的以鏈(Chain)為中心的生態系統，但原始的 `requests` API 不會帶我們到那裡。在幕後，每個不在本地託管(Hosting)的 LangChain 聊天模型都必須依賴這樣的 API，但面向開發者的 API 是一個更乾淨的 [`LLM` 或 `SimpleChatModel` 風格介面](https://python.langchain.com/v0.1/docs/modules/model_io/)，具有預設參數和一些簡單的實用函式(function)，如 `invoke` 和 `stream`。\n",
    "\n",
    "為了開始我們對 LangChain 介面的探索，我們將使用 `ChatNVIDIA` 連接器(Connector)與我們的 `chat/completions` 端點(Endpoints)介面。這個模型是 LangChain 擴展生態系統的一部分，可以透過 `pip install langchain-nvidia-ai-endpoints` 在本地安裝。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36f5990-8f3e-4359-b46f-2deeaad6b0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': 'Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!', 'token_usage': {'prompt_tokens': 11, 'total_tokens': 84, 'completion_tokens': 73}, 'finish_reason': 'stop', 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1'}, id='run--8b4eb261-4142-4ffa-94de-32491e34264a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 73, 'total_tokens': 84}, role='assistant')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "## NVIDIA_API_KEY pulled from environment\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", mode=\"open\", base_url=\"http://llm_client:9000/v1\")\n",
    "llm.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cbfe72a-b69b-4fe1-8d21-bf45c1ba6789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'http://llm_client:9000/v1/chat/completions',\n",
       " 'headers': {'Accept': 'application/json',\n",
       "  'Authorization': 'Bearer **********',\n",
       "  'User-Agent': 'langchain-nvidia-ai-endpoints'},\n",
       " 'json': {'messages': [{'role': 'user', 'content': 'Hello World'}],\n",
       "  'model': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
       "  'max_tokens': 1024,\n",
       "  'stream': False}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._client.last_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82418e9-71fd-45ab-a558-9c17ea696864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-3716d625089147c3a9e1fd7411745574',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1750230420,\n",
       " 'model': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Hello! It\\'s nice to meet you. \"Hello World\" is often the first program that people write when they are learning a new programming language. It\\'s a simple program that outputs the text \"Hello World\" to the console. I\\'m here to help answer any questions you have about programming or technology, so feel free to ask me anything!'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'stop_reason': None}],\n",
       " 'usage': {'prompt_tokens': 11, 'total_tokens': 84, 'completion_tokens': 73}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm._client.last_response\n",
    "llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240e9b1-7ee6-400a-b0ab-5f24950b04af",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### **[注意事項]**\n",
    "\n",
    "\n",
    "-   **課程使用 `ai-endpoints` 連接器(Connector)的修改分支，具有幾個對我們課程環境更有用的功能。** 這些功能尚未在主版本中，正在與來自 [**LlamaIndex**](https://docs.llamaindex.ai/en/stable/examples/embeddings/nvidia/) 和 [**Haystack**](https://docs.haystack.deepset.ai/docs/nvidiagenerator) 對應方的其他開發和要求一起主動整合。\n",
    "\n",
    "-   **ChatNVIDIA 預設為 `llm_client` 微服務(MICROSERVICES)，因為我們設置了一些環境變數來實現這一點**：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4cd957f-28b5-4089-8c63-68b6875248b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NVIDIA_DEFAULT_MODE': 'open', 'NVIDIA_BASE_URL': 'http://llm_client:9000/v1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "{k:v for k,v in os.environ.items() if k.startswith(\"NVIDIA_\")}\n",
    "## Translation: Use the base_url of llm_client:9000 for the requests,\n",
    "## and use \"open\"api-spec access for model discovery and url formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035197d8-384c-4715-acb5-49d711f7673c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "\n",
    "**在整個課程中，請隨意試用您選擇的模型。** 以下是作為這門課程一部分提供的模型選擇，其中一個選擇應該在任何給定時間運作。\n",
    "\n",
    "您也可以自由嘗試其他模型，儘管您可能需要升級 `langchain_nvidia_ai_endpoints` 函式庫(library)並提供您自己的金鑰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9f0ab9-0751-4fd5-a44b-16ae2c4c18f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: 01-ai/yi-large. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL: meta/llama-3.1-70b-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and respond to human language, generating text based on the input I receive. I'm a large language model, I don't have a personal identity or consciousness, but I'm here to help answer your questions, provide information, and assist with tasks to the best of my abilities!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.1-8b-instruct\n",
      "I'm an artificial intelligence designed to assist and communicate with humans, and I don't have a physical body or personal identity, but I can provide information and answer questions on a wide range of topics. I'm a large language model, trained on a massive dataset of text to generate human-like responses, and I'm here to help with any questions or topics you'd like to discuss!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-11b-vision-instruct\n",
      "I'm an AI language model, designed to provide information and answer questions on a wide range of topics. I don't have a personal identity or feelings, but I'm here to assist and provide knowledge to the best of my abilities.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-1b-instruct\n",
      "I'm an artificial intelligence designed to assist and communicate with humans in a helpful and informative way. I don't have a personal identity or experiences, but I'm here to help answer your questions and provide insights on a wide range of topics!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-3b-instruct\n",
      "I'm an artificial intelligence model designed to provide information and assist with tasks, and I don't have a personal history or emotions like humans do. I exist solely to help users like you with your queries, and I'm constantly learning and improving my responses based on the data and conversations I have with users.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.2-90b-vision-instruct\n",
      "I'm an AI designed to assist and communicate with users in a helpful and informative way, providing answers and insights on a wide range of topics. I don't have personal experiences or emotions, but I'm here to help with any questions or information you might need.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-3.3-70b-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and respond to human language in a way that simulates conversation and answers questions to the best of my ability. I don't have a physical body or personal experiences, but I've been trained on a massive dataset of text to generate human-like responses and provide helpful information on a wide range of topics!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-4-maverick-17b-128e-instruct\n",
      "I'm an AI designed to assist and communicate with users, providing information and answering questions to the best of my ability. I'm a large language model, so I don't have a personal identity or emotions like humans do, but I'm here to help and provide helpful responses.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama-4-scout-17b-16e-instruct\n",
      "I'm an artificial intelligence language model, which means I'm a computer program designed to understand and generate human-like text. I'm a conversational AI, so I can chat with you, answer questions, and provide information on a wide range of topics - I'm here to help!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama2-70b\n",
      "EXCEPTION: [404] Not Found\n",
      "Function '2fddadfb-7e76-4c8a-9b82-f7d3fab94471': Not found for account 'e8n0r9691j0-U_D7IYJAimxr6klUDXtEyuzSgggm2ns'\n",
      "\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama3-70b-instruct\n",
      "I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner, trained on a massive dataset of text to generate human-like responses. I don't have personal experiences, emotions, or physical existence, but I'm designed to assist and provide helpful information on a wide range of topics!\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: meta/llama3-8b-instruct\n",
      "I'm LLaMA, a large language model trained by a team of researcher at Meta AI. I can understand and respond to human input in a conversational manner, capable of generating human-like text based on the input given to me.\n",
      "\n",
      "====================================================================================\n",
      "TRIAL: nvquery/meta/llama-3.3-70b-instruct\n",
      "EXCEPTION: [404] Not Found\n",
      "Function 'be3c60eb-b2e8-470b-bf75-f2008ec61c93': Not found for account 'e8n0r9691j0-U_D7IYJAimxr6klUDXtEyuzSgggm2ns'\n",
      "\n",
      "\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "model_list = ChatNVIDIA.get_available_models()\n",
    "\n",
    "for model_card in model_list:\n",
    "    model_name = model_card.id\n",
    "    ## If you want to, might be a good idea to not go through EVERY model\n",
    "    if not any([keyword in model_name for keyword in [\"meta/llama\"]]): continue\n",
    "    if \"405b\" in model_name: continue\n",
    "    if \"embed\" in model_name: continue\n",
    "    \n",
    "    llm = ChatNVIDIA(model=model_name)\n",
    "    print(f\"TRIAL: {model_name}\")\n",
    "    try: \n",
    "        for token in llm.stream(\"Tell me about yourself! 2 sentences.\", max_tokens=100):\n",
    "            print(token.content, end=\"\")\n",
    "    except Exception as e: \n",
    "        print(f\"EXCEPTION: {e}\")    ## If some models fail, feel free to use others\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Stopped manually\")  ## Feel free to hit square while running\n",
    "        break\n",
    "    print(\"\\n\\n\" + \"=\"*84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aWLCNC3fAsVx",
   "metadata": {
    "id": "aWLCNC3fAsVx"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第五部分：** 總結\n",
    "\n",
    "\n",
    "這個 notebook 的目標是提供一些圍繞 LLM 服務託管(Hosting)策略的討論，並向您介紹 AI 基礎模型端點(Endpoints)。在這個過程中，我們希望您對如何從邊緣裝置(Edge Device)提供和存取遠端 LLM 系統有直觀的理解！\n",
    "\n",
    "<font color=\"#76b900\">**做得很好！**</font>\n",
    "\n",
    "\n",
    "**下一步：**\n",
    "\n",
    "\n",
    "1.  **[可選]** 重新訪問 notebook 頂部的**「值得思考的問題」部分**，並思考一些可能的答案。\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffac91-77fa-4a6c-a35a-a970fb652617",
   "metadata": {
    "id": "25ffac91-77fa-4a6c-a35a-a970fb652617"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876fa49-e42f-49c3-a235-79989938a7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
