{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [評量(Assessment)]:** RAG 評估</font>\n",
    "\n",
    "歡迎來到本課程的最後一個 notebook！在前一個 notebook 中，您將向量存儲庫(Vector Store)解決方案整合到 RAG 管線(Pipeline)中！在這個 notebook 中，您將採用相同的管線(Pipeline)並使用包含用LLM 作為裁判(LLM-as-a-Judge)衡量指標(Metric)的數值 RAG 評估技術來評估它！\n",
    "\n",
    "<br>\n",
    "\n",
    "### **學習目標：**\n",
    "\n",
    "-   學習如何整合來自先前 notebook 的技術，以數值方式概估您的 RAG 管線(Pipeline)的優良程度。\n",
    "-   **最終練習**：***透過在課程環境中完成這個 notebook，* 您將能夠提交課程的程式設計部分！**\n",
    "<br>\n",
    "\n",
    "### **值得思考的問題：**\n",
    "\n",
    "-   在進行過程中，請記住我們的衡量指標(Metric)實際代表什麼。我們的管線(Pipeline)應該通過這些目標嗎？我們的裁判 LLM 足以評估管線(Pipeline)嗎？特定衡量指標(Metric)對我們的使用案例是否重要？\n",
    "\n",
    "-   如果我們在鏈(Chain)中保留向量存儲庫(Vector Store)作為記憶體組件，您認為它仍然會通過評估嗎？此外，評估對於評估向量存儲庫(Vector Store)作為記憶體效能是否有用？\n",
    "<br>\n",
    "\n",
    "### **環境設置：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## **第一部分：** 預發布(Pre-Release)評估\n",
    "\n",
    "\n",
    "在我們之前的 notebook 中，我們成功結合了幾個概念來創建一個文件聊天機器人，目標是回應式和資訊豐富的互動。然而，使用者互動的多樣性需要全面測試才能真正了解聊天機器人的效能。在各種情境中進行徹底測試對於確保系統不僅穩健和多功能，而且符合使用者和提供者期望至關重要。\n",
    "\n",
    "在定義聊天機器人的角色並實作必要功能後，評估它成為一個多階段過程：\n",
    "\n",
    "-   **典型使用檢查：** 從測試與您的使用案例最相關的情境開始。看看您的聊天機器人是否能在有限的人工干預下可靠地訪問(Navigate)討論。\n",
    "\n",
    "    -   此外，識別應該重新導向給人類進行檢查/監督的限制或隔間（即人工交換以確認交易或執行敏感訪問(Navigate)）並實作這些選項。\n",
    "\n",
    "-   **特殊案例(Edge Case)檢查：** 探索典型使用的邊界，識別聊天機器人如何處理不太常見但合理的情境。\n",
    "\n",
    "    -   在任何公開發布之前，評估可能構成責任風險的關鍵邊界條件，例如可能生成不當內容的潛在風險。\n",
    "\n",
    "    -   在所有輸出（可能還有輸入）上實作經過充分測試的護欄(Guardrails)，以限制不良互動並將使用者重新導向到可預測的對話流程中。\n",
    "\n",
    "-   **漸進式推出(Progressive Rollout)：** 將您的模型推出給有限的受眾（首先是內部，然後是 [A/B](https://en.wikipedia.org/wiki/A/B_testing)）並實作分析功能，如使用分析儀表板和回饋途徑（標記/喜歡/不喜歡等）。\n",
    "\n",
    "在這三個步驟中，前兩個可以由小團隊或個人完成，並應作為開發過程的一部分進行疊代(Iterate)。不幸的是，這需要經常進行，並且容易出現人為錯誤。**幸運的是，LLM 可以用來幫助用LLM 作為裁判(LLM-as-a-Judge)制定！**\n",
    "\n",
    "*（是的，這可能現在並不令人驚訝。LLM 的強大是這門課程存在的原因...）*\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第二部分：** 用LLM 作為裁判(LLM-as-a-Judge)制定\n",
    "\n",
    "\n",
    "在對話式 AI 領域，使用 LLM 作為評估者或「裁判」已成為可配置自動測試自然語言任務效能的有用方法：\n",
    "\n",
    "-   LLM 可以模擬一系列互動情境並生成合成資料，允許評估開發者生成有針對性的輸入(Intake)以從您的聊天機器人中引出一系列行為。\n",
    "-   聊天機器人在合成資料上的對應/檢索(Retrieval)可以由 LLM 評估或解析，並且可以強制執行一致的輸出格式，如「通過」/「失敗」、相似性或提取。\n",
    "-   許多這樣的結果可以整合(Aggregate)，並且可以導出一個衡量指標(Metric)，解釋諸如「通過評估的百分比」、「來源相關細節的平均數量」、「平均餘弦相似性(Cosine Similarity)」等內容。\n",
    "\n",
    "這種使用 LLM 來測試和量化聊天機器人品質的想法，被稱為 [**「用LLM 作為裁判(LLM-as-a-Judge)」**](https://arxiv.org/abs/2306.05685)，允許易於測試規範，與人類判斷密切一致，並且可以在規模上微調和複製。\n",
    "\n",
    "**有幾個常見(popular)的現成裁判制定框架，包括：**\n",
    "\n",
    "-   [**RAGAs（RAG 評量(Assessment)的縮寫）**](https://docs.ragas.io/en/stable/)，為您自己的評估努力提供了一套很好的起點。\n",
    "\n",
    "-   [**LangChain 評估器**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/)，這些是類似的第一方選項，具有許多隱式可構建的 Agent。\n",
    "\n",
    "我們將不按原樣使用鏈(Chain)，而是擴展這些想法並使用更自訂的解決方案評估我們的系統。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第三部分：[評量(Assessment)準備]** 成對(Pairwise)評估器\n",
    "\n",
    "\n",
    "以下練習將充實簡化的 [LangChain 成對(Pairwise)字串評估器](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/) 的自訂實作。\n",
    "\n",
    "**為了準備我們的 RAG 鏈(Chain)評估，我們需要：**\n",
    "\n",
    "-   拉入我們的文件索引（我們在前一個 notebook 中保存的那個）。\n",
    "\n",
    "-   重新創建我們選擇的 RAG 管線(Pipeline)。\n",
    "\n",
    "**我們將具體實作一個具有以下步驟的裁判制定：**\n",
    "\n",
    "-   從 RAG Agent 文件池(document pool)中取樣以找到兩個文件分塊(Chunking)。\n",
    "\n",
    "-   使用這兩個文件分塊(Chunking)生成合成的「基準」問答對。\n",
    "\n",
    "-   使用 RAG Agent 生成自己的答案。\n",
    "\n",
    "-   使用裁判 LLM 比較兩個回應，同時將合成生成作為「真實正確」的基礎。\n",
    "\n",
    "**鏈(Chain)應該是一個簡單但強大的過程，測試以下目標：**\n",
    "\n",
    "> ***我的 RAG 鏈(Chain)是否優於具有有限文件存取權限的窄聊天機器人。***\n",
    "\n",
    "**這將是用於最終評估的系統！** 要查看此系統如何整合到自動評分器中，請查看 [`frontend/server_app.py`](frontend/server_app.py) 中的實作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任務 1：** 拉入您的文件檢索(Retrieval)索引\n",
    "\n",
    "\n",
    "對於這個練習，您將拉入您作為早期 notebook 一部分創建的 `docstore_index` 檔案。以下程式碼區塊應該能夠按原樣載入存儲。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m238\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "\n",
      "Summary: Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when fine-tuned on\n",
      "downstream NLP tasks. However, their ability to access and precisely manipulate\n",
      "knowledge is still limited, and hence on knowledge-intensive tasks, their\n",
      "performance lags behind task-specific architectures. Additionally, providing\n",
      "provenance for their decisions and updating their world knowledge remain open\n",
      "research problems. Pre-trained models with a differentiable access mechanism to\n",
      "explicit non-parametric memory can overcome this issue, but have so far been\n",
      "only investigated for extractive downstream tasks. We explore a general-purpose\n",
      "fine-tuning recipe for retrieval-augmented generation (RAG) -- models which\n",
      "combine pre-trained parametric and non-parametric memory for language\n",
      "generation. We introduce RAG models where the parametric memory is a\n",
      "pre-trained seq2seq model and the non-parametric memory is a dense vector index\n",
      "of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\n",
      "formulations, one which conditions on the same retrieved passages across the\n",
      "whole generated sequence, the other can use different passages per token. We\n",
      "fine-tune and evaluate our models on a wide range of knowledge-intensive NLP\n",
      "tasks and set the state-of-the-art on three open domain QA tasks, outperforming\n",
      "parametric seq2seq models and task-specific retrieve-and-extract architectures.\n",
      "For language generation tasks, we find that RAG models generate more specific,\n",
      "diverse and factual language than a state-of-the-art parametric-only seq2seq\n",
      "baseline.\n",
      "\n",
      "Page Body: .S.\\nRAG-T It\\u2019s the only U.S. state named for a U.S. president\\nRAG-S It\\u2019s the state where you\\u2019ll \\ufb01nd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART\\n*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante\\u2019s \\\"Inferno\\\" is the \\ufb01rst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"\\nFor 2-way classi\\ufb01cation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\\nby RAG and gold evidence annotations\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任務 2：[練習]** 拉入您的 RAG 鏈(Chain)\n",
    "\n",
    "\n",
    "現在我們有了索引，我們可以從前一個 notebook 重新創建 RAG Agent！\n",
    "\n",
    "**關鍵修改：**\n",
    "\n",
    "-   為了保持簡單，請隨意忽略向量存儲庫(Vector Store)作為記憶體組件。納入它將會使練習變得更複雜。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've got a fascinating fact for you! Did you know that the world's largest snowflake was recorded in Montana, USA, on January 28, 1887, and it measured a whopping 15 inches (38 cm) in diameter and 8 inches (20 cm) thick?! That's one massive flake!\n",
      "\n",
      "Would you like to know more about interesting document-related facts or is there something else I can help you with?I've got a fascinating fact for you! Did you know that the world's largest snowflake was recorded in Montana, USA, on January 28, 1887, and it measured a whopping 15 inches (38 cm) in diameter and 8 inches (20 cm) thick?! That's one massive flake!\n",
      "\n",
      "Would you like to know more about interesting document-related facts or is there something else I can help you with?"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain Specs: \"Hello World\" -> retrieval_chain\n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "# context_getter = RunnableLambda(lambda x: x)  ## TODO\n",
    "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain Specs: retrieval_chain -> generator_chain\n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "# generator_chain = RunnableLambda(lambda x: x)  ## TODO\n",
    "generator_chain = chat_prompt | llm  ## TODO\n",
    "generator_chain = {\"output\" : generator_chain } | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **步驟 3：** 生成合成(Synthetic)問答對(Question-Answer Pairs)\n",
    "\n",
    "\n",
    "在本節中，我們可以實作評估例程的前幾個部分：\n",
    "\n",
    "-   **從 RAG Agent 文件池(document pool)中取樣以找到兩個文件分塊(Chunking)。**\n",
    "\n",
    "-   **使用這兩個文件分塊(Chunking)生成合成的「基準」問答對。**\n",
    "\n",
    "-   使用 RAG Agent 生成自己的答案。\n",
    "\n",
    "-   使用裁判 LLM 比較兩個回應，同時將合成生成作為「真實正確」的基礎。\n",
    "\n",
    "鏈(Chain)應該是一個簡單但強大的過程，測試以下目標：\n",
    "\n",
    "> 我的 RAG 鏈(Chain)是否優於具有有限文件存取權限的聊天機器人？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does Mistral 7B's implementation of sliding window attention (SWA) in conjunction with grouped-query </span>\n",
       "<span style=\"font-weight: bold\">attention (GQA) facilitate more efficient inference for longer sequences compared to other language models?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does Mistral 7B's implementation of sliding window attention \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSWA\u001b[0m\u001b[1m)\u001b[0m\u001b[1m in conjunction with grouped-query \u001b[0m\n",
       "\u001b[1mattention \u001b[0m\u001b[1m(\u001b[0m\u001b[1mGQA\u001b[0m\u001b[1m)\u001b[0m\u001b[1m facilitate more efficient inference for longer sequences compared to other language models?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The introduction of SWA in Mistral 7B, coupled with the use of GQA, allows it to effectively handle </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sequences of arbitrary length at a reduced inference cost, thereby enabling more efficient inference for longer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sequences compared to other language models.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The introduction of SWA in Mistral 7B, coupled with the use of GQA, allows it to effectively handle \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msequences of arbitrary length at a reduced inference cost, thereby enabling more efficient inference for longer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msequences compared to other language models.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the retrieval-augmented generation (RAG) model, which combines pre-trained parametric and </span>\n",
       "<span style=\"font-weight: bold\">non-parametric memory for language generation, compare to traditional language generation models in terms of </span>\n",
       "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks and language generation quality?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does the retrieval-augmented generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m model, which combines pre-trained parametric and \u001b[0m\n",
       "\u001b[1mnon-parametric memory for language generation, compare to traditional language generation models in terms of \u001b[0m\n",
       "\u001b[1mknowledge-intensive NLP tasks and language generation quality?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the document </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG model outperforms traditional parametric seq2seq models and task-specific retrieve-and-extract architectures on</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge-intensive NLP tasks, setting the state-of-the-art on three open domain QA tasks. Additionally, for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language generation tasks, the RAG model generates more specific, diverse, and factual language than a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art parametric-only seq2seq baseline. This suggests that the RAG model's ability to access and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">precisely manipulate knowledge from its non-parametric memory leads to improved performance and quality in language</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: According to the document \u001b[0m\u001b[32m\"Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mRAG model outperforms traditional parametric seq2seq models and task-specific retrieve-and-extract architectures on\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge-intensive NLP tasks, setting the state-of-the-art on three open domain QA tasks. Additionally, for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage generation tasks, the RAG model generates more specific, diverse, and factual language than a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art parametric-only seq2seq baseline. This suggests that the RAG model's ability to access and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprecisely manipulate knowledge from its non-parametric memory leads to improved performance and quality in language\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration tasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the Retrieval-Augmented Generation (RAG) model, which combines pre-trained parametric and </span>\n",
       "<span style=\"font-weight: bold\">non-parametric memory for language generation, compare to a state-of-the-art parametric-only seq2seq baseline in </span>\n",
       "<span style=\"font-weight: bold\">terms of generating specific, diverse, and factual language?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does the Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m model, which combines pre-trained parametric and \u001b[0m\n",
       "\u001b[1mnon-parametric memory for language generation, compare to a state-of-the-art parametric-only seq2seq baseline in \u001b[0m\n",
       "\u001b[1mterms of generating specific, diverse, and factual language?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, RAG models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This is one of the key findings of the paper, which shows that the RAG model's ability to access and precisely </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">manipulate knowledge from a non-parametric memory source, such as Wikipedia, leads to improved performance on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language generation tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: According to the paper \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m, RAG models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgenerate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThis is one of the key findings of the paper, which shows that the RAG model's ability to access and precisely \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmanipulate knowledge from a non-parametric memory source, such as Wikipedia, leads to improved performance on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage generation tasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **步驟 4：** 回答合成(Synthetic)問題\n",
    "\n",
    "\n",
    "在本節中，我們可以實作評估例程的第三部分：\n",
    "\n",
    "-   從 RAG Agent 文件池(document pool)中取樣以找到兩個文件分塊(Chunking)。\n",
    "\n",
    "-   使用這兩個文件分塊(Chunking)生成合成的「基準」問答對。\n",
    "\n",
    "-   **使用 RAG Agent 生成自己的答案。**\n",
    "\n",
    "-   使用裁判 LLM 比較兩個回應，同時將合成生成作為「真實正確」的基礎。\n",
    "\n",
    "鏈(Chain)應該是一個簡單但強大的過程，測試以下目標：\n",
    "\n",
    "> 我的 RAG 鏈(Chain)是否優於具有有限文件存取權限的聊天機器人？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How does Mistral 7B's implementation of sliding window attention (SWA) in conjunction with grouped-query </span>\n",
       "<span style=\"font-weight: bold\">attention (GQA) facilitate more efficient inference for longer sequences compared to other language models?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How does Mistral 7B's implementation of sliding window attention \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSWA\u001b[0m\u001b[1m)\u001b[0m\u001b[1m in conjunction with grouped-query \u001b[0m\n",
       "\u001b[1mattention \u001b[0m\u001b[1m(\u001b[0m\u001b[1mGQA\u001b[0m\u001b[1m)\u001b[0m\u001b[1m facilitate more efficient inference for longer sequences compared to other language models?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the document, Mistral 7B's implementation of sliding window attention (SWA) in conjunction</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with grouped-query attention (GQA) facilitates more efficient inference for longer sequences by reducing the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">computational cost and memory requirements. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">SWA is designed to handle longer sequences more effectively by letting each token attend to at most W tokens from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the previous layer. This means that information can move forward by W tokens at each attention layer, allowing for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a theoretical attention span of approximately 131K tokens. In practice, this yields a 2x speed improvement over a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">vanilla attention baseline for a sequence length of 16K. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">GQA, on the other hand, accelerates inference speed and reduces memory requirements during decoding, allowing for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">higher batch sizes and higher throughput. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Together, SWA and GQA contribute to the enhanced performance and efficiency of Mistral 7B, enabling it to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outperform other language models, including the best 13B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) and the best released 34B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the document, Mistral 7B's implementation of sliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in conjunction\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m facilitates more efficient inference for longer sequences by reducing the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomputational cost and memory requirements. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSWA is designed to handle longer sequences more effectively by letting each token attend to at most W tokens from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe previous layer. This means that information can move forward by W tokens at each attention layer, allowing for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma theoretical attention span of approximately 131K tokens. In practice, this yields a 2x speed improvement over a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvanilla attention baseline for a sequence length of 16K. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mGQA, on the other hand, accelerates inference speed and reduces memory requirements during decoding, allowing for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhigher batch sizes and higher throughput. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTogether, SWA and GQA contribute to the enhanced performance and efficiency of Mistral 7B, enabling it to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutperform other language models, including the best 13B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and the best released 34B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the retrieval-augmented generation (RAG) model, which combines pre-trained parametric and </span>\n",
       "<span style=\"font-weight: bold\">non-parametric memory for language generation, compare to traditional language generation models in terms of </span>\n",
       "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks and language generation quality?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: How does the retrieval-augmented generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m model, which combines pre-trained parametric and \u001b[0m\n",
       "\u001b[1mnon-parametric memory for language generation, compare to traditional language generation models in terms of \u001b[0m\n",
       "\u001b[1mknowledge-intensive NLP tasks and language generation quality?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The RAG model is a game-changer when it comes to knowledge-intensive NLP tasks. Essentially, it </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">combines the strengths of pre-trained parametric and non-parametric memories to generate more accurate and specific</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Let me break it down for you. Traditional language generation models, like seq2seq models, rely solely on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parametric memory, which is learned from the data itself. While this works well for many tasks, it has its </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">limitations. For instance, parametric memory can be brittle and prone to hallucinations, where the model makes </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">things up that aren't actually true.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG models, on the other hand, tap into non-parametric memory, which is essentially an external knowledge base, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like Wikipedia. By accessing this external knowledge, RAG models can generate more accurate and specific language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that's grounded in real-world facts.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Studies have shown that RAG models outperform traditional parametric-only seq2seq models on knowledge-intensive </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks, such as open-domain QA [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. For example, on the Natural Questions [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] dataset, RAG models achieve </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art results, outperforming previous extractive approaches.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Moreover, RAG models are able to generate more factual and specific language, as shown in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. In fact,users found </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that RAG's generation was more preferable to purely parametric BART, as it was found to be more factual and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">specific.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, the RAG model's ability to combine pre-trained parametric and non-parametric memories makes it a more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">robust and accurate language generation model, especially for knowledge-intensive tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] TriviaQA</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Natural Questions</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] WebQuestions</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] CuratedTrec</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The RAG model is a game-changer when it comes to knowledge-intensive NLP tasks. Essentially, it \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcombines the strengths of pre-trained parametric and non-parametric memories to generate more accurate and specific\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mLet me break it down for you. Traditional language generation models, like seq2seq models, rely solely on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mparametric memory, which is learned from the data itself. While this works well for many tasks, it has its \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlimitations. For instance, parametric memory can be brittle and prone to hallucinations, where the model makes \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthings up that aren't actually true.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mRAG models, on the other hand, tap into non-parametric memory, which is essentially an external knowledge base, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike Wikipedia. By accessing this external knowledge, RAG models can generate more accurate and specific language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat's grounded in real-world facts.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mStudies have shown that RAG models outperform traditional parametric-only seq2seq models on knowledge-intensive \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks, such as open-domain QA \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. For example, on the Natural Questions \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m dataset, RAG models achieve \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art results, outperforming previous extractive approaches.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mMoreover, RAG models are able to generate more factual and specific language, as shown in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. In fact,users found \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat RAG's generation was more preferable to purely parametric BART, as it was found to be more factual and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspecific.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, the RAG model's ability to combine pre-trained parametric and non-parametric memories makes it a more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrobust and accurate language generation model, especially for knowledge-intensive tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m TriviaQA\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Natural Questions\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m WebQuestions\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m CuratedTrec\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the Retrieval-Augmented Generation (RAG) model, which combines pre-trained parametric and </span>\n",
       "<span style=\"font-weight: bold\">non-parametric memory for language generation, compare to a state-of-the-art parametric-only seq2seq baseline in </span>\n",
       "<span style=\"font-weight: bold\">terms of generating specific, diverse, and factual language?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How does the Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m model, which combines pre-trained parametric and \u001b[0m\n",
       "\u001b[1mnon-parametric memory for language generation, compare to a state-of-the-art parametric-only seq2seq baseline in \u001b[0m\n",
       "\u001b[1mterms of generating specific, diverse, and factual language?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The RAG model, which combines pre-trained parametric and non-parametric memory, outperformed a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art parametric-only seq2seq baseline in generating specific, diverse, and factual language. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This comparison was done on several knowledge-intensive NLP tasks such as MS-MARCO and Jeopardy question </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation. The results showed that RAG models generated responses that were more factual, specific, and diverse </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">than the BART baseline. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The same study showed that, in the FEVER fact verification task, RAG achieved results within </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% of the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art pipeline models. This suggests that the hybrid model is effective in generating factual language.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The RAG model, which combines pre-trained parametric and non-parametric memory, outperformed a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art parametric-only seq2seq baseline in generating specific, diverse, and factual language. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThis comparison was done on several knowledge-intensive NLP tasks such as MS-MARCO and Jeopardy question \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration. The results showed that RAG models generated responses that were more factual, specific, and diverse \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthan the BART baseline. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe same study showed that, in the FEVER fact verification task, RAG achieved results within \u001b[0m\u001b[1;36m4.3\u001b[0m\u001b[1;38;2;118;185;0m% of the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art pipeline models. This suggests that the hybrid model is effective in generating factual language.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = \"\"\n",
    "    rag_answer = rag_chain.invoke(q)\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **步驟 5：** 實作人類偏好衡量指標(Metric)\n",
    "\n",
    "\n",
    "在本節中，我們可以實作評估例程的第四部分：\n",
    "\n",
    "-   從 RAG Agent 文件池(document pool)中取樣以找到兩個文件分塊(Chunking)。\n",
    "\n",
    "-   使用這兩個文件分塊(Chunking)生成合成的「基準」問答對。\n",
    "\n",
    "-   使用 RAG Agent 生成自己的答案。\n",
    "\n",
    "-   **使用裁判 LLM 比較兩個回應，同時將合成生成作為「真實正確」的基礎。**\n",
    "\n",
    "鏈(Chain)應該是一個簡單但強大的過程，測試以下目標：\n",
    "\n",
    "> 我的 RAG 鏈(Chain)是否優於具有有限文件存取權限的聊天機器人？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does Mistral 7B's implementation of sliding window attention (SWA) in conjunction with </span>\n",
       "<span style=\"font-weight: bold\">grouped-query attention (GQA) facilitate more efficient inference for longer sequences compared to other language </span>\n",
       "<span style=\"font-weight: bold\">models?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does Mistral 7B's implementation of sliding window attention \u001b[0m\u001b[1m(\u001b[0m\u001b[1mSWA\u001b[0m\u001b[1m)\u001b[0m\u001b[1m in conjunction with \u001b[0m\n",
       "\u001b[1mgrouped-query attention \u001b[0m\u001b[1m(\u001b[0m\u001b[1mGQA\u001b[0m\u001b[1m)\u001b[0m\u001b[1m facilitate more efficient inference for longer sequences compared to other language \u001b[0m\n",
       "\u001b[1mmodels?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The introduction of SWA in Mistral 7B, coupled with the use of GQA, allows it to effectively </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">handle sequences of arbitrary length at a reduced inference cost, thereby enabling more efficient inference for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">longer sequences compared to other language models.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The introduction of SWA in Mistral 7B, coupled with the use of GQA, allows it to effectively \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhandle sequences of arbitrary length at a reduced inference cost, thereby enabling more efficient inference for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlonger sequences compared to other language models.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the document, Mistral 7B's implementation of sliding window attention (SWA) in conjunction</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with grouped-query attention (GQA) facilitates more efficient inference for longer sequences by reducing the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">computational cost and memory requirements. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">SWA is designed to handle longer sequences more effectively by letting each token attend to at most W tokens from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the previous layer. This means that information can move forward by W tokens at each attention layer, allowing for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a theoretical attention span of approximately 131K tokens. In practice, this yields a 2x speed improvement over a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">vanilla attention baseline for a sequence length of 16K. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">GQA, on the other hand, accelerates inference speed and reduces memory requirements during decoding, allowing for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">higher batch sizes and higher throughput. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Together, SWA and GQA contribute to the enhanced performance and efficiency of Mistral 7B, enabling it to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outperform other language models, including the best 13B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) and the best released 34B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the document, Mistral 7B's implementation of sliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in conjunction\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m facilitates more efficient inference for longer sequences by reducing the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomputational cost and memory requirements. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSWA is designed to handle longer sequences more effectively by letting each token attend to at most W tokens from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe previous layer. This means that information can move forward by W tokens at each attention layer, allowing for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma theoretical attention span of approximately 131K tokens. In practice, this yields a 2x speed improvement over a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvanilla attention baseline for a sequence length of 16K. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mGQA, on the other hand, accelerates inference speed and reduces memory requirements during decoding, allowing for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhigher batch sizes and higher throughput. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTogether, SWA and GQA contribute to the enhanced performance and efficiency of Mistral 7B, enabling it to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutperform other language models, including the best 13B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and the best released 34B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer provides additional information and specific details about how the implementation of SWA and GQA </span>\n",
       "<span style=\"font-weight: bold\">facilitates more efficient inference for longer sequences in Mistral 7B, including the benefits in terms of </span>\n",
       "<span style=\"font-weight: bold\">computational cost, memory requirements, and inference speed. However, this additional information does not alter </span>\n",
       "<span style=\"font-weight: bold\">the core assertion of the first answer. Furthermore, the second answer introduces a claim about the outperformance </span>\n",
       "<span style=\"font-weight: bold\">of Mistral 7B over other language models (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">), which may introduce inconsistencies, as it wasn't </span>\n",
       "<span style=\"font-weight: bold\">directly addressed in the question. Therefore, the overall score is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m Justification\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer provides additional information and specific details about how the implementation of SWA and GQA \u001b[0m\n",
       "\u001b[1mfacilitates more efficient inference for longer sequences in Mistral 7B, including the benefits in terms of \u001b[0m\n",
       "\u001b[1mcomputational cost, memory requirements, and inference speed. However, this additional information does not alter \u001b[0m\n",
       "\u001b[1mthe core assertion of the first answer. Furthermore, the second answer introduces a claim about the outperformance \u001b[0m\n",
       "\u001b[1mof Mistral 7B over other language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m, which may introduce inconsistencies, as it wasn't \u001b[0m\n",
       "\u001b[1mdirectly addressed in the question. Therefore, the overall score is \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does the retrieval-augmented generation (RAG) model, which combines pre-trained parametric </span>\n",
       "<span style=\"font-weight: bold\">and non-parametric memory for language generation, compare to traditional language generation models in terms of </span>\n",
       "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks and language generation quality?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does the retrieval-augmented generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m model, which combines pre-trained parametric \u001b[0m\n",
       "\u001b[1mand non-parametric memory for language generation, compare to traditional language generation models in terms of \u001b[0m\n",
       "\u001b[1mknowledge-intensive NLP tasks and language generation quality?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the document </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the RAG model outperforms traditional parametric seq2seq models and task-specific retrieve-and-extract </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architectures on knowledge-intensive NLP tasks, setting the state-of-the-art on three open domain QA tasks. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Additionally, for language generation tasks, the RAG model generates more specific, diverse, and factual language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">than a state-of-the-art parametric-only seq2seq baseline. This suggests that the RAG model's ability to access and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">precisely manipulate knowledge from its non-parametric memory leads to improved performance and quality in language</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the document \u001b[0m\u001b[32m\"Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP \u001b[0m\n",
       "\u001b[32mTasks\"\u001b[0m\u001b[1;38;2;118;185;0m, the RAG model outperforms traditional parametric seq2seq models and task-specific retrieve-and-extract \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitectures on knowledge-intensive NLP tasks, setting the state-of-the-art on three open domain QA tasks. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAdditionally, for language generation tasks, the RAG model generates more specific, diverse, and factual language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthan a state-of-the-art parametric-only seq2seq baseline. This suggests that the RAG model's ability to access and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprecisely manipulate knowledge from its non-parametric memory leads to improved performance and quality in language\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration tasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The RAG model is a game-changer when it comes to knowledge-intensive NLP tasks. Essentially, it </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">combines the strengths of pre-trained parametric and non-parametric memories to generate more accurate and specific</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Let me break it down for you. Traditional language generation models, like seq2seq models, rely solely on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parametric memory, which is learned from the data itself. While this works well for many tasks, it has its </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">limitations. For instance, parametric memory can be brittle and prone to hallucinations, where the model makes </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">things up that aren't actually true.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG models, on the other hand, tap into non-parametric memory, which is essentially an external knowledge base, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like Wikipedia. By accessing this external knowledge, RAG models can generate more accurate and specific language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that's grounded in real-world facts.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Studies have shown that RAG models outperform traditional parametric-only seq2seq models on knowledge-intensive </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks, such as open-domain QA [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. For example, on the Natural Questions [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] dataset, RAG models achieve </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art results, outperforming previous extractive approaches.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Moreover, RAG models are able to generate more factual and specific language, as shown in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. In fact,users found </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that RAG's generation was more preferable to purely parametric BART, as it was found to be more factual and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">specific.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, the RAG model's ability to combine pre-trained parametric and non-parametric memories makes it a more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">robust and accurate language generation model, especially for knowledge-intensive tasks.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] TriviaQA</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Natural Questions</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] WebQuestions</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] CuratedTrec</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The RAG model is a game-changer when it comes to knowledge-intensive NLP tasks. Essentially, it \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcombines the strengths of pre-trained parametric and non-parametric memories to generate more accurate and specific\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mLet me break it down for you. Traditional language generation models, like seq2seq models, rely solely on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mparametric memory, which is learned from the data itself. While this works well for many tasks, it has its \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlimitations. For instance, parametric memory can be brittle and prone to hallucinations, where the model makes \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthings up that aren't actually true.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mRAG models, on the other hand, tap into non-parametric memory, which is essentially an external knowledge base, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike Wikipedia. By accessing this external knowledge, RAG models can generate more accurate and specific language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat's grounded in real-world facts.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mStudies have shown that RAG models outperform traditional parametric-only seq2seq models on knowledge-intensive \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks, such as open-domain QA \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. For example, on the Natural Questions \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m dataset, RAG models achieve \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art results, outperforming previous extractive approaches.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mMoreover, RAG models are able to generate more factual and specific language, as shown in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. In fact,users found \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat RAG's generation was more preferable to purely parametric BART, as it was found to be more factual and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspecific.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, the RAG model's ability to combine pre-trained parametric and non-parametric memories makes it a more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrobust and accurate language generation model, especially for knowledge-intensive tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m TriviaQA\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Natural Questions\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m WebQuestions\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m CuratedTrec\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer provides an overview of the RAG model, its comparison to traditional language generation models, </span>\n",
       "<span style=\"font-weight: bold\">and its performance on knowledge-intensive tasks. However, upon closer examination, it becomes evident that the </span>\n",
       "<span style=\"font-weight: bold\">second answer is largely a rehash of the first answer without adding any new insights or analysis. </span>\n",
       "\n",
       "<span style=\"font-weight: bold\">While the second answer correctly identifies the strengths of the RAG model, it does not provide any quantitative </span>\n",
       "<span style=\"font-weight: bold\">evidence or specific examples to support its claims, which could make it seem less convincing than the first </span>\n",
       "<span style=\"font-weight: bold\">answer. Also, some of the claims made in the second answer seem to be based on third-party studies and references </span>\n",
       "<span style=\"font-weight: bold\">that are not explicitly mentioned in the answer, which may raise questions about the accuracy and objectivity of </span>\n",
       "<span style=\"font-weight: bold\">the information presented.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m Justification\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer provides an overview of the RAG model, its comparison to traditional language generation models, \u001b[0m\n",
       "\u001b[1mand its performance on knowledge-intensive tasks. However, upon closer examination, it becomes evident that the \u001b[0m\n",
       "\u001b[1msecond answer is largely a rehash of the first answer without adding any new insights or analysis. \u001b[0m\n",
       "\n",
       "\u001b[1mWhile the second answer correctly identifies the strengths of the RAG model, it does not provide any quantitative \u001b[0m\n",
       "\u001b[1mevidence or specific examples to support its claims, which could make it seem less convincing than the first \u001b[0m\n",
       "\u001b[1manswer. Also, some of the claims made in the second answer seem to be based on third-party studies and references \u001b[0m\n",
       "\u001b[1mthat are not explicitly mentioned in the answer, which may raise questions about the accuracy and objectivity of \u001b[0m\n",
       "\u001b[1mthe information presented.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does the Retrieval-Augmented Generation (RAG) model, which combines pre-trained parametric </span>\n",
       "<span style=\"font-weight: bold\">and non-parametric memory for language generation, compare to a state-of-the-art parametric-only seq2seq baseline </span>\n",
       "<span style=\"font-weight: bold\">in terms of generating specific, diverse, and factual language?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does the Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m model, which combines pre-trained parametric \u001b[0m\n",
       "\u001b[1mand non-parametric memory for language generation, compare to a state-of-the-art parametric-only seq2seq baseline \u001b[0m\n",
       "\u001b[1min terms of generating specific, diverse, and factual language?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">baseline. This is one of the key findings of the paper, which shows that the RAG model's ability to access and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">precisely manipulate knowledge from a non-parametric memory source, such as Wikipedia, leads to improved </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance on language generation tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the paper \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mRAG models generate more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbaseline. This is one of the key findings of the paper, which shows that the RAG model's ability to access and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprecisely manipulate knowledge from a non-parametric memory source, such as Wikipedia, leads to improved \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance on language generation tasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The RAG model, which combines pre-trained parametric and non-parametric memory, outperformed a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art parametric-only seq2seq baseline in generating specific, diverse, and factual language. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This comparison was done on several knowledge-intensive NLP tasks such as MS-MARCO and Jeopardy question </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation. The results showed that RAG models generated responses that were more factual, specific, and diverse </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">than the BART baseline. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The same study showed that, in the FEVER fact verification task, RAG achieved results within </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% of the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art pipeline models. This suggests that the hybrid model is effective in generating factual language.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The RAG model, which combines pre-trained parametric and non-parametric memory, outperformed a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art parametric-only seq2seq baseline in generating specific, diverse, and factual language. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThis comparison was done on several knowledge-intensive NLP tasks such as MS-MARCO and Jeopardy question \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration. The results showed that RAG models generated responses that were more factual, specific, and diverse \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthan the BART baseline. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe same study showed that, in the FEVER fact verification task, RAG achieved results within \u001b[0m\u001b[1;36m4.3\u001b[0m\u001b[1;38;2;118;185;0m% of the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art pipeline models. This suggests that the hybrid model is effective in generating factual language.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer is rated as better than the first because:</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">* Both answers consistently state that the RAG model outperforms the parametric-only seq2seq baseline in generating</span>\n",
       "<span style=\"font-weight: bold\">specific, diverse, and factual language.</span>\n",
       "<span style=\"font-weight: bold\">* The second answer provides additional specific findings from the paper, including:</span>\n",
       "<span style=\"font-weight: bold\">        + The comparison was done on several knowledge-intensive NLP tasks (MS-MARCO and Jeopardy question </span>\n",
       "<span style=\"font-weight: bold\">generation).</span>\n",
       "<span style=\"font-weight: bold\">        + The results showed that RAG models generated responses that were more factual, specific, and diverse than</span>\n",
       "<span style=\"font-weight: bold\">the BART baseline.</span>\n",
       "<span style=\"font-weight: bold\">        + The study also showed that RAG achieved results within </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.3</span><span style=\"font-weight: bold\">% of the state-of-the-art pipeline models in </span>\n",
       "<span style=\"font-weight: bold\">the FEVER fact verification task.</span>\n",
       "<span style=\"font-weight: bold\">* The second answer does not introduce any inconsistencies with the information in the first answer. In fact, it </span>\n",
       "<span style=\"font-weight: bold\">reinforces the main finding of the first answer and provides more details to support it.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m Justification\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer is rated as better than the first because:\u001b[0m\n",
       "\n",
       "\u001b[1m* Both answers consistently state that the RAG model outperforms the parametric-only seq2seq baseline in generating\u001b[0m\n",
       "\u001b[1mspecific, diverse, and factual language.\u001b[0m\n",
       "\u001b[1m* The second answer provides additional specific findings from the paper, including:\u001b[0m\n",
       "\u001b[1m        + The comparison was done on several knowledge-intensive NLP tasks \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMS-MARCO and Jeopardy question \u001b[0m\n",
       "\u001b[1mgeneration\u001b[0m\u001b[1m)\u001b[0m\u001b[1m.\u001b[0m\n",
       "\u001b[1m        + The results showed that RAG models generated responses that were more factual, specific, and diverse than\u001b[0m\n",
       "\u001b[1mthe BART baseline.\u001b[0m\n",
       "\u001b[1m        + The study also showed that RAG achieved results within \u001b[0m\u001b[1;36m4.3\u001b[0m\u001b[1m% of the state-of-the-art pipeline models in \u001b[0m\n",
       "\u001b[1mthe FEVER fact verification task.\u001b[0m\n",
       "\u001b[1m* The second answer does not introduce any inconsistencies with the information in the first answer. In fact, it \u001b[0m\n",
       "\u001b[1mreinforces the main finding of the first answer and provides more details to support it.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "## If it's llama, maybe system message would be good?\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**恭喜！我們現在有一個 LLM 系統，可以對我們的管線(Pipeline)進行分析推理(Reasoning)並嘗試評估它！** 現在我們有了一些裁判結果，我們可以簡單地整合(Aggregate)結果，看看我們的結果吻合 LLM 預期的頻率如何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "## **第四部分：** 進階應用\n",
    "\n",
    "\n",
    "上面的練習旨在為您準備課程的最終評量(Assessment)，並展示了一個簡單但有效的評估器鏈(Chain)。目標和實作細節已為您提供，現在您已經看到它的實際運作，使用它的邏輯可能是有意義的。\n",
    "\n",
    "話雖如此，這個衡量指標(Metric)僅僅是我們指定以下內容的產物：\n",
    "\n",
    "-   **對我們的管線(Pipeline)來說，什麼樣的行為是重要的？**\n",
    "\n",
    "-   **為了展示和評估這種行為，我們需要做什麼？**\n",
    "\n",
    "從這兩個問題，我們可能會想出許多其他評估衡量指標(Metric)，這些衡量指標(Metric)可能評估不同的屬性，納入不同的評估器鏈(Chain)技術，甚至需要不同的管線(Pipeline)組織策略。雖然遠非詳盡清單，但您可能會遇到的一些常見制定可能包括：\n",
    "\n",
    "-   **風格(Style)評估：** 一些評估制定可能簡單到「讓我問一些問題，看看輸出是否感覺理想」。這可能用於查看聊天機器人是否「按照應該的方式行動」，基於提供給裁判 LLM 的描述。我們使用引號，因為這種評估可以合理地僅透過提示(Prompt)工程和 while 迴圈來實現。\n",
    "\n",
    "-   **真實基準(Ground-Truth)評估：** 在我們的鏈(Chain)中，我們使用合成生成來創建一些使用取樣策略的隨機問題和答案，但實際上您可能實際上有一些代表性問題和答案，您需要您的聊天機器人始終正確回答！在這種情況下，應該實作上述練習鏈(Chain)的修改，並在您開發管線(Pipeline)時密切監控。\n",
    "\n",
    "-   **檢索(Retrieval)/增強(Augmentation)評估：** 本課程對什麼樣的預處理和提示(Prompt)步驟對您的管線(Pipeline)有好處做了許多假設，其中大部分是透過實驗確定的。文件預處理、分塊(Chunking)策略、模型選擇和提示(Prompt)規範等因素都發揮了重要作用，因此創建衡量指標(Metric)來驗證這些決策可能是有興趣的。這種衡量指標(Metric)可能需要您的管線(Pipeline)輸出您的脈絡資訊(Context)分塊(Chunking)，或者甚至可能僅依賴內嵌(Embedding)相似性比較，因此在嘗試實作與多個評估策略配合使用的鏈(Chain)時請記住這一點。考慮 [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) 想法(abstractions)作為製作自訂一般化(Generalized)評估例程的良好起點。\n",
    "\n",
    "-   **軌跡評估器(trajectory evaluators)：** 使用更高級的 Agent 制定，您可以實作多查詢策略，假設存在對話記憶(conversation memory)。有了這個，您可以實作一個評估 Agent，它可以：\n",
    "\n",
    "    -   按順序提出一系列問題，以評估 Agent 能夠適應和迎合情境的程度。這種系統通常考慮一系列對應，並旨在挑出和評估 Agent 如何訪問(Navigate)對話的「軌跡」。[**LangChain 軌跡評估器(trajectory evaluators)文件**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) 是一個好的起點。\n",
    "\n",
    "    -   或者，您也可以實作一個評估 Agent，嘗試透過與聊天機器人互動來實現目標。這樣的 Agent 可以輸出他們是否能夠以自然方式訪問(Navigate)到他們的解決方案，甚至可以用來生成關於感知效能的報告。[**LangChain Agent 文件**](https://python.langchain.com/v0.1/docs/modules/agents/) 是一個好的起點！\n",
    "\n",
    "歸根結底，只要確保適當地使用您可用的工具。到課程的這一點，您應該已經很熟悉 LLM 核心價值主張：**它們強大、可擴展、可預測、可控制和可流程協調管理(Orchestration)...但當您只是期望它們預設工作時會表現不可預測。** 評估您的需求，正式化(Formalize)和驗證您的管線(Pipeline)，提供足夠的資訊，並添加盡可能多的控制，使您的系統一致、高效和有效地工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第五部分：[評量(Assessment)]** 評估以獲得學分\n",
    "\n",
    "\n",
    "歡迎來到課程的最後練習！希望您喜歡這些材料，並準備好實際獲得這些 notebook 的學分！對於這部分：\n",
    "\n",
    "-   **確保您在課程環境中**\n",
    "\n",
    "-   **確保 `docstore_index/` 已上傳到課程環境...**\n",
    "\n",
    "    -   **...並包含 [至少一篇最近更新的 Arxiv 論文](https://arxiv.org/search/advanced)。**\n",
    "\n",
    "-   **確保您沒有 [`09_langserve.ipynb`](09_langserve.ipynb) 的舊會話已經佔用連接埠。您的評量(Assessment)需要您實作新的 `/retriever` 和 `/generator` 端點(Endpoints)！！**\n",
    "\n",
    "**目標：** 在啟動時，[**`frontend/frontend_block.py`**](frontend/frontend_block.py) 有幾行程式碼觸發課程通過條件。您的目標是透過使用您的管線(Pipeline)通過**評估**檢查來invoke該系列命令！回想 [`09_langserve.ipynb`](09_langserve.ipynb) 並將其作為起始範例！作為建議，考慮複製它，這樣您可以保留原始版本作為權威參考。\n",
    "\n",
    "**完成後：** 當您的課程環境仍然開啟時，請訪問(Navigate)回到您的課程環境啟動器區域並點擊**「評估任務(Assess Task)」**按鈕！之後，您就完成了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**恭喜完成課程**</font>\n",
    "\n",
    "\n",
    "希望這門課程不僅令人興奮和具有挑戰性，而且也充分為您在 LLM 和 RAG 系統開發的前沿工作做好準備！展望未來，您應該具備處理行業級挑戰和探索使用開源模型和框架的 RAG 部署架構(Deployment)所需的技能。\n",
    "\n",
    "**您可能會發現有趣的一些與此相關的 NVIDIA 特定發布包括：**\n",
    "\n",
    "-   [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/)，提供可在本地計算上部署架構(Deployment)的微服務啟動例程。\n",
    "\n",
    "-   [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) 是目前在正式環境(Production)設置中部署架構(Deployment) GPU 加速 LLM 模型引擎的推薦框架。\n",
    "\n",
    "-   [**NVIDIA 的生成式 AI 範例儲存庫**](https://github.com/NVIDIA/GenerativeAIExamples)，包括當前規範微服務範例應用程式，並將隨著新正式環境(Production)工作流程的發布而更新新資源。\n",
    "\n",
    "-   [**基於知識的聊天機器人技術簡報**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief)，討論關於正式環境(Production)化 RAG 系統的額外公開可存取細節。\n",
    "\n",
    "**此外，您可能有興趣深入研究的一些關鍵主題包括：**\n",
    "\n",
    "-   [**LlamaIndex**](https://www.llamaindex.ai/)，具有可以增強並偶爾改進 LangChain RAG 功能的強大組件。\n",
    "\n",
    "-   [**LangSmith**](https://docs.smith.langchain.com/)，LangChain 提供的即將推出的 Agent 正式環境(Production)化服務。\n",
    "\n",
    "-   [**Gradio**](https://www.gradio.app/)，雖然在課程中有所涉及，但有許多更多值得調查的介面選項。為了獲得靈感，考慮查看 [**HuggingFace Spaces**](https://huggingface.co/spaces) 以獲得範例。\n",
    "\n",
    "-   [**LangGraph**](https://python.langchain.com/docs/langgraph/) 是基於圖形的 LLM 流程協調管理(Orchestration)框架，對於那些對 [多 Agent 工作流程](https://blog.langchain.dev/langgraph-multi-agent-workflows/) 感興趣的人來說是自然的下一步。\n",
    "\n",
    "-   [**DSPy**](https://github.com/stanfordnlp/dspy)，一個流程工程框架，允許您基於經驗效能結果最佳化 LLM 流程協調管理(Orchestration)管線(Pipeline)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
